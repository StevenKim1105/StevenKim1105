{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenKim1105/StevenKim1105/blob/main/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq-qKTNWsGAS",
        "outputId": "36ba94b7-19a8-49e5-a469-979dbcd7d3c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hydroDL'...\n",
            "remote: Enumerating objects: 1046, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 1046 (delta 46), reused 19 (delta 10), pack-reused 960\u001b[K\n",
            "Receiving objects: 100% (1046/1046), 60.67 MiB | 29.32 MiB/s, done.\n",
            "Resolving deltas: 100% (429/429), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mhpi/hydroDL.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content\")\n",
        "os.getcwd()\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1ZeX-M2fA-HKNg1nWwDDsI66O6seUwpz4' -O 'CAMELS.zip'\n",
        "!unzip 'CAMELS.zip' -d '/content/hydroDL/example/linear_regression/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMqcc4r4su1l",
        "outputId": "7e39845a-635f-458f-a594-f6634d80e53a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-19 01:59:48--  https://drive.google.com/uc?export=download&id=1ZeX-M2fA-HKNg1nWwDDsI66O6seUwpz4\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.195.139, 74.125.195.138, 74.125.195.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.195.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-38-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/b9eoemsvtuuigij20gd20kj549r42jen/1671415125000/07681100160477168945/*/1ZeX-M2fA-HKNg1nWwDDsI66O6seUwpz4?e=download&uuid=66da968c-e518-4b05-89c8-d75462a95e95 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-19 01:59:48--  https://doc-0s-38-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/b9eoemsvtuuigij20gd20kj549r42jen/1671415125000/07681100160477168945/*/1ZeX-M2fA-HKNg1nWwDDsI66O6seUwpz4?e=download&uuid=66da968c-e518-4b05-89c8-d75462a95e95\n",
            "Resolving doc-0s-38-docs.googleusercontent.com (doc-0s-38-docs.googleusercontent.com)... 108.177.98.132, 2607:f8b0:400e:c06::84\n",
            "Connecting to doc-0s-38-docs.googleusercontent.com (doc-0s-38-docs.googleusercontent.com)|108.177.98.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 376286 (367K) [application/x-zip-compressed]\n",
            "Saving to: ‘CAMELS.zip’\n",
            "\n",
            "CAMELS.zip          100%[===================>] 367.47K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-12-19 01:59:49 (163 MB/s) - ‘CAMELS.zip’ saved [376286/376286]\n",
            "\n",
            "Archive:  CAMELS.zip\n",
            "  inflating: /content/hydroDL/example/linear_regression/CAMELS/attributes.csv  \n",
            "  inflating: /content/hydroDL/example/linear_regression/CAMELS/camels_attributes_v2.0.xlsx  \n",
            "  inflating: /content/hydroDL/example/linear_regression/CAMELS/precipitation_mm.csv  \n",
            "  inflating: /content/hydroDL/example/linear_regression/CAMELS/runoff_mm.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Parameter\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "AfqcQkhEtKSx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class one_layer(torch.nn.Module):\n",
        "    def __init__(self, inputSize: int, outputSize: int, act=None, dr=0) -> None:\n",
        "        super(one_layer, self).__init__()\n",
        "        self.outputSize = outputSize    # hidden size\n",
        "        self.w = Parameter(torch.randn(inputSize, outputSize))\n",
        "        self.b = Parameter(torch.randn(outputSize))\n",
        "        self.act = act\n",
        "        self.dr = dr\n",
        "        self.reset_parameters() # initialize model weights. This could be done differently.\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
        "        Y = torch.matmul(input,self.w)+self.b # + is a smart plus\n",
        "        #Y = self.linear(input)\n",
        "        if self.act is not None:\n",
        "            Y = self.act(Y)\n",
        "        return torch.dropout(Y,p=self.dr,train=self.training)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / math.sqrt(self.w.size(0))\n",
        "        for para in self.parameters():\n",
        "            para.data.uniform_(-std, std)"
      ],
      "metadata": {
        "id": "tQqRF5BWtN0w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class two_layer(torch.nn.Module):\n",
        "    def __init__(self, inputSize: int, hiddenSize: int, outputSize: int, act=None, dr=0) -> None:\n",
        "        super(one_layer, self).__init__()\n",
        "        self.outputSize = outputSize    # hidden size\n",
        "        self.w1 = Parameter(torch.randn(inputSize, hiddenSize))\n",
        "        self.b1 = Parameter(torch.randn(hiddenSize))\n",
        "        self.w2 = Parameter(torch.randn(hiddenSize, outputSize))\n",
        "        self.b2 = Parameter(torch.randn(outputSize))\n",
        "        self.act = act\n",
        "        self.dr = dr\n",
        "        self.reset_parameters() # initialize model weights. This could be done differently.\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
        "        Y = torch.matmul(input,self.w1)+self.b1 # + is a smart plus\n",
        "        Y2 = torch.matmul(Y,self.w2)+self.b2 # + is a smart plus\n",
        "        #Y = self.linear(input)\n",
        "        if self.act is not None:\n",
        "            Y2 = self.act(Y2)\n",
        "        return torch.dropout(Y2,p=self.dr,train=self.training)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / math.sqrt(self.w.size(0))\n",
        "        for para in self.parameters():\n",
        "            para.data.uniform_(-std, std)\n"
      ],
      "metadata": {
        "id": "7EJIrXdKwTxc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # read in data\n",
        "    import numpy as np\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    # download the csv data files from here (OneDrive link shortened) https://bit.ly/3vTnYTT\n",
        "    # save this .py file to the same directory and browse to it.\n",
        "    os.chdir(r'/content/hydroDL/example/linear_regression') # edit this for your folder\n",
        "    runoff_pd = pd.read_csv('CAMELS/runoff_mm.csv') # working directory must be above CAMELS. If not, use os.chdir to change.\n",
        "    precip_pd = pd.read_csv('CAMELS/precipitation_mm.csv')\n",
        "    # some data wrangling from source data to easy-to-use format\n",
        "    # get [years, basin]\n",
        "    # pd is a pandas object with which you can run a lot of data operations https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
        "    runoff_mean = runoff_pd.mean(0).to_numpy()[1:]\n",
        "    #[1:] is to remove the basin number # get basin-averaged runoff by averaging along the 0-th dimension (years)\n",
        "    runoff_mean = np.expand_dims(runoff_mean,1) # see the effect of this statement on shape\n",
        "    precip_mean = precip_pd.mean(0).to_numpy()[1:] # turn into a numpy ndarray\n",
        "    precip_mean = np.expand_dims(precip_mean,1) # get basin-averaged runoff"
      ],
      "metadata": {
        "id": "bFa2AJLktOZg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(runoff_pd.shape)\n",
        "print(runoff_mean.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__wXGoc9heBW",
        "outputId": "c5a2f6b1-7bcf-4e53-b617-ee95d136580e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 672)\n",
            "(671, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    ax,fig=plt.subplots()\n",
        "    plt.plot(precip_mean,runoff_mean,'.')\n",
        "    plt.xlabel('long-term-average precipitation (mm/year)')\n",
        "    plt.ylabel('long-term-average runoff (mm/year)')\n",
        "    #plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "0s7ETDcFtse5",
        "outputId": "610ebb24-735f-41a2-de72-4e3945875c1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'long-term-average runoff (mm/year)')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # scale data and get Tensor for PyTorch\n",
        "    import torch\n",
        "    torch_option = 0\n",
        "    precip1 = (precip_mean - precip_mean.mean())/precip_mean.std()\n",
        "    runoff1 = (runoff_mean - runoff_mean.mean())/runoff_mean.std()\n",
        "    xT = torch.Tensor(precip1) # scaled\n",
        "    y = torch.Tensor(runoff1) # scaled\n",
        "    learning_rate = 0.005; niter=800\n",
        "\n"
      ],
      "metadata": {
        "id": "eyrwhvOgGtMC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xT.shape)\n",
        "type(xT)\n",
        "print(xT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3uKROfzis3H",
        "outputId": "0f79cf31-9776-4d40-a74c-671929d0aa65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([671, 1])\n",
            "tensor([[-1.8444e-01],\n",
            "        [ 1.2700e-01],\n",
            "        [ 7.1849e-02],\n",
            "        [ 1.5939e-01],\n",
            "        [ 3.0607e-01],\n",
            "        [ 2.9152e-01],\n",
            "        [ 7.0581e-01],\n",
            "        [ 1.3076e-01],\n",
            "        [ 1.7519e-01],\n",
            "        [ 1.9042e-01],\n",
            "        [ 5.9602e-02],\n",
            "        [ 2.8038e-01],\n",
            "        [ 2.0235e-01],\n",
            "        [ 1.6313e-01],\n",
            "        [ 9.3746e-02],\n",
            "        [ 7.1504e-01],\n",
            "        [-1.9416e-02],\n",
            "        [-2.6270e-02],\n",
            "        [ 1.9023e-01],\n",
            "        [ 1.7602e-01],\n",
            "        [ 1.0663e-01],\n",
            "        [ 4.0485e-01],\n",
            "        [ 4.6576e-01],\n",
            "        [ 5.1457e-01],\n",
            "        [ 4.8146e-01],\n",
            "        [ 2.0454e-01],\n",
            "        [ 1.6668e-01],\n",
            "        [ 2.3219e-01],\n",
            "        [-1.7506e-01],\n",
            "        [-1.2258e-01],\n",
            "        [ 5.4569e-01],\n",
            "        [ 1.1275e-02],\n",
            "        [ 1.7922e-02],\n",
            "        [ 8.1117e-02],\n",
            "        [ 1.6596e-02],\n",
            "        [ 9.3138e-03],\n",
            "        [ 6.4898e-01],\n",
            "        [ 4.9933e-01],\n",
            "        [ 2.1969e-01],\n",
            "        [ 1.3863e-01],\n",
            "        [ 3.0128e-01],\n",
            "        [ 2.4858e-01],\n",
            "        [ 1.2126e-01],\n",
            "        [ 3.1377e-02],\n",
            "        [ 8.7993e-02],\n",
            "        [ 1.9722e-02],\n",
            "        [ 3.6890e-02],\n",
            "        [ 2.8783e-02],\n",
            "        [-1.2683e-02],\n",
            "        [-3.7945e-01],\n",
            "        [-2.7013e-01],\n",
            "        [-2.3292e-01],\n",
            "        [ 2.6508e-02],\n",
            "        [-1.0730e-01],\n",
            "        [-8.2913e-02],\n",
            "        [-9.6083e-02],\n",
            "        [-1.5035e-01],\n",
            "        [-1.0615e-01],\n",
            "        [-2.3941e-01],\n",
            "        [-2.7205e-01],\n",
            "        [-2.7510e-01],\n",
            "        [-2.3934e-01],\n",
            "        [-4.1249e-02],\n",
            "        [ 1.2128e-01],\n",
            "        [-1.4031e-01],\n",
            "        [-7.1208e-02],\n",
            "        [-7.8238e-02],\n",
            "        [ 6.1821e-02],\n",
            "        [ 3.4450e-02],\n",
            "        [-6.8526e-02],\n",
            "        [-6.1203e-02],\n",
            "        [ 2.9461e-01],\n",
            "        [-1.4532e-01],\n",
            "        [-1.9329e-01],\n",
            "        [-1.2396e-01],\n",
            "        [-2.2172e-01],\n",
            "        [-1.9327e-01],\n",
            "        [-2.8176e-01],\n",
            "        [-3.5229e-01],\n",
            "        [-2.8136e-01],\n",
            "        [-3.1463e-02],\n",
            "        [-5.8661e-02],\n",
            "        [-3.7135e-02],\n",
            "        [-1.3117e-01],\n",
            "        [ 5.6692e-02],\n",
            "        [ 1.3008e-01],\n",
            "        [ 1.1243e-01],\n",
            "        [ 3.9483e-02],\n",
            "        [ 4.4990e-02],\n",
            "        [-9.5144e-02],\n",
            "        [-9.6041e-03],\n",
            "        [-2.0071e-01],\n",
            "        [-1.6048e-01],\n",
            "        [-1.4308e-01],\n",
            "        [-1.1205e-01],\n",
            "        [-1.2116e-01],\n",
            "        [-1.2600e-01],\n",
            "        [ 2.5403e-01],\n",
            "        [ 1.9982e-01],\n",
            "        [ 1.6643e-01],\n",
            "        [ 5.7563e-03],\n",
            "        [ 7.2257e-02],\n",
            "        [-1.7987e-03],\n",
            "        [ 2.9987e-02],\n",
            "        [ 1.2253e-01],\n",
            "        [-1.2754e-01],\n",
            "        [-1.5052e-02],\n",
            "        [ 2.7076e-02],\n",
            "        [ 4.3814e-02],\n",
            "        [ 1.3625e-03],\n",
            "        [ 2.9481e-03],\n",
            "        [ 3.6363e-01],\n",
            "        [ 2.1594e-01],\n",
            "        [ 1.0015e-01],\n",
            "        [ 3.6671e-02],\n",
            "        [ 1.0534e-01],\n",
            "        [ 1.1301e-01],\n",
            "        [ 3.4852e-01],\n",
            "        [ 6.9925e-02],\n",
            "        [ 1.0039e-01],\n",
            "        [ 3.0986e-01],\n",
            "        [ 3.1936e-01],\n",
            "        [ 3.0717e-01],\n",
            "        [ 2.4879e-01],\n",
            "        [ 1.9024e-01],\n",
            "        [ 2.1296e-01],\n",
            "        [ 1.8822e-02],\n",
            "        [ 6.3076e-02],\n",
            "        [ 2.7050e-01],\n",
            "        [ 4.3066e-01],\n",
            "        [ 2.7371e-01],\n",
            "        [ 4.4837e-01],\n",
            "        [ 3.0720e-01],\n",
            "        [ 4.7134e-01],\n",
            "        [ 1.3988e+00],\n",
            "        [ 1.1689e+00],\n",
            "        [ 7.0258e-02],\n",
            "        [ 3.9684e-02],\n",
            "        [ 4.0526e-02],\n",
            "        [ 1.4123e-01],\n",
            "        [ 1.5369e-01],\n",
            "        [ 1.0579e-01],\n",
            "        [ 1.1954e-01],\n",
            "        [ 1.3097e-01],\n",
            "        [ 4.0919e-01],\n",
            "        [ 4.0238e-01],\n",
            "        [ 3.6049e-01],\n",
            "        [ 3.8668e-01],\n",
            "        [ 4.2002e-01],\n",
            "        [ 3.0095e-01],\n",
            "        [ 3.3900e-01],\n",
            "        [ 3.5498e-01],\n",
            "        [ 3.1608e-01],\n",
            "        [ 3.7083e-01],\n",
            "        [ 3.3384e-01],\n",
            "        [ 2.2602e-01],\n",
            "        [ 3.4704e-01],\n",
            "        [ 3.0486e-01],\n",
            "        [ 3.1264e-01],\n",
            "        [ 3.5010e-01],\n",
            "        [ 4.7698e-01],\n",
            "        [ 7.8508e-01],\n",
            "        [ 2.8150e-01],\n",
            "        [ 1.0810e-01],\n",
            "        [ 2.5230e-01],\n",
            "        [ 3.7149e-01],\n",
            "        [ 4.3595e-01],\n",
            "        [ 9.0375e-01],\n",
            "        [ 5.4079e-01],\n",
            "        [ 5.9917e-01],\n",
            "        [ 7.6217e-01],\n",
            "        [ 6.9361e-01],\n",
            "        [ 5.3689e-01],\n",
            "        [ 3.4890e-01],\n",
            "        [ 4.7336e-01],\n",
            "        [ 5.3292e-01],\n",
            "        [ 5.7634e-01],\n",
            "        [ 6.2779e-01],\n",
            "        [ 5.8536e-01],\n",
            "        [ 5.8788e-01],\n",
            "        [ 5.8021e-01],\n",
            "        [ 6.1206e-01],\n",
            "        [ 4.8611e-01],\n",
            "        [ 5.7186e-01],\n",
            "        [ 5.7411e-01],\n",
            "        [ 6.5922e-01],\n",
            "        [ 7.0069e-01],\n",
            "        [ 7.3783e-01],\n",
            "        [ 7.1431e-01],\n",
            "        [ 8.8608e-01],\n",
            "        [ 8.4907e-01],\n",
            "        [ 1.0039e+00],\n",
            "        [ 9.5050e-01],\n",
            "        [-1.4678e-01],\n",
            "        [ 7.2070e-02],\n",
            "        [ 9.6009e-03],\n",
            "        [ 2.5608e-03],\n",
            "        [ 4.6143e-02],\n",
            "        [ 6.6159e-02],\n",
            "        [-1.9264e-01],\n",
            "        [-3.2014e-01],\n",
            "        [ 4.9287e-01],\n",
            "        [ 4.3295e-01],\n",
            "        [ 4.2514e-01],\n",
            "        [ 2.1277e-01],\n",
            "        [ 2.1179e-01],\n",
            "        [ 2.2262e-02],\n",
            "        [-3.1097e-01],\n",
            "        [-2.4219e-01],\n",
            "        [-2.2954e-01],\n",
            "        [ 4.0093e-01],\n",
            "        [ 1.5624e-01],\n",
            "        [ 7.7673e-03],\n",
            "        [-7.5016e-02],\n",
            "        [-2.7455e-01],\n",
            "        [ 3.9815e-01],\n",
            "        [ 1.7852e-01],\n",
            "        [ 6.1722e-01],\n",
            "        [ 6.0413e-01],\n",
            "        [-7.9331e-02],\n",
            "        [-1.7137e-01],\n",
            "        [-1.1390e-01],\n",
            "        [-8.6613e-02],\n",
            "        [-2.1100e-01],\n",
            "        [ 1.3876e-01],\n",
            "        [ 2.2882e-01],\n",
            "        [ 2.1264e-01],\n",
            "        [ 2.1443e-01],\n",
            "        [ 1.7762e-01],\n",
            "        [ 2.4126e-01],\n",
            "        [-2.1912e-01],\n",
            "        [-1.5338e-02],\n",
            "        [-4.8578e-03],\n",
            "        [ 5.5388e-02],\n",
            "        [ 3.7622e-02],\n",
            "        [ 1.8955e-01],\n",
            "        [ 1.4367e-01],\n",
            "        [ 1.4929e-01],\n",
            "        [ 1.7937e+00],\n",
            "        [-7.3173e-02],\n",
            "        [ 1.2894e+00],\n",
            "        [ 7.4924e-01],\n",
            "        [ 7.2148e-01],\n",
            "        [ 5.2458e-01],\n",
            "        [ 5.1219e-03],\n",
            "        [ 2.1611e-02],\n",
            "        [ 3.7215e-01],\n",
            "        [-9.4770e-02],\n",
            "        [ 3.4077e-01],\n",
            "        [ 1.1260e+00],\n",
            "        [ 8.1143e-01],\n",
            "        [ 1.1333e+00],\n",
            "        [ 4.8578e-01],\n",
            "        [ 5.8856e-01],\n",
            "        [ 4.7762e-01],\n",
            "        [-6.4337e-01],\n",
            "        [-6.3909e-01],\n",
            "        [-4.9074e-01],\n",
            "        [-4.9811e-01],\n",
            "        [-6.3166e-01],\n",
            "        [-5.7317e-01],\n",
            "        [-4.9925e-01],\n",
            "        [-5.2075e-01],\n",
            "        [-5.6708e-01],\n",
            "        [-6.9547e-01],\n",
            "        [-6.4373e-01],\n",
            "        [-6.0048e-01],\n",
            "        [-2.8153e-01],\n",
            "        [-4.8824e-01],\n",
            "        [-3.7375e-01],\n",
            "        [-3.5966e-01],\n",
            "        [-5.4189e-01],\n",
            "        [-6.0434e-01],\n",
            "        [-5.3504e-01],\n",
            "        [-5.0635e-01],\n",
            "        [-3.8059e-01],\n",
            "        [-4.1372e-01],\n",
            "        [-2.9875e-01],\n",
            "        [-3.3212e-01],\n",
            "        [-2.2411e-01],\n",
            "        [-5.7264e-01],\n",
            "        [-1.1403e-01],\n",
            "        [-2.2809e-01],\n",
            "        [-3.7428e-01],\n",
            "        [-2.0053e-01],\n",
            "        [ 2.1139e-01],\n",
            "        [ 9.0297e-02],\n",
            "        [-1.2432e+00],\n",
            "        [-1.2166e+00],\n",
            "        [-1.2052e+00],\n",
            "        [-9.5919e-01],\n",
            "        [-1.1162e+00],\n",
            "        [-1.3245e+00],\n",
            "        [-1.2919e+00],\n",
            "        [-8.7542e-01],\n",
            "        [-9.3173e-01],\n",
            "        [-1.0371e+00],\n",
            "        [-5.4919e-01],\n",
            "        [-5.8912e-01],\n",
            "        [-5.7728e-01],\n",
            "        [-4.0738e-01],\n",
            "        [-3.8582e-01],\n",
            "        [-4.6927e-01],\n",
            "        [-4.4366e-01],\n",
            "        [-3.2287e-01],\n",
            "        [-3.4811e-01],\n",
            "        [-4.3450e-01],\n",
            "        [-3.8189e-01],\n",
            "        [-3.4344e-01],\n",
            "        [-3.6282e-01],\n",
            "        [-3.1470e-01],\n",
            "        [-2.2695e-01],\n",
            "        [-2.7130e-01],\n",
            "        [-1.9120e-01],\n",
            "        [-1.4709e-01],\n",
            "        [-1.2352e-01],\n",
            "        [-1.2349e-01],\n",
            "        [-9.7420e-02],\n",
            "        [-2.3428e-01],\n",
            "        [-3.4686e-01],\n",
            "        [-2.4596e-01],\n",
            "        [-2.3992e-01],\n",
            "        [-1.4857e-01],\n",
            "        [-2.0655e-01],\n",
            "        [-1.0321e-01],\n",
            "        [-9.2494e-02],\n",
            "        [-1.2966e-01],\n",
            "        [-5.3042e-02],\n",
            "        [-3.2120e-01],\n",
            "        [-4.6673e-01],\n",
            "        [-1.3624e+00],\n",
            "        [-6.7848e-01],\n",
            "        [-7.0117e-01],\n",
            "        [-8.3642e-01],\n",
            "        [-7.8780e-01],\n",
            "        [-4.8485e-01],\n",
            "        [-6.9636e-01],\n",
            "        [-8.3581e-01],\n",
            "        [-1.0091e+00],\n",
            "        [-7.9047e-01],\n",
            "        [-1.4637e+00],\n",
            "        [-1.4835e+00],\n",
            "        [-1.4297e+00],\n",
            "        [-1.4937e+00],\n",
            "        [-1.4580e+00],\n",
            "        [-1.4468e+00],\n",
            "        [-1.4140e+00],\n",
            "        [-1.4232e+00],\n",
            "        [-1.3890e+00],\n",
            "        [-1.1758e+00],\n",
            "        [-1.2258e+00],\n",
            "        [-6.9352e-01],\n",
            "        [-8.5029e-01],\n",
            "        [-7.0795e-01],\n",
            "        [-1.3637e+00],\n",
            "        [-1.3256e+00],\n",
            "        [-1.3224e+00],\n",
            "        [-1.2645e+00],\n",
            "        [-1.2212e+00],\n",
            "        [-1.2633e+00],\n",
            "        [-9.7311e-01],\n",
            "        [-1.1582e+00],\n",
            "        [-1.2292e+00],\n",
            "        [-1.2245e+00],\n",
            "        [-1.0718e+00],\n",
            "        [-9.9495e-01],\n",
            "        [-1.0774e+00],\n",
            "        [-1.0744e+00],\n",
            "        [-6.8539e-01],\n",
            "        [-3.0080e-01],\n",
            "        [-3.7265e-01],\n",
            "        [ 5.7412e-02],\n",
            "        [-7.4416e-01],\n",
            "        [-1.6223e-01],\n",
            "        [-9.5687e-01],\n",
            "        [-6.9832e-01],\n",
            "        [-6.5901e-01],\n",
            "        [-4.8953e-01],\n",
            "        [-1.0954e+00],\n",
            "        [-7.7762e-01],\n",
            "        [-7.0908e-01],\n",
            "        [-5.8101e-01],\n",
            "        [-3.7929e-01],\n",
            "        [-4.2365e-01],\n",
            "        [-3.2592e-01],\n",
            "        [-3.0830e-01],\n",
            "        [-2.9826e-01],\n",
            "        [-2.3086e-01],\n",
            "        [-3.0876e-01],\n",
            "        [ 1.7352e-02],\n",
            "        [-2.6420e-01],\n",
            "        [-2.3876e-01],\n",
            "        [ 2.0012e-02],\n",
            "        [ 1.6300e-01],\n",
            "        [ 1.9306e-01],\n",
            "        [ 1.9828e-01],\n",
            "        [ 1.9064e-01],\n",
            "        [ 2.0784e-01],\n",
            "        [ 8.8924e-02],\n",
            "        [ 1.8721e-01],\n",
            "        [ 1.3959e-01],\n",
            "        [ 1.2038e-01],\n",
            "        [ 2.2866e-01],\n",
            "        [ 2.1794e-01],\n",
            "        [ 2.1456e-01],\n",
            "        [ 2.3317e-01],\n",
            "        [-9.1024e-01],\n",
            "        [-8.5173e-01],\n",
            "        [-4.5445e-01],\n",
            "        [-8.0394e-01],\n",
            "        [-7.5701e-01],\n",
            "        [-5.7144e-01],\n",
            "        [-2.5919e-01],\n",
            "        [-4.1884e-01],\n",
            "        [ 1.3580e-01],\n",
            "        [ 1.9231e-01],\n",
            "        [ 3.7086e-01],\n",
            "        [ 2.8985e-01],\n",
            "        [-1.0968e+00],\n",
            "        [-1.3386e+00],\n",
            "        [ 3.4612e-01],\n",
            "        [ 2.9759e-01],\n",
            "        [ 7.0148e-01],\n",
            "        [ 7.6463e-01],\n",
            "        [ 8.2776e-01],\n",
            "        [ 1.0432e+00],\n",
            "        [-9.5290e-01],\n",
            "        [-9.9506e-01],\n",
            "        [-9.9918e-01],\n",
            "        [-5.5627e-01],\n",
            "        [-3.6711e-01],\n",
            "        [ 7.5953e-01],\n",
            "        [ 7.5410e-01],\n",
            "        [ 2.5771e-01],\n",
            "        [ 7.9408e-01],\n",
            "        [ 4.7171e-01],\n",
            "        [ 5.3967e-01],\n",
            "        [ 6.4349e-01],\n",
            "        [ 9.6935e-01],\n",
            "        [ 9.7349e-01],\n",
            "        [ 7.8110e-01],\n",
            "        [ 9.2414e-01],\n",
            "        [ 3.6117e-01],\n",
            "        [ 5.8035e-01],\n",
            "        [ 5.7939e-01],\n",
            "        [-3.0036e-01],\n",
            "        [ 3.3518e-01],\n",
            "        [ 3.5553e-01],\n",
            "        [ 1.9211e-01],\n",
            "        [ 2.1548e-01],\n",
            "        [-1.2060e+00],\n",
            "        [-8.8612e-01],\n",
            "        [-7.7986e-01],\n",
            "        [-7.8533e-01],\n",
            "        [-6.3614e-01],\n",
            "        [-6.6300e-01],\n",
            "        [-5.4285e-01],\n",
            "        [-4.7487e-01],\n",
            "        [-7.7149e-01],\n",
            "        [-6.1195e-01],\n",
            "        [-5.6178e-01],\n",
            "        [-5.8005e-01],\n",
            "        [-2.5577e-01],\n",
            "        [-3.2363e-01],\n",
            "        [-2.1333e-01],\n",
            "        [-7.2501e-01],\n",
            "        [-4.7295e-01],\n",
            "        [-4.8652e-01],\n",
            "        [-4.6564e-01],\n",
            "        [-6.0962e-01],\n",
            "        [-5.6932e-01],\n",
            "        [-8.5497e-01],\n",
            "        [-9.0491e-01],\n",
            "        [-1.0992e+00],\n",
            "        [-7.1801e-01],\n",
            "        [-7.8059e-01],\n",
            "        [-6.8575e-01],\n",
            "        [-6.1048e-01],\n",
            "        [-6.8028e-01],\n",
            "        [-8.6385e-01],\n",
            "        [-1.0885e+00],\n",
            "        [-8.2563e-01],\n",
            "        [-9.5393e-01],\n",
            "        [-3.1541e-01],\n",
            "        [-5.2704e-01],\n",
            "        [-9.6002e-01],\n",
            "        [-5.9892e-01],\n",
            "        [-6.6149e-01],\n",
            "        [-8.7202e-01],\n",
            "        [-9.0808e-01],\n",
            "        [-8.4473e-01],\n",
            "        [-7.6417e-01],\n",
            "        [-8.4386e-01],\n",
            "        [-8.5720e-01],\n",
            "        [-3.0030e-01],\n",
            "        [-8.8958e-01],\n",
            "        [-1.2215e+00],\n",
            "        [-7.5440e-01],\n",
            "        [-1.3556e+00],\n",
            "        [-1.1444e+00],\n",
            "        [-2.2126e-01],\n",
            "        [-1.0932e+00],\n",
            "        [-1.0849e+00],\n",
            "        [-1.3087e+00],\n",
            "        [-1.2972e+00],\n",
            "        [-1.2841e+00],\n",
            "        [-9.6669e-01],\n",
            "        [-1.4094e+00],\n",
            "        [-1.3192e+00],\n",
            "        [-9.7367e-01],\n",
            "        [-1.3982e+00],\n",
            "        [-7.8576e-01],\n",
            "        [-9.7946e-01],\n",
            "        [-1.2083e+00],\n",
            "        [-1.0818e+00],\n",
            "        [-9.7737e-01],\n",
            "        [-9.7582e-01],\n",
            "        [-9.6610e-01],\n",
            "        [-1.2371e+00],\n",
            "        [-1.2366e+00],\n",
            "        [-1.3663e+00],\n",
            "        [-1.3753e+00],\n",
            "        [-1.1009e+00],\n",
            "        [-1.4131e+00],\n",
            "        [-1.3609e+00],\n",
            "        [-1.3068e+00],\n",
            "        [-9.2737e-01],\n",
            "        [-1.1001e+00],\n",
            "        [-8.2904e-01],\n",
            "        [-1.4252e+00],\n",
            "        [-1.2243e+00],\n",
            "        [-1.7523e+00],\n",
            "        [-1.7498e+00],\n",
            "        [-1.8361e+00],\n",
            "        [-1.8608e+00],\n",
            "        [-1.9012e+00],\n",
            "        [-1.6112e+00],\n",
            "        [-1.6143e+00],\n",
            "        [-1.0173e+00],\n",
            "        [-1.3732e+00],\n",
            "        [-5.6825e-01],\n",
            "        [-6.6527e-01],\n",
            "        [-1.1683e+00],\n",
            "        [-5.2171e-01],\n",
            "        [-1.0519e+00],\n",
            "        [-1.0019e+00],\n",
            "        [-9.6289e-01],\n",
            "        [-7.8950e-01],\n",
            "        [-1.0285e+00],\n",
            "        [-3.7926e-01],\n",
            "        [-1.0678e+00],\n",
            "        [-1.3408e+00],\n",
            "        [-4.4806e-01],\n",
            "        [-1.2595e+00],\n",
            "        [-1.2932e+00],\n",
            "        [-1.1465e+00],\n",
            "        [-1.4822e+00],\n",
            "        [-4.0389e-01],\n",
            "        [-5.5743e-02],\n",
            "        [-1.6470e+00],\n",
            "        [-2.5284e-01],\n",
            "        [-1.9272e-01],\n",
            "        [-1.5477e+00],\n",
            "        [-1.5278e+00],\n",
            "        [-2.5884e-01],\n",
            "        [-8.8366e-01],\n",
            "        [ 4.1275e-01],\n",
            "        [ 4.1657e-01],\n",
            "        [-4.8771e-01],\n",
            "        [ 2.2352e-01],\n",
            "        [ 6.0960e-01],\n",
            "        [ 1.1411e+00],\n",
            "        [ 1.1131e+00],\n",
            "        [ 8.1645e-01],\n",
            "        [ 7.5215e-01],\n",
            "        [ 9.2533e-01],\n",
            "        [ 1.2843e+00],\n",
            "        [ 4.8307e-01],\n",
            "        [ 1.1744e-01],\n",
            "        [ 2.1000e-01],\n",
            "        [ 3.0715e+00],\n",
            "        [ 2.9093e+00],\n",
            "        [ 2.5467e+00],\n",
            "        [ 1.6460e+00],\n",
            "        [ 6.5488e-01],\n",
            "        [ 1.5307e+00],\n",
            "        [ 2.6144e+00],\n",
            "        [ 4.8664e+00],\n",
            "        [ 5.7467e+00],\n",
            "        [ 4.2602e+00],\n",
            "        [ 1.9094e+00],\n",
            "        [ 1.8317e+00],\n",
            "        [ 3.9371e+00],\n",
            "        [ 3.1233e-01],\n",
            "        [ 1.9655e+00],\n",
            "        [ 2.1246e+00],\n",
            "        [ 1.1495e+00],\n",
            "        [ 1.7837e+00],\n",
            "        [ 1.9053e+00],\n",
            "        [ 1.9502e+00],\n",
            "        [ 1.9533e+00],\n",
            "        [ 3.0723e+00],\n",
            "        [ 2.6313e+00],\n",
            "        [ 2.5399e+00],\n",
            "        [ 1.6400e+00],\n",
            "        [ 2.5932e+00],\n",
            "        [ 3.0138e+00],\n",
            "        [ 2.9109e+00],\n",
            "        [ 1.8152e+00],\n",
            "        [ 2.3416e+00],\n",
            "        [ 4.0819e+00],\n",
            "        [ 3.4839e+00],\n",
            "        [ 1.7838e-01],\n",
            "        [-1.1730e+00],\n",
            "        [ 3.3279e-01],\n",
            "        [ 4.7101e-01],\n",
            "        [-6.1320e-02],\n",
            "        [-1.0477e-01],\n",
            "        [-1.0164e+00],\n",
            "        [-2.7739e-01],\n",
            "        [ 1.1220e-01],\n",
            "        [ 1.3845e-01],\n",
            "        [-8.1292e-01],\n",
            "        [ 9.0878e-01],\n",
            "        [ 3.7664e-01],\n",
            "        [-4.7844e-01],\n",
            "        [-4.6650e-01],\n",
            "        [-7.2483e-01],\n",
            "        [-4.6353e-01],\n",
            "        [-1.3103e+00],\n",
            "        [-1.3427e+00],\n",
            "        [-3.9185e-01],\n",
            "        [ 1.1444e-01],\n",
            "        [-3.2644e-01],\n",
            "        [-2.0038e-01],\n",
            "        [ 1.8528e-01],\n",
            "        [ 2.9197e-01],\n",
            "        [-5.6323e-01],\n",
            "        [-2.0182e-01],\n",
            "        [ 5.9887e-01],\n",
            "        [-1.6660e-01],\n",
            "        [ 3.6383e-01],\n",
            "        [-9.6371e-01],\n",
            "        [ 1.8418e+00],\n",
            "        [ 3.1924e+00],\n",
            "        [ 2.9441e+00],\n",
            "        [ 3.1164e+00],\n",
            "        [ 2.7894e+00],\n",
            "        [ 2.6045e+00],\n",
            "        [ 9.6173e-01],\n",
            "        [ 2.0692e+00],\n",
            "        [ 1.8260e+00],\n",
            "        [ 4.7018e-01],\n",
            "        [ 2.2417e+00],\n",
            "        [ 2.4076e+00],\n",
            "        [ 2.5365e+00],\n",
            "        [ 1.8545e+00],\n",
            "        [ 2.2249e+00],\n",
            "        [ 3.3318e+00],\n",
            "        [ 1.2967e+00],\n",
            "        [ 1.7648e+00],\n",
            "        [ 2.7911e+00],\n",
            "        [ 3.1467e+00],\n",
            "        [ 1.8499e+00],\n",
            "        [ 2.0266e+00],\n",
            "        [ 1.8231e-01],\n",
            "        [ 8.0916e-01],\n",
            "        [ 7.2271e-01],\n",
            "        [ 2.6663e+00],\n",
            "        [-7.7256e-01],\n",
            "        [ 3.5358e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        # a. manual gradient calculation + explicit update\n",
        "        d = 1 # only one regressor.\n",
        "        w = torch.randn(1,d,requires_grad=True)\n",
        "        b = torch.randn(1,requires_grad=True)\n",
        "\n",
        "        for t in range(niter):\n",
        "            y_pred = torch.matmul(xT,w)+b # first dimension in xT is minibatch\n",
        "            # the statement above can be generalized to y = model(x)\n",
        "            err = (y_pred - y)\n",
        "            loss = err.pow(2.0).mean() # mean squared error\n",
        "            print(f\"iteration {t}, loss is: {loss}, w is: {float(w)}, b is {float(b)}\")\n",
        "            prod = err * xT\n",
        "            w_grad = 2.0*prod.mean()\n",
        "            b_grad = 2.0*err.mean()\n",
        "            with torch.no_grad():\n",
        "                w = w - learning_rate * w_grad # you can also use prod.mean()\n",
        "                b = b - learning_rate * b_grad # you can also use err.mean()"
      ],
      "metadata": {
        "id": "9l_HeLxaHGjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eaf556c-88d6-4038-f68f-40ee68ac8b31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0, loss is: 3.4788260459899902, w is: -0.3764531910419464, b is -1.2908025979995728\n",
            "iteration 1, loss is: 3.4136946201324463, w is: -0.3637773394584656, b is -1.2778946161270142\n",
            "iteration 2, loss is: 3.3498589992523193, w is: -0.351228266954422, b is -1.2651156187057495\n",
            "iteration 3, loss is: 3.2872941493988037, w is: -0.338804692029953, b is -1.2524644136428833\n",
            "iteration 4, loss is: 3.2259740829467773, w is: -0.3265053331851959, b is -1.23993980884552\n",
            "iteration 5, loss is: 3.1658740043640137, w is: -0.31432896852493286, b is -1.227540373802185\n",
            "iteration 6, loss is: 3.1069703102111816, w is: -0.3022743761539459, b is -1.215264916419983\n",
            "iteration 7, loss is: 3.049238443374634, w is: -0.2903403341770172, b is -1.203112244606018\n",
            "iteration 8, loss is: 2.9926562309265137, w is: -0.27852562069892883, b is -1.191081166267395\n",
            "iteration 9, loss is: 2.937199592590332, w is: -0.26682907342910767, b is -1.1791703701019287\n",
            "iteration 10, loss is: 2.8828463554382324, w is: -0.2552494704723358, b is -1.1673786640167236\n",
            "iteration 11, loss is: 2.8295748233795166, w is: -0.24378566443920135, b is -1.1557048559188843\n",
            "iteration 12, loss is: 2.7773633003234863, w is: -0.23243650794029236, b is -1.1441477537155151\n",
            "iteration 13, loss is: 2.7261910438537598, w is: -0.2212008386850357, b is -1.1327062845230103\n",
            "iteration 14, loss is: 2.676037073135376, w is: -0.21007752418518066, b is -1.1213792562484741\n",
            "iteration 15, loss is: 2.6268811225891113, w is: -0.1990654468536377, b is -1.1101654767990112\n",
            "iteration 16, loss is: 2.5787034034729004, w is: -0.18816348910331726, b is -1.0990638732910156\n",
            "iteration 17, loss is: 2.531484603881836, w is: -0.17737054824829102, b is -1.0880732536315918\n",
            "iteration 18, loss is: 2.4852049350738525, w is: -0.1666855365037918, b is -1.0771925449371338\n",
            "iteration 19, loss is: 2.4398465156555176, w is: -0.15610738098621368, b is -1.0664206743240356\n",
            "iteration 20, loss is: 2.395390748977661, w is: -0.14563500881195068, b is -1.0557564496994019\n",
            "iteration 21, loss is: 2.3518197536468506, w is: -0.13526736199855804, b is -1.045198917388916\n",
            "iteration 22, loss is: 2.3091156482696533, w is: -0.125003382563591, b is -1.0347468852996826\n",
            "iteration 23, loss is: 2.267261505126953, w is: -0.11484204232692719, b is -1.0243993997573853\n",
            "iteration 24, loss is: 2.2262396812438965, w is: -0.10478232055902481, b is -1.014155387878418\n",
            "iteration 25, loss is: 2.186034679412842, w is: -0.0948231965303421, b is -1.0040137767791748\n",
            "iteration 26, loss is: 2.146629810333252, w is: -0.08496366441249847, b is -0.9939736127853394\n",
            "iteration 27, loss is: 2.108008861541748, w is: -0.07520272582769394, b is -0.9840338826179504\n",
            "iteration 28, loss is: 2.0701568126678467, w is: -0.0655393972992897, b is -0.9741935729980469\n",
            "iteration 29, loss is: 2.033057689666748, w is: -0.05597269907593727, b is -0.9644516110420227\n",
            "iteration 30, loss is: 1.996696949005127, w is: -0.04650166630744934, b is -0.9548071026802063\n",
            "iteration 31, loss is: 1.9610599279403687, w is: -0.037125345319509506, b is -0.9452590346336365\n",
            "iteration 32, loss is: 1.9261319637298584, w is: -0.02784278802573681, b is -0.9358064532279968\n",
            "iteration 33, loss is: 1.8918992280960083, w is: -0.018653053790330887, b is -0.926448404788971\n",
            "iteration 34, loss is: 1.8583475351333618, w is: -0.009555218741297722, b is -0.9171839356422424\n",
            "iteration 35, loss is: 1.8254634141921997, w is: -0.0005483618006110191, b is -0.9080120921134949\n",
            "iteration 36, loss is: 1.7932339906692505, w is: 0.008368426002562046, b is -0.8989319801330566\n",
            "iteration 37, loss is: 1.761646032333374, w is: 0.0171960461884737, b is -0.8899426460266113\n",
            "iteration 38, loss is: 1.7306859493255615, w is: 0.02593538910150528, b is -0.8810431957244873\n",
            "iteration 39, loss is: 1.7003426551818848, w is: 0.03458733856678009, b is -0.8722327947616577\n",
            "iteration 40, loss is: 1.6706031560897827, w is: 0.04315277189016342, b is -0.8635104894638062\n",
            "iteration 41, loss is: 1.6414551734924316, w is: 0.05163254961371422, b is -0.854875385761261\n",
            "iteration 42, loss is: 1.6128873825073242, w is: 0.060027528554201126, b is -0.8463266491889954\n",
            "iteration 43, loss is: 1.584888219833374, w is: 0.0683385580778122, b is -0.8378633856773376\n",
            "iteration 44, loss is: 1.557446002960205, w is: 0.07656647264957428, b is -0.829484760761261\n",
            "iteration 45, loss is: 1.5305500030517578, w is: 0.08471211045980453, b is -0.8211899399757385\n",
            "iteration 46, loss is: 1.504189133644104, w is: 0.09277629107236862, b is -0.8129780292510986\n",
            "iteration 47, loss is: 1.4783527851104736, w is: 0.1007598340511322, b is -0.8048482537269592\n",
            "iteration 48, loss is: 1.4530307054519653, w is: 0.10866354405879974, b is -0.7967997789382935\n",
            "iteration 49, loss is: 1.4282126426696777, w is: 0.11648821085691452, b is -0.7888317704200745\n",
            "iteration 50, loss is: 1.403888463973999, w is: 0.1242346316576004, b is -0.7809434533119202\n",
            "iteration 51, loss is: 1.3800480365753174, w is: 0.13190358877182007, b is -0.7731339931488037\n",
            "iteration 52, loss is: 1.356682300567627, w is: 0.139495849609375, b is -0.7654026746749878\n",
            "iteration 53, loss is: 1.3337814807891846, w is: 0.14701218903064728, b is -0.7577486634254456\n",
            "iteration 54, loss is: 1.311336636543274, w is: 0.1544533669948578, b is -0.7501711845397949\n",
            "iteration 55, loss is: 1.2893378734588623, w is: 0.16182014346122742, b is -0.7426694631576538\n",
            "iteration 56, loss is: 1.2677773237228394, w is: 0.16911324858665466, b is -0.7352427840232849\n",
            "iteration 57, loss is: 1.2466456890106201, w is: 0.17633342742919922, b is -0.7278903722763062\n",
            "iteration 58, loss is: 1.225934624671936, w is: 0.1834813952445984, b is -0.7206114530563354\n",
            "iteration 59, loss is: 1.2056355476379395, w is: 0.19055788218975067, b is -0.7134053111076355\n",
            "iteration 60, loss is: 1.18574059009552, w is: 0.19756360352039337, b is -0.706271231174469\n",
            "iteration 61, loss is: 1.1662415266036987, w is: 0.2044992744922638, b is -0.6992084980010986\n",
            "iteration 62, loss is: 1.1471303701400757, w is: 0.21136558055877686, b is -0.6922163963317871\n",
            "iteration 63, loss is: 1.1283996105194092, w is: 0.21816322207450867, b is -0.6852942109107971\n",
            "iteration 64, loss is: 1.110041618347168, w is: 0.22489289939403534, b is -0.6784412860870361\n",
            "iteration 65, loss is: 1.0920488834381104, w is: 0.2315552681684494, b is -0.6716568470001221\n",
            "iteration 66, loss is: 1.0744142532348633, w is: 0.23815101385116577, b is -0.6649402976036072\n",
            "iteration 67, loss is: 1.0571305751800537, w is: 0.24468080699443817, b is -0.6582909226417542\n",
            "iteration 68, loss is: 1.0401909351348877, w is: 0.25114530324935913, b is -0.6517080068588257\n",
            "iteration 69, loss is: 1.0235881805419922, w is: 0.25754514336586, b is -0.645190954208374\n",
            "iteration 70, loss is: 1.007315993309021, w is: 0.26388099789619446, b is -0.6387390494346619\n",
            "iteration 71, loss is: 0.9913674592971802, w is: 0.2701534926891327, b is -0.6323516368865967\n",
            "iteration 72, loss is: 0.9757365584373474, w is: 0.2763632535934448, b is -0.6260281205177307\n",
            "iteration 73, loss is: 0.9604164958000183, w is: 0.2825109362602234, b is -0.6197678446769714\n",
            "iteration 74, loss is: 0.9454012513160706, w is: 0.28859713673591614, b is -0.6135701537132263\n",
            "iteration 75, loss is: 0.9306848645210266, w is: 0.2946224808692932, b is -0.6074344515800476\n",
            "iteration 76, loss is: 0.9162613749504089, w is: 0.30058756470680237, b is -0.6013600826263428\n",
            "iteration 77, loss is: 0.9021248817443848, w is: 0.30649298429489136, b is -0.5953465104103088\n",
            "iteration 78, loss is: 0.88826984167099, w is: 0.3123393654823303, b is -0.5893930196762085\n",
            "iteration 79, loss is: 0.8746903538703918, w is: 0.31812727451324463, b is -0.5834990739822388\n",
            "iteration 80, loss is: 0.8613810539245605, w is: 0.32385730743408203, b is -0.5776640772819519\n",
            "iteration 81, loss is: 0.8483368158340454, w is: 0.3295300304889679, b is -0.5718874335289001\n",
            "iteration 82, loss is: 0.8355519771575928, w is: 0.33514603972435, b is -0.5661685466766357\n",
            "iteration 83, loss is: 0.8230217099189758, w is: 0.34070587158203125, b is -0.5605068802833557\n",
            "iteration 84, loss is: 0.8107408285140991, w is: 0.3462101221084595, b is -0.5549018383026123\n",
            "iteration 85, loss is: 0.7987041473388672, w is: 0.3516593277454376, b is -0.5493528246879578\n",
            "iteration 86, loss is: 0.7869070768356323, w is: 0.3570540249347687, b is -0.5438593029975891\n",
            "iteration 87, loss is: 0.7753447890281677, w is: 0.362394779920578, b is -0.5384207367897034\n",
            "iteration 88, loss is: 0.7640126347541809, w is: 0.3676821291446686, b is -0.5330365300178528\n",
            "iteration 89, loss is: 0.7529059052467346, w is: 0.3729166090488434, b is -0.5277061462402344\n",
            "iteration 90, loss is: 0.7420202493667603, w is: 0.3780987560749054, b is -0.5224291086196899\n",
            "iteration 91, loss is: 0.7313511371612549, w is: 0.3832290768623352, b is -0.5172048211097717\n",
            "iteration 92, loss is: 0.7208943963050842, w is: 0.3883080780506134, b is -0.5120327472686768\n",
            "iteration 93, loss is: 0.7106457948684692, w is: 0.39333629608154297, b is -0.5069124102592468\n",
            "iteration 94, loss is: 0.7006009817123413, w is: 0.3983142375946045, b is -0.501843273639679\n",
            "iteration 95, loss is: 0.6907561421394348, w is: 0.40324240922927856, b is -0.49682483077049255\n",
            "iteration 96, loss is: 0.6811072826385498, w is: 0.4081212878227234, b is -0.49185657501220703\n",
            "iteration 97, loss is: 0.6716503500938416, w is: 0.41295138001441956, b is -0.4869379997253418\n",
            "iteration 98, loss is: 0.662381649017334, w is: 0.41773316264152527, b is -0.48206862807273865\n",
            "iteration 99, loss is: 0.6532974243164062, w is: 0.4224671423435211, b is -0.477247953414917\n",
            "iteration 100, loss is: 0.6443939208984375, w is: 0.4271537661552429, b is -0.47247546911239624\n",
            "iteration 101, loss is: 0.635667622089386, w is: 0.43179354071617126, b is -0.4677507281303406\n",
            "iteration 102, loss is: 0.6271150708198547, w is: 0.43638691306114197, b is -0.4630732238292694\n",
            "iteration 103, loss is: 0.6187325119972229, w is: 0.44093436002731323, b is -0.45844247937202454\n",
            "iteration 104, loss is: 0.6105169057846069, w is: 0.4454363286495209, b is -0.45385804772377014\n",
            "iteration 105, loss is: 0.6024647951126099, w is: 0.4498932659626007, b is -0.4493194818496704\n",
            "iteration 106, loss is: 0.594572901725769, w is: 0.45430564880371094, b is -0.44482627511024475\n",
            "iteration 107, loss is: 0.5868379473686218, w is: 0.458673894405365, b is -0.44037801027297974\n",
            "iteration 108, loss is: 0.5792571306228638, w is: 0.4629984498023987, b is -0.43597424030303955\n",
            "iteration 109, loss is: 0.5718269944190979, w is: 0.4672797620296478, b is -0.431614488363266\n",
            "iteration 110, loss is: 0.5645447969436646, w is: 0.47151827812194824, b is -0.42729833722114563\n",
            "iteration 111, loss is: 0.5574074983596802, w is: 0.47571438550949097, b is -0.42302533984184265\n",
            "iteration 112, loss is: 0.5504122376441956, w is: 0.4798685312271118, b is -0.41879507899284363\n",
            "iteration 113, loss is: 0.5435561537742615, w is: 0.4839811623096466, b is -0.41460713744163513\n",
            "iteration 114, loss is: 0.5368365049362183, w is: 0.4880526661872864, b is -0.41046106815338135\n",
            "iteration 115, loss is: 0.530250608921051, w is: 0.49208343029022217, b is -0.40635645389556885\n",
            "iteration 116, loss is: 0.5237957239151001, w is: 0.4960739016532898, b is -0.4022928774356842\n",
            "iteration 117, loss is: 0.5174694061279297, w is: 0.5000244379043579, b is -0.3982699513435364\n",
            "iteration 118, loss is: 0.5112689137458801, w is: 0.5039355158805847, b is -0.39428725838661194\n",
            "iteration 119, loss is: 0.5051917433738708, w is: 0.5078074932098389, b is -0.39034438133239746\n",
            "iteration 120, loss is: 0.4992355704307556, w is: 0.511640727519989, b is -0.3864409327507019\n",
            "iteration 121, loss is: 0.49339792132377625, w is: 0.5154356360435486, b is -0.38257652521133423\n",
            "iteration 122, loss is: 0.4876765012741089, w is: 0.5191925764083862, b is -0.3787507712841034\n",
            "iteration 123, loss is: 0.482068806886673, w is: 0.5229119658470154, b is -0.37496325373649597\n",
            "iteration 124, loss is: 0.47657281160354614, w is: 0.5265941619873047, b is -0.3712136149406433\n",
            "iteration 125, loss is: 0.4711861312389374, w is: 0.5302395224571228, b is -0.36750146746635437\n",
            "iteration 126, loss is: 0.46590664982795715, w is: 0.5338484048843384, b is -0.3638264536857605\n",
            "iteration 127, loss is: 0.46073228120803833, w is: 0.5374212265014648, b is -0.36018818616867065\n",
            "iteration 128, loss is: 0.4556608200073242, w is: 0.5409583449363708, b is -0.3565863072872162\n",
            "iteration 129, loss is: 0.45069026947021484, w is: 0.5444600582122803, b is -0.35302042961120605\n",
            "iteration 130, loss is: 0.4458186626434326, w is: 0.5479267835617065, b is -0.349490225315094\n",
            "iteration 131, loss is: 0.44104403257369995, w is: 0.5513588190078735, b is -0.34599533677101135\n",
            "iteration 132, loss is: 0.43636444211006165, w is: 0.5547565221786499, b is -0.3425353765487671\n",
            "iteration 133, loss is: 0.4317778944969177, w is: 0.5581202507019043, b is -0.33911001682281494\n",
            "iteration 134, loss is: 0.4272826910018921, w is: 0.5614503622055054, b is -0.33571892976760864\n",
            "iteration 135, loss is: 0.42287689447402954, w is: 0.564747154712677, b is -0.33236172795295715\n",
            "iteration 136, loss is: 0.418558806180954, w is: 0.5680109858512878, b is -0.3290381133556366\n",
            "iteration 137, loss is: 0.41432663798332214, w is: 0.5712421536445618, b is -0.3257477283477783\n",
            "iteration 138, loss is: 0.41017866134643555, w is: 0.5744410157203674, b is -0.32249024510383606\n",
            "iteration 139, loss is: 0.40611329674720764, w is: 0.5776079297065735, b is -0.31926533579826355\n",
            "iteration 140, loss is: 0.40212875604629517, w is: 0.580743134021759, b is -0.3160726726055145\n",
            "iteration 141, loss is: 0.39822354912757874, w is: 0.5838469862937927, b is -0.3129119575023651\n",
            "iteration 142, loss is: 0.394396036863327, w is: 0.5869198441505432, b is -0.30978283286094666\n",
            "iteration 143, loss is: 0.39064469933509827, w is: 0.5899619460105896, b is -0.3066850006580353\n",
            "iteration 144, loss is: 0.3869680166244507, w is: 0.5929736495018005, b is -0.3036181628704071\n",
            "iteration 145, loss is: 0.3833644688129425, w is: 0.5959551930427551, b is -0.30058199167251587\n",
            "iteration 146, loss is: 0.3798326551914215, w is: 0.598906934261322, b is -0.2975761592388153\n",
            "iteration 147, loss is: 0.3763711452484131, w is: 0.6018291711807251, b is -0.29460039734840393\n",
            "iteration 148, loss is: 0.3729785084724426, w is: 0.6047222018241882, b is -0.29165440797805786\n",
            "iteration 149, loss is: 0.3696534037590027, w is: 0.6075862646102905, b is -0.28873786330223083\n",
            "iteration 150, loss is: 0.3663944602012634, w is: 0.6104217171669006, b is -0.28585049510002136\n",
            "iteration 151, loss is: 0.3632003366947174, w is: 0.6132287979125977, b is -0.2829919755458832\n",
            "iteration 152, loss is: 0.36006978154182434, w is: 0.6160078048706055, b is -0.2801620662212372\n",
            "iteration 153, loss is: 0.35700154304504395, w is: 0.618759036064148, b is -0.2773604393005371\n",
            "iteration 154, loss is: 0.35399436950683594, w is: 0.6214827299118042, b is -0.27458682656288147\n",
            "iteration 155, loss is: 0.35104700922966003, w is: 0.6241791844367981, b is -0.2718409597873688\n",
            "iteration 156, loss is: 0.3481582999229431, w is: 0.6268486976623535, b is -0.26912254095077515\n",
            "iteration 157, loss is: 0.34532713890075684, w is: 0.6294915080070496, b is -0.2664313018321991\n",
            "iteration 158, loss is: 0.34255221486091614, w is: 0.6321079134941101, b is -0.26376697421073914\n",
            "iteration 159, loss is: 0.3398325741291046, w is: 0.6346981525421143, b is -0.2611292898654938\n",
            "iteration 160, loss is: 0.33716705441474915, w is: 0.6372624635696411, b is -0.2585180103778839\n",
            "iteration 161, loss is: 0.3345545530319214, w is: 0.6398011445999146, b is -0.25593283772468567\n",
            "iteration 162, loss is: 0.33199408650398254, w is: 0.6423144340515137, b is -0.25337350368499756\n",
            "iteration 163, loss is: 0.3294845223426819, w is: 0.6448025703430176, b is -0.2508397698402405\n",
            "iteration 164, loss is: 0.3270249366760254, w is: 0.6472658514976501, b is -0.24833136796951294\n",
            "iteration 165, loss is: 0.3246142566204071, w is: 0.6497045159339905, b is -0.24584805965423584\n",
            "iteration 166, loss is: 0.3222516179084778, w is: 0.6521188020706177, b is -0.2433895766735077\n",
            "iteration 167, loss is: 0.3199358880519867, w is: 0.6545089483261108, b is -0.2409556806087494\n",
            "iteration 168, loss is: 0.31766635179519653, w is: 0.6568751335144043, b is -0.23854611814022064\n",
            "iteration 169, loss is: 0.31544193625450134, w is: 0.6592177152633667, b is -0.23616065084934235\n",
            "iteration 170, loss is: 0.3132617473602295, w is: 0.6615368127822876, b is -0.2337990403175354\n",
            "iteration 171, loss is: 0.3111250102519989, w is: 0.6638327240943909, b is -0.2314610481262207\n",
            "iteration 172, loss is: 0.30903077125549316, w is: 0.6661056876182556, b is -0.22914643585681915\n",
            "iteration 173, loss is: 0.30697816610336304, w is: 0.6683559417724609, b is -0.22685496509075165\n",
            "iteration 174, loss is: 0.30496644973754883, w is: 0.6705836653709412, b is -0.22458641231060028\n",
            "iteration 175, loss is: 0.3029947876930237, w is: 0.6727891564369202, b is -0.22234055399894714\n",
            "iteration 176, loss is: 0.30106228590011597, w is: 0.6749725937843323, b is -0.22011715173721313\n",
            "iteration 177, loss is: 0.29916831851005554, w is: 0.6771341562271118, b is -0.21791598200798035\n",
            "iteration 178, loss is: 0.29731202125549316, w is: 0.6792741417884827, b is -0.21573682129383087\n",
            "iteration 179, loss is: 0.29549261927604675, w is: 0.6813927292823792, b is -0.2135794460773468\n",
            "iteration 180, loss is: 0.2937094569206238, w is: 0.6834900975227356, b is -0.21144364774227142\n",
            "iteration 181, loss is: 0.29196178913116455, w is: 0.6855664849281311, b is -0.20932921767234802\n",
            "iteration 182, loss is: 0.29024890065193176, w is: 0.6876221299171448, b is -0.2072359323501587\n",
            "iteration 183, loss is: 0.2885700762271881, w is: 0.6896572113037109, b is -0.20516356825828552\n",
            "iteration 184, loss is: 0.28692466020584106, w is: 0.6916719675064087, b is -0.203111931681633\n",
            "iteration 185, loss is: 0.2853119671344757, w is: 0.6936665773391724, b is -0.2010808140039444\n",
            "iteration 186, loss is: 0.2837314307689667, w is: 0.6956412196159363, b is -0.199070006608963\n",
            "iteration 187, loss is: 0.2821822762489319, w is: 0.6975961327552795, b is -0.19707930088043213\n",
            "iteration 188, loss is: 0.28066402673721313, w is: 0.6995314955711365, b is -0.19510850310325623\n",
            "iteration 189, loss is: 0.27917593717575073, w is: 0.7014474868774414, b is -0.19315741956233978\n",
            "iteration 190, loss is: 0.2777174711227417, w is: 0.7033443450927734, b is -0.1912258416414261\n",
            "iteration 191, loss is: 0.27628806233406067, w is: 0.7052221894264221, b is -0.1893135905265808\n",
            "iteration 192, loss is: 0.2748870551586151, w is: 0.7070812582969666, b is -0.18742045760154724\n",
            "iteration 193, loss is: 0.27351394295692444, w is: 0.7089217305183411, b is -0.18554624915122986\n",
            "iteration 194, loss is: 0.2721681594848633, w is: 0.7107438445091248, b is -0.18369078636169434\n",
            "iteration 195, loss is: 0.2708491384983063, w is: 0.7125477194786072, b is -0.18185387551784515\n",
            "iteration 196, loss is: 0.2695564031600952, w is: 0.7143335342407227, b is -0.18003533780574799\n",
            "iteration 197, loss is: 0.26828935742378235, w is: 0.7161015272140503, b is -0.1782349795103073\n",
            "iteration 198, loss is: 0.26704755425453186, w is: 0.7178518176078796, b is -0.17645263671875\n",
            "iteration 199, loss is: 0.26583045721054077, w is: 0.719584584236145, b is -0.17468811571598053\n",
            "iteration 200, loss is: 0.2646375596523285, w is: 0.7213000655174255, b is -0.17294123768806458\n",
            "iteration 201, loss is: 0.2634683847427368, w is: 0.7229983806610107, b is -0.1712118238210678\n",
            "iteration 202, loss is: 0.2623225152492523, w is: 0.724679708480835, b is -0.1694997102022171\n",
            "iteration 203, loss is: 0.2611994445323944, w is: 0.7263442277908325, b is -0.16780471801757812\n",
            "iteration 204, loss is: 0.26009872555732727, w is: 0.7279921174049377, b is -0.16612666845321655\n",
            "iteration 205, loss is: 0.2590199112892151, w is: 0.7296234965324402, b is -0.16446539759635925\n",
            "iteration 206, loss is: 0.25796255469322205, w is: 0.7312385439872742, b is -0.1628207415342331\n",
            "iteration 207, loss is: 0.2569262385368347, w is: 0.732837438583374, b is -0.16119253635406494\n",
            "iteration 208, loss is: 0.25591057538986206, w is: 0.7344203591346741, b is -0.15958061814308167\n",
            "iteration 209, loss is: 0.2549150884151459, w is: 0.7359874844551086, b is -0.15798480808734894\n",
            "iteration 210, loss is: 0.2539393901824951, w is: 0.7375389337539673, b is -0.15640495717525482\n",
            "iteration 211, loss is: 0.2529831528663635, w is: 0.7390748262405396, b is -0.15484090149402618\n",
            "iteration 212, loss is: 0.2520458996295929, w is: 0.7405954003334045, b is -0.1532924920320511\n",
            "iteration 213, loss is: 0.25112733244895935, w is: 0.7421007752418518, b is -0.1517595648765564\n",
            "iteration 214, loss is: 0.25022703409194946, w is: 0.7435910701751709, b is -0.15024197101593018\n",
            "iteration 215, loss is: 0.24934469163417816, w is: 0.7450664639472961, b is -0.1487395465373993\n",
            "iteration 216, loss is: 0.24847985804080963, w is: 0.7465270757675171, b is -0.147252157330513\n",
            "iteration 217, loss is: 0.24763227999210358, w is: 0.7479730844497681, b is -0.14577963948249817\n",
            "iteration 218, loss is: 0.24680152535438538, w is: 0.7494046688079834, b is -0.14432184398174286\n",
            "iteration 219, loss is: 0.24598729610443115, w is: 0.7508219480514526, b is -0.14287862181663513\n",
            "iteration 220, loss is: 0.24518929421901703, w is: 0.7522250413894653, b is -0.14144983887672424\n",
            "iteration 221, loss is: 0.24440719187259674, w is: 0.753614068031311, b is -0.14003534615039825\n",
            "iteration 222, loss is: 0.24364063143730164, w is: 0.7549892067909241, b is -0.13863499462604523\n",
            "iteration 223, loss is: 0.24288931488990784, w is: 0.7563506364822388, b is -0.13724865019321442\n",
            "iteration 224, loss is: 0.24215297400951385, w is: 0.7576984167098999, b is -0.13587616384029388\n",
            "iteration 225, loss is: 0.24143126606941223, w is: 0.7590327262878418, b is -0.13451740145683289\n",
            "iteration 226, loss is: 0.24072392284870148, w is: 0.760353684425354, b is -0.13317222893238068\n",
            "iteration 227, loss is: 0.2400306612253189, w is: 0.7616614699363708, b is -0.1318405121564865\n",
            "iteration 228, loss is: 0.23935121297836304, w is: 0.7629561424255371, b is -0.13052210211753845\n",
            "iteration 229, loss is: 0.2386852502822876, w is: 0.7642378807067871, b is -0.12921687960624695\n",
            "iteration 230, loss is: 0.23803256452083588, w is: 0.7655068039894104, b is -0.12792471051216125\n",
            "iteration 231, loss is: 0.237392857670784, w is: 0.7667630314826965, b is -0.12664546072483063\n",
            "iteration 232, loss is: 0.23676590621471405, w is: 0.7680066823959351, b is -0.12537901103496552\n",
            "iteration 233, loss is: 0.23615141212940216, w is: 0.7692379355430603, b is -0.12412521988153458\n",
            "iteration 234, loss is: 0.23554910719394684, w is: 0.770456850528717, b is -0.12288396805524826\n",
            "iteration 235, loss is: 0.23495884239673615, w is: 0.7716636061668396, b is -0.12165512889623642\n",
            "iteration 236, loss is: 0.23438027501106262, w is: 0.7728582620620728, b is -0.1204385757446289\n",
            "iteration 237, loss is: 0.23381327092647552, w is: 0.7740409970283508, b is -0.11923418939113617\n",
            "iteration 238, loss is: 0.23325753211975098, w is: 0.7752118706703186, b is -0.11804185062646866\n",
            "iteration 239, loss is: 0.23271283507347107, w is: 0.7763710618019104, b is -0.11686143279075623\n",
            "iteration 240, loss is: 0.23217900097370148, w is: 0.777518630027771, b is -0.11569281667470932\n",
            "iteration 241, loss is: 0.2316557765007019, w is: 0.7786547541618347, b is -0.11453589051961899\n",
            "iteration 242, loss is: 0.23114299774169922, w is: 0.7797794938087463, b is -0.11339053511619568\n",
            "iteration 243, loss is: 0.23064036667346954, w is: 0.7808930277824402, b is -0.11225663125514984\n",
            "iteration 244, loss is: 0.23014776408672333, w is: 0.781995415687561, b is -0.11113406717777252\n",
            "iteration 245, loss is: 0.22966496646404266, w is: 0.7830867767333984, b is -0.11002272367477417\n",
            "iteration 246, loss is: 0.22919178009033203, w is: 0.7841672301292419, b is -0.10892249643802643\n",
            "iteration 247, loss is: 0.2287280261516571, w is: 0.7852368354797363, b is -0.10783327370882034\n",
            "iteration 248, loss is: 0.22827346622943878, w is: 0.7862957715988159, b is -0.10675494372844696\n",
            "iteration 249, loss is: 0.22782795131206512, w is: 0.7873440980911255, b is -0.10568739473819733\n",
            "iteration 250, loss is: 0.2273913472890854, w is: 0.7883819341659546, b is -0.10463052242994308\n",
            "iteration 251, loss is: 0.22696338593959808, w is: 0.7894093990325928, b is -0.10358421504497528\n",
            "iteration 252, loss is: 0.22654394805431366, w is: 0.7904266119003296, b is -0.10254837572574615\n",
            "iteration 253, loss is: 0.22613291442394257, w is: 0.7914336323738098, b is -0.10152289271354675\n",
            "iteration 254, loss is: 0.22572997212409973, w is: 0.792430579662323, b is -0.10050766170024872\n",
            "iteration 255, loss is: 0.22533509135246277, w is: 0.7934175729751587, b is -0.09950258582830429\n",
            "iteration 256, loss is: 0.22494806349277496, w is: 0.7943947315216064, b is -0.09850756078958511\n",
            "iteration 257, loss is: 0.22456873953342438, w is: 0.795362114906311, b is -0.09752248227596283\n",
            "iteration 258, loss is: 0.22419695556163788, w is: 0.7963197827339172, b is -0.09654726088047028\n",
            "iteration 259, loss is: 0.22383256256580353, w is: 0.7972679138183594, b is -0.0955817848443985\n",
            "iteration 260, loss is: 0.22347545623779297, w is: 0.7982065677642822, b is -0.09462596476078033\n",
            "iteration 261, loss is: 0.2231254279613495, w is: 0.7991358041763306, b is -0.09367970377206802\n",
            "iteration 262, loss is: 0.22278237342834473, w is: 0.800055742263794, b is -0.0927429050207138\n",
            "iteration 263, loss is: 0.22244615852832794, w is: 0.8009665012359619, b is -0.09181547909975052\n",
            "iteration 264, loss is: 0.2221166044473648, w is: 0.8018681406974792, b is -0.0908973217010498\n",
            "iteration 265, loss is: 0.22179362177848816, w is: 0.8027607798576355, b is -0.0899883508682251\n",
            "iteration 266, loss is: 0.22147709131240845, w is: 0.8036444783210754, b is -0.08908846974372864\n",
            "iteration 267, loss is: 0.22116681933403015, w is: 0.8045193552970886, b is -0.08819758147001266\n",
            "iteration 268, loss is: 0.2208627462387085, w is: 0.8053854703903198, b is -0.08731560409069061\n",
            "iteration 269, loss is: 0.22056472301483154, w is: 0.8062429428100586, b is -0.08644244819879532\n",
            "iteration 270, loss is: 0.22027260065078735, w is: 0.8070918321609497, b is -0.08557802438735962\n",
            "iteration 271, loss is: 0.21998634934425354, w is: 0.8079321980476379, b is -0.08472224324941635\n",
            "iteration 272, loss is: 0.219705730676651, w is: 0.8087641596794128, b is -0.08387502282857895\n",
            "iteration 273, loss is: 0.21943072974681854, w is: 0.809587836265564, b is -0.08303627371788025\n",
            "iteration 274, loss is: 0.21916121244430542, w is: 0.8104032874107361, b is -0.08220591396093369\n",
            "iteration 275, loss is: 0.2188970446586609, w is: 0.811210572719574, b is -0.0813838541507721\n",
            "iteration 276, loss is: 0.21863813698291779, w is: 0.8120097517967224, b is -0.08057001233100891\n",
            "iteration 277, loss is: 0.21838440001010895, w is: 0.812800943851471, b is -0.07976431399583817\n",
            "iteration 278, loss is: 0.21813566982746124, w is: 0.8135842680931091, b is -0.0789666697382927\n",
            "iteration 279, loss is: 0.21789191663265228, w is: 0.814359724521637, b is -0.07817700505256653\n",
            "iteration 280, loss is: 0.21765300631523132, w is: 0.815127432346344, b is -0.0773952379822731\n",
            "iteration 281, loss is: 0.2174188643693924, w is: 0.815887451171875, b is -0.07662128657102585\n",
            "iteration 282, loss is: 0.21718935668468475, w is: 0.8166399002075195, b is -0.0758550763130188\n",
            "iteration 283, loss is: 0.216964453458786, w is: 0.8173847794532776, b is -0.07509652525186539\n",
            "iteration 284, loss is: 0.21674402058124542, w is: 0.8181222081184387, b is -0.07434555888175964\n",
            "iteration 285, loss is: 0.21652795374393463, w is: 0.8188523054122925, b is -0.0736021026968956\n",
            "iteration 286, loss is: 0.21631616353988647, w is: 0.8195750713348389, b is -0.07286608219146729\n",
            "iteration 287, loss is: 0.21610862016677856, w is: 0.8202906250953674, b is -0.07213742285966873\n",
            "iteration 288, loss is: 0.21590520441532135, w is: 0.820999026298523, b is -0.07141605019569397\n",
            "iteration 289, loss is: 0.21570584177970886, w is: 0.8217003345489502, b is -0.07070188969373703\n",
            "iteration 290, loss is: 0.21551042795181274, w is: 0.822394609451294, b is -0.06999487429857254\n",
            "iteration 291, loss is: 0.21531890332698822, w is: 0.8230819702148438, b is -0.06929492205381393\n",
            "iteration 292, loss is: 0.21513120830059052, w is: 0.8237624764442444, b is -0.06860197335481644\n",
            "iteration 293, loss is: 0.21494723856449127, w is: 0.8244361281394958, b is -0.06791595369577408\n",
            "iteration 294, loss is: 0.21476693451404572, w is: 0.8251030445098877, b is -0.06723679602146149\n",
            "iteration 295, loss is: 0.2145901918411255, w is: 0.8257633447647095, b is -0.06656442582607269\n",
            "iteration 296, loss is: 0.2144170105457306, w is: 0.8264170289039612, b is -0.06589878350496292\n",
            "iteration 297, loss is: 0.21424727141857147, w is: 0.8270641565322876, b is -0.0652397945523262\n",
            "iteration 298, loss is: 0.21408088505268097, w is: 0.8277048468589783, b is -0.06458739936351776\n",
            "iteration 299, loss is: 0.2139178067445755, w is: 0.8283390998840332, b is -0.06394152343273163\n",
            "iteration 300, loss is: 0.21375799179077148, w is: 0.8289670348167419, b is -0.06330210715532303\n",
            "iteration 301, loss is: 0.21360133588314056, w is: 0.8295886516571045, b is -0.06266908347606659\n",
            "iteration 302, loss is: 0.21344783902168274, w is: 0.8302040696144104, b is -0.062042392790317535\n",
            "iteration 303, loss is: 0.2132973074913025, w is: 0.8308133482933044, b is -0.061421968042850494\n",
            "iteration 304, loss is: 0.21314984560012817, w is: 0.8314165472984314, b is -0.06080774962902069\n",
            "iteration 305, loss is: 0.21300533413887024, w is: 0.8320136666297913, b is -0.06019967049360275\n",
            "iteration 306, loss is: 0.21286365389823914, w is: 0.8326048254966736, b is -0.0595976747572422\n",
            "iteration 307, loss is: 0.21272480487823486, w is: 0.8331900835037231, b is -0.05900169909000397\n",
            "iteration 308, loss is: 0.21258872747421265, w is: 0.8337695002555847, b is -0.05841168388724327\n",
            "iteration 309, loss is: 0.21245534718036652, w is: 0.8343431353569031, b is -0.05782756581902504\n",
            "iteration 310, loss is: 0.2123246192932129, w is: 0.8349109888076782, b is -0.0572492890059948\n",
            "iteration 311, loss is: 0.21219651401042938, w is: 0.8354731798171997, b is -0.056676797568798065\n",
            "iteration 312, loss is: 0.21207094192504883, w is: 0.8360297679901123, b is -0.05611002817749977\n",
            "iteration 313, loss is: 0.21194790303707123, w is: 0.836580753326416, b is -0.055548928678035736\n",
            "iteration 314, loss is: 0.21182726323604584, w is: 0.8371262550354004, b is -0.054993439465761185\n",
            "iteration 315, loss is: 0.21170903742313385, w is: 0.8376662731170654, b is -0.05444350466132164\n",
            "iteration 316, loss is: 0.2115931659936905, w is: 0.8382009267807007, b is -0.053899068385362625\n",
            "iteration 317, loss is: 0.21147960424423218, w is: 0.8387302160263062, b is -0.05336007848381996\n",
            "iteration 318, loss is: 0.21136833727359772, w is: 0.8392542004585266, b is -0.05282647907733917\n",
            "iteration 319, loss is: 0.21125923097133636, w is: 0.8397729396820068, b is -0.05229821428656578\n",
            "iteration 320, loss is: 0.21115230023860931, w is: 0.8402864933013916, b is -0.05177523195743561\n",
            "iteration 321, loss is: 0.21104754507541656, w is: 0.8407949209213257, b is -0.051257479935884476\n",
            "iteration 322, loss is: 0.21094481647014618, w is: 0.8412982821464539, b is -0.050744906067848206\n",
            "iteration 323, loss is: 0.21084414422512054, w is: 0.8417965769767761, b is -0.05023745819926262\n",
            "iteration 324, loss is: 0.21074549853801727, w is: 0.842289924621582, b is -0.04973508417606354\n",
            "iteration 325, loss is: 0.21064883470535278, w is: 0.8427783250808716, b is -0.04923773184418678\n",
            "iteration 326, loss is: 0.21055404841899872, w is: 0.8432618379592896, b is -0.048745352774858475\n",
            "iteration 327, loss is: 0.21046115458011627, w is: 0.8437405228614807, b is -0.04825789853930473\n",
            "iteration 328, loss is: 0.21037010848522186, w is: 0.8442144393920898, b is -0.04777532070875168\n",
            "iteration 329, loss is: 0.2102809101343155, w is: 0.8446835875511169, b is -0.04729756712913513\n",
            "iteration 330, loss is: 0.2101934552192688, w is: 0.8451480269432068, b is -0.04682459309697151\n",
            "iteration 331, loss is: 0.2101077288389206, w is: 0.8456078767776489, b is -0.04635634645819664\n",
            "iteration 332, loss is: 0.21002374589443207, w is: 0.8460630774497986, b is -0.045892782509326935\n",
            "iteration 333, loss is: 0.20994140207767487, w is: 0.8465137481689453, b is -0.045433856546878815\n",
            "iteration 334, loss is: 0.20986072719097137, w is: 0.8469598889350891, b is -0.0449795164167881\n",
            "iteration 335, loss is: 0.20978164672851562, w is: 0.8474016189575195, b is -0.04452972114086151\n",
            "iteration 336, loss is: 0.20970413088798523, w is: 0.8478388786315918, b is -0.044084422290325165\n",
            "iteration 337, loss is: 0.2096281498670578, w is: 0.8482717871665955, b is -0.04364357888698578\n",
            "iteration 338, loss is: 0.20955370366573334, w is: 0.8487003445625305, b is -0.04320714250206947\n",
            "iteration 339, loss is: 0.20948073267936707, w is: 0.8491246700286865, b is -0.042775072157382965\n",
            "iteration 340, loss is: 0.2094092071056366, w is: 0.8495447039604187, b is -0.04234732314944267\n",
            "iteration 341, loss is: 0.20933909714221954, w is: 0.8499605655670166, b is -0.041923850774765015\n",
            "iteration 342, loss is: 0.2092704027891159, w is: 0.8503722548484802, b is -0.04150461405515671\n",
            "iteration 343, loss is: 0.20920304954051971, w is: 0.8507798314094543, b is -0.04108956828713417\n",
            "iteration 344, loss is: 0.20913705229759216, w is: 0.8511833548545837, b is -0.04067867249250412\n",
            "iteration 345, loss is: 0.20907235145568848, w is: 0.8515828251838684, b is -0.04027188569307327\n",
            "iteration 346, loss is: 0.20900897681713104, w is: 0.8519783020019531, b is -0.039869166910648346\n",
            "iteration 347, loss is: 0.2089468240737915, w is: 0.8523698449134827, b is -0.039470475167036057\n",
            "iteration 348, loss is: 0.20888595283031464, w is: 0.852757453918457, b is -0.03907576948404312\n",
            "iteration 349, loss is: 0.2088262438774109, w is: 0.853141188621521, b is -0.038685012608766556\n",
            "iteration 350, loss is: 0.20876772701740265, w is: 0.8535211086273193, b is -0.03829816356301308\n",
            "iteration 351, loss is: 0.2087104171514511, w is: 0.853897213935852, b is -0.0379151813685894\n",
            "iteration 352, loss is: 0.2086542248725891, w is: 0.8542695641517639, b is -0.037536028772592545\n",
            "iteration 353, loss is: 0.20859913527965546, w is: 0.8546381592750549, b is -0.03716066852211952\n",
            "iteration 354, loss is: 0.20854516327381134, w is: 0.8550030589103699, b is -0.03678906336426735\n",
            "iteration 355, loss is: 0.208492249250412, w is: 0.8553643226623535, b is -0.03642117232084274\n",
            "iteration 356, loss is: 0.2084404081106186, w is: 0.8557220101356506, b is -0.03605696186423302\n",
            "iteration 357, loss is: 0.20838958024978638, w is: 0.8560761213302612, b is -0.03569639101624489\n",
            "iteration 358, loss is: 0.20833978056907654, w is: 0.8564266562461853, b is -0.03533942624926567\n",
            "iteration 359, loss is: 0.2082909494638443, w is: 0.8567736744880676, b is -0.03498603031039238\n",
            "iteration 360, loss is: 0.20824308693408966, w is: 0.857117235660553, b is -0.03463616967201233\n",
            "iteration 361, loss is: 0.208196222782135, w is: 0.8574573397636414, b is -0.034289807081222534\n",
            "iteration 362, loss is: 0.2081502377986908, w is: 0.8577940464019775, b is -0.03394690901041031\n",
            "iteration 363, loss is: 0.2081051915884018, w is: 0.8581274151802063, b is -0.03360743820667267\n",
            "iteration 364, loss is: 0.20806102454662323, w is: 0.8584574460983276, b is -0.033271364867687225\n",
            "iteration 365, loss is: 0.2080177664756775, w is: 0.8587841987609863, b is -0.032938651740550995\n",
            "iteration 366, loss is: 0.2079753577709198, w is: 0.8591076731681824, b is -0.03260926529765129\n",
            "iteration 367, loss is: 0.20793379843235016, w is: 0.8594279289245605, b is -0.03228317201137543\n",
            "iteration 368, loss is: 0.20789304375648499, w is: 0.8597449660301208, b is -0.031960342079401016\n",
            "iteration 369, loss is: 0.20785310864448547, w is: 0.8600588440895081, b is -0.03164073824882507\n",
            "iteration 370, loss is: 0.20781399309635162, w is: 0.8603695631027222, b is -0.03132433071732521\n",
            "iteration 371, loss is: 0.20777563750743866, w is: 0.860677182674408, b is -0.03101108781993389\n",
            "iteration 372, loss is: 0.2077380269765854, w is: 0.8609817028045654, b is -0.03070097602903843\n",
            "iteration 373, loss is: 0.207701176404953, w is: 0.8612831830978394, b is -0.030393965542316437\n",
            "iteration 374, loss is: 0.20766504108905792, w is: 0.8615816831588745, b is -0.030090026557445526\n",
            "iteration 375, loss is: 0.20762968063354492, w is: 0.8618771433830261, b is -0.02978912554681301\n",
            "iteration 376, loss is: 0.20759499073028564, w is: 0.8621696829795837, b is -0.029491234570741653\n",
            "iteration 377, loss is: 0.20756101608276367, w is: 0.8624593019485474, b is -0.029196321964263916\n",
            "iteration 378, loss is: 0.20752766728401184, w is: 0.862746000289917, b is -0.02890435792505741\n",
            "iteration 379, loss is: 0.20749501883983612, w is: 0.8630298376083374, b is -0.0286153145134449\n",
            "iteration 380, loss is: 0.20746298134326935, w is: 0.8633108139038086, b is -0.028329161927103996\n",
            "iteration 381, loss is: 0.2074316442012787, w is: 0.8635889887809753, b is -0.02804587036371231\n",
            "iteration 382, loss is: 0.20740088820457458, w is: 0.8638644218444824, b is -0.027765411883592606\n",
            "iteration 383, loss is: 0.20737074315547943, w is: 0.8641370534896851, b is -0.027487758547067642\n",
            "iteration 384, loss is: 0.20734119415283203, w is: 0.8644070029258728, b is -0.027212880551815033\n",
            "iteration 385, loss is: 0.20731227099895477, w is: 0.8646742105484009, b is -0.02694075182080269\n",
            "iteration 386, loss is: 0.2072838991880417, w is: 0.8649387955665588, b is -0.02667134441435337\n",
            "iteration 387, loss is: 0.20725607872009277, w is: 0.8652006983757019, b is -0.02640463039278984\n",
            "iteration 388, loss is: 0.20722883939743042, w is: 0.8654599785804749, b is -0.02614058367908001\n",
            "iteration 389, loss is: 0.20720212161540985, w is: 0.8657166957855225, b is -0.025879178196191788\n",
            "iteration 390, loss is: 0.20717594027519226, w is: 0.8659708499908447, b is -0.025620386004447937\n",
            "iteration 391, loss is: 0.20715028047561646, w is: 0.8662224411964417, b is -0.025364182889461517\n",
            "iteration 392, loss is: 0.20712512731552124, w is: 0.866471529006958, b is -0.02511054091155529\n",
            "iteration 393, loss is: 0.20710046589374542, w is: 0.8667181134223938, b is -0.024859435856342316\n",
            "iteration 394, loss is: 0.2070763260126114, w is: 0.8669622540473938, b is -0.024610841646790504\n",
            "iteration 395, loss is: 0.20705264806747437, w is: 0.867203950881958, b is -0.024364734068512917\n",
            "iteration 396, loss is: 0.20702944695949554, w is: 0.8674432039260864, b is -0.024121087044477463\n",
            "iteration 397, loss is: 0.20700669288635254, w is: 0.8676800727844238, b is -0.023879876360297203\n",
            "iteration 398, loss is: 0.20698440074920654, w is: 0.8679145574569702, b is -0.023641077801585197\n",
            "iteration 399, loss is: 0.20696257054805756, w is: 0.8681467175483704, b is -0.023404667153954506\n",
            "iteration 400, loss is: 0.2069411724805832, w is: 0.8683765530586243, b is -0.02317062020301819\n",
            "iteration 401, loss is: 0.20692016184329987, w is: 0.8686040639877319, b is -0.022938914597034454\n",
            "iteration 402, loss is: 0.20689958333969116, w is: 0.8688293099403381, b is -0.022709526121616364\n",
            "iteration 403, loss is: 0.20687943696975708, w is: 0.8690523505210876, b is -0.022482430562376976\n",
            "iteration 404, loss is: 0.20685967803001404, w is: 0.8692731261253357, b is -0.0222576055675745\n",
            "iteration 405, loss is: 0.20684032142162323, w is: 0.869491696357727, b is -0.022035028785467148\n",
            "iteration 406, loss is: 0.20682133734226227, w is: 0.8697080612182617, b is -0.021814677864313126\n",
            "iteration 407, loss is: 0.20680271089076996, w is: 0.8699222803115845, b is -0.021596530452370644\n",
            "iteration 408, loss is: 0.20678450167179108, w is: 0.8701343536376953, b is -0.02138056606054306\n",
            "iteration 409, loss is: 0.20676665008068085, w is: 0.870344340801239, b is -0.021166760474443436\n",
            "iteration 410, loss is: 0.20674912631511688, w is: 0.8705521821975708, b is -0.020955093204975128\n",
            "iteration 411, loss is: 0.20673194527626038, w is: 0.8707579374313354, b is -0.020745541900396347\n",
            "iteration 412, loss is: 0.20671513676643372, w is: 0.8709616661071777, b is -0.02053808607161045\n",
            "iteration 413, loss is: 0.20669865608215332, w is: 0.8711633682250977, b is -0.020332705229520798\n",
            "iteration 414, loss is: 0.2066824585199356, w is: 0.8713630437850952, b is -0.020129378885030746\n",
            "iteration 415, loss is: 0.20666663348674774, w is: 0.8715606927871704, b is -0.019928084686398506\n",
            "iteration 416, loss is: 0.20665110647678375, w is: 0.871756374835968, b is -0.019728804007172585\n",
            "iteration 417, loss is: 0.20663590729236603, w is: 0.871950089931488, b is -0.01953151635825634\n",
            "iteration 418, loss is: 0.206620991230011, w is: 0.8721418976783752, b is -0.01933620125055313\n",
            "iteration 419, loss is: 0.20660638809204102, w is: 0.8723317980766296, b is -0.019142840057611465\n",
            "iteration 420, loss is: 0.20659203827381134, w is: 0.8725197911262512, b is -0.0189514122903347\n",
            "iteration 421, loss is: 0.20657801628112793, w is: 0.87270587682724, b is -0.018761897459626198\n",
            "iteration 422, loss is: 0.20656424760818481, w is: 0.8728901147842407, b is -0.01857427880167961\n",
            "iteration 423, loss is: 0.20655076205730438, w is: 0.8730725049972534, b is -0.0183885358273983\n",
            "iteration 424, loss is: 0.20653755962848663, w is: 0.8732531070709229, b is -0.018204649910330772\n",
            "iteration 425, loss is: 0.2065245658159256, w is: 0.8734318614006042, b is -0.018022604286670685\n",
            "iteration 426, loss is: 0.20651188492774963, w is: 0.8736088275909424, b is -0.017842378467321396\n",
            "iteration 427, loss is: 0.20649944245815277, w is: 0.873784065246582, b is -0.017663953825831413\n",
            "iteration 428, loss is: 0.2064872533082962, w is: 0.8739575147628784, b is -0.017487313598394394\n",
            "iteration 429, loss is: 0.20647528767585754, w is: 0.8741292357444763, b is -0.017312441021203995\n",
            "iteration 430, loss is: 0.20646357536315918, w is: 0.8742992281913757, b is -0.017139317467808723\n",
            "iteration 431, loss is: 0.20645210146903992, w is: 0.8744675517082214, b is -0.016967924311757088\n",
            "iteration 432, loss is: 0.20644082129001617, w is: 0.8746342062950134, b is -0.016798244789242744\n",
            "iteration 433, loss is: 0.20642980933189392, w is: 0.8747991919517517, b is -0.01663026213645935\n",
            "iteration 434, loss is: 0.20641900599002838, w is: 0.8749625086784363, b is -0.016463959589600563\n",
            "iteration 435, loss is: 0.20640841126441956, w is: 0.8751242160797119, b is -0.01629932038486004\n",
            "iteration 436, loss is: 0.20639802515506744, w is: 0.8752842545509338, b is -0.016136327758431435\n",
            "iteration 437, loss is: 0.20638783276081085, w is: 0.8754427433013916, b is -0.015974964946508408\n",
            "iteration 438, loss is: 0.20637784898281097, w is: 0.8755996227264404, b is -0.015815215185284615\n",
            "iteration 439, loss is: 0.2063680738210678, w is: 0.8757549524307251, b is -0.01565706357359886\n",
            "iteration 440, loss is: 0.20635849237442017, w is: 0.8759087324142456, b is -0.015500493347644806\n",
            "iteration 441, loss is: 0.20634910464286804, w is: 0.876060962677002, b is -0.015345488674938679\n",
            "iteration 442, loss is: 0.20633989572525024, w is: 0.8762116432189941, b is -0.015192033722996712\n",
            "iteration 443, loss is: 0.20633089542388916, w is: 0.8763608336448669, b is -0.015040113590657711\n",
            "iteration 444, loss is: 0.20632202923297882, w is: 0.8765085339546204, b is -0.014889712445437908\n",
            "iteration 445, loss is: 0.206313356757164, w is: 0.8766547441482544, b is -0.01474081538617611\n",
            "iteration 446, loss is: 0.2063048779964447, w is: 0.8767995238304138, b is -0.01459340751171112\n",
            "iteration 447, loss is: 0.20629654824733734, w is: 0.8769428133964539, b is -0.014447473920881748\n",
            "iteration 448, loss is: 0.2062883973121643, w is: 0.8770846724510193, b is -0.014302998781204224\n",
            "iteration 449, loss is: 0.20628036558628082, w is: 0.8772251009941101, b is -0.014159969054162502\n",
            "iteration 450, loss is: 0.20627254247665405, w is: 0.8773641586303711, b is -0.01401836983859539\n",
            "iteration 451, loss is: 0.20626488327980042, w is: 0.8775018453598022, b is -0.013878186233341694\n",
            "iteration 452, loss is: 0.20625732839107513, w is: 0.8776381015777588, b is -0.013739404268562794\n",
            "iteration 453, loss is: 0.20624995231628418, w is: 0.8777730464935303, b is -0.01360200997442007\n",
            "iteration 454, loss is: 0.20624272525310516, w is: 0.8779066205024719, b is -0.013465989381074905\n",
            "iteration 455, loss is: 0.20623566210269928, w is: 0.8780388832092285, b is -0.013331329450011253\n",
            "iteration 456, loss is: 0.20622870326042175, w is: 0.8781697750091553, b is -0.013198016211390495\n",
            "iteration 457, loss is: 0.20622189342975616, w is: 0.878299355506897, b is -0.013066035695374012\n",
            "iteration 458, loss is: 0.20621523261070251, w is: 0.8784276843070984, b is -0.012935375794768333\n",
            "iteration 459, loss is: 0.20620866119861603, w is: 0.8785547018051147, b is -0.012806021608412266\n",
            "iteration 460, loss is: 0.20620226860046387, w is: 0.8786804676055908, b is -0.012677961029112339\n",
            "iteration 461, loss is: 0.20619598031044006, w is: 0.8788049817085266, b is -0.012551181018352509\n",
            "iteration 462, loss is: 0.206189826130867, w is: 0.8789282441139221, b is -0.012425669468939304\n",
            "iteration 463, loss is: 0.2061837911605835, w is: 0.8790502548217773, b is -0.012301412411034107\n",
            "iteration 464, loss is: 0.20617789030075073, w is: 0.8791710734367371, b is -0.012178398668766022\n",
            "iteration 465, loss is: 0.20617207884788513, w is: 0.8792906403541565, b is -0.012056614272296429\n",
            "iteration 466, loss is: 0.20616638660430908, w is: 0.8794090151786804, b is -0.011936048045754433\n",
            "iteration 467, loss is: 0.20616081357002258, w is: 0.8795262575149536, b is -0.011816687881946564\n",
            "iteration 468, loss is: 0.20615535974502563, w is: 0.8796423077583313, b is -0.011698520742356777\n",
            "iteration 469, loss is: 0.20615001022815704, w is: 0.8797571659088135, b is -0.011581535451114178\n",
            "iteration 470, loss is: 0.2061447650194168, w is: 0.8798708915710449, b is -0.011465719901025295\n",
            "iteration 471, loss is: 0.20613963901996613, w is: 0.8799834847450256, b is -0.011351062916219234\n",
            "iteration 472, loss is: 0.2061346024274826, w is: 0.8800949454307556, b is -0.011237552389502525\n",
            "iteration 473, loss is: 0.20612965524196625, w is: 0.8802052736282349, b is -0.011125177145004272\n",
            "iteration 474, loss is: 0.20612481236457825, w is: 0.8803145289421082, b is -0.011013925075531006\n",
            "iteration 475, loss is: 0.2061200886964798, w is: 0.8804227113723755, b is -0.010903785936534405\n",
            "iteration 476, loss is: 0.20611542463302612, w is: 0.8805297613143921, b is -0.010794747620821\n",
            "iteration 477, loss is: 0.206110879778862, w is: 0.8806357383728027, b is -0.010686799883842468\n",
            "iteration 478, loss is: 0.20610639452934265, w is: 0.8807407021522522, b is -0.010579931549727917\n",
            "iteration 479, loss is: 0.20610201358795166, w is: 0.8808445930480957, b is -0.010474132373929024\n",
            "iteration 480, loss is: 0.20609775185585022, w is: 0.880947470664978, b is -0.010369391180574894\n",
            "iteration 481, loss is: 0.20609354972839355, w is: 0.8810492753982544, b is -0.010265696793794632\n",
            "iteration 482, loss is: 0.20608943700790405, w is: 0.8811500668525696, b is -0.010163039900362492\n",
            "iteration 483, loss is: 0.20608538389205933, w is: 0.8812498450279236, b is -0.010061409324407578\n",
            "iteration 484, loss is: 0.20608144998550415, w is: 0.8813486695289612, b is -0.009960795752704144\n",
            "iteration 485, loss is: 0.20607756078243256, w is: 0.8814464807510376, b is -0.009861188009381294\n",
            "iteration 486, loss is: 0.20607374608516693, w is: 0.8815433382987976, b is -0.009762575849890709\n",
            "iteration 487, loss is: 0.20607002079486847, w is: 0.8816391825675964, b is -0.009664949961006641\n",
            "iteration 488, loss is: 0.20606637001037598, w is: 0.8817340731620789, b is -0.009568300098180771\n",
            "iteration 489, loss is: 0.20606280863285065, w is: 0.8818280100822449, b is -0.009472616948187351\n",
            "iteration 490, loss is: 0.2060593068599701, w is: 0.8819210529327393, b is -0.009377891197800636\n",
            "iteration 491, loss is: 0.20605584979057312, w is: 0.8820131421089172, b is -0.009284112602472305\n",
            "iteration 492, loss is: 0.2060524821281433, w is: 0.8821043372154236, b is -0.009191271848976612\n",
            "iteration 493, loss is: 0.20604918897151947, w is: 0.8821945786476135, b is -0.009099358692765236\n",
            "iteration 494, loss is: 0.2060459554195404, w is: 0.8822839260101318, b is -0.009008364751935005\n",
            "iteration 495, loss is: 0.20604276657104492, w is: 0.8823723793029785, b is -0.008918280713260174\n",
            "iteration 496, loss is: 0.2060396820306778, w is: 0.8824599385261536, b is -0.00882909819483757\n",
            "iteration 497, loss is: 0.20603662729263306, w is: 0.8825466632843018, b is -0.008740806952118874\n",
            "iteration 498, loss is: 0.2060336321592331, w is: 0.8826324939727783, b is -0.008653398603200912\n",
            "iteration 499, loss is: 0.2060306966304779, w is: 0.882717490196228, b is -0.008566864766180515\n",
            "iteration 500, loss is: 0.2060278356075287, w is: 0.8828015923500061, b is -0.008481196127831936\n",
            "iteration 501, loss is: 0.20602500438690186, w is: 0.8828848600387573, b is -0.008396384306252003\n",
            "iteration 502, loss is: 0.2060222625732422, w is: 0.8829672932624817, b is -0.00831241998821497\n",
            "iteration 503, loss is: 0.2060195505619049, w is: 0.883048951625824, b is -0.00822929572314024\n",
            "iteration 504, loss is: 0.2060169130563736, w is: 0.8831297755241394, b is -0.008147003129124641\n",
            "iteration 505, loss is: 0.20601435005664825, w is: 0.883209764957428, b is -0.008065532892942429\n",
            "iteration 506, loss is: 0.20601177215576172, w is: 0.8832889795303345, b is -0.007984877564013004\n",
            "iteration 507, loss is: 0.20600926876068115, w is: 0.8833674192428589, b is -0.007905028760433197\n",
            "iteration 508, loss is: 0.20600683987140656, w is: 0.8834450244903564, b is -0.007825978100299835\n",
            "iteration 509, loss is: 0.20600445568561554, w is: 0.8835218548774719, b is -0.007747718133032322\n",
            "iteration 510, loss is: 0.2060021162033081, w is: 0.8835979700088501, b is -0.007670240942388773\n",
            "iteration 511, loss is: 0.20599979162216187, w is: 0.8836733102798462, b is -0.007593538612127304\n",
            "iteration 512, loss is: 0.2059975415468216, w is: 0.8837478756904602, b is -0.007517603226006031\n",
            "iteration 513, loss is: 0.2059953510761261, w is: 0.8838217258453369, b is -0.007442427333444357\n",
            "iteration 514, loss is: 0.2059931755065918, w is: 0.8838948011398315, b is -0.0073680030182003975\n",
            "iteration 515, loss is: 0.20599104464054108, w is: 0.8839671611785889, b is -0.007294322829693556\n",
            "iteration 516, loss is: 0.20598897337913513, w is: 0.8840388059616089, b is -0.007221379783004522\n",
            "iteration 517, loss is: 0.20598694682121277, w is: 0.8841097354888916, b is -0.007149165961891413\n",
            "iteration 518, loss is: 0.2059849500656128, w is: 0.884179949760437, b is -0.0070776743814349174\n",
            "iteration 519, loss is: 0.205982968211174, w is: 0.8842494487762451, b is -0.0070068975910544395\n",
            "iteration 520, loss is: 0.2059810608625412, w is: 0.8843182325363159, b is -0.006936828605830669\n",
            "iteration 521, loss is: 0.20597916841506958, w is: 0.8843863606452942, b is -0.006867460440844297\n",
            "iteration 522, loss is: 0.20597732067108154, w is: 0.8844537734985352, b is -0.006798786111176014\n",
            "iteration 523, loss is: 0.20597553253173828, w is: 0.8845205307006836, b is -0.006730798166245222\n",
            "iteration 524, loss is: 0.2059737592935562, w is: 0.8845866322517395, b is -0.0066634900867938995\n",
            "iteration 525, loss is: 0.20597203075885773, w is: 0.8846520781517029, b is -0.006596855353564024\n",
            "iteration 526, loss is: 0.20597031712532043, w is: 0.8847168684005737, b is -0.006530886981636286\n",
            "iteration 527, loss is: 0.20596866309642792, w is: 0.884781002998352, b is -0.006465577986091375\n",
            "iteration 528, loss is: 0.2059670239686966, w is: 0.8848444819450378, b is -0.006400922313332558\n",
            "iteration 529, loss is: 0.20596542954444885, w is: 0.8849073648452759, b is -0.006336912978440523\n",
            "iteration 530, loss is: 0.2059638351202011, w is: 0.8849695920944214, b is -0.006273543927818537\n",
            "iteration 531, loss is: 0.20596230030059814, w is: 0.8850312232971191, b is -0.006210808642208576\n",
            "iteration 532, loss is: 0.20596081018447876, w is: 0.8850921988487244, b is -0.006148700602352619\n",
            "iteration 533, loss is: 0.20595933496952057, w is: 0.8851525783538818, b is -0.006087213754653931\n",
            "iteration 534, loss is: 0.20595785975456238, w is: 0.8852123618125916, b is -0.006026341579854488\n",
            "iteration 535, loss is: 0.20595644414424896, w is: 0.8852715492248535, b is -0.005966078024357557\n",
            "iteration 536, loss is: 0.20595505833625793, w is: 0.8853301405906677, b is -0.0059064170345664024\n",
            "iteration 537, loss is: 0.2059537172317505, w is: 0.8853881359100342, b is -0.005847353022545576\n",
            "iteration 538, loss is: 0.20595236122608185, w is: 0.8854455351829529, b is -0.005788879469037056\n",
            "iteration 539, loss is: 0.20595106482505798, w is: 0.8855023980140686, b is -0.005730990786105394\n",
            "iteration 540, loss is: 0.20594976842403412, w is: 0.8855586647987366, b is -0.005673680920153856\n",
            "iteration 541, loss is: 0.20594851672649384, w is: 0.8856143951416016, b is -0.005616944283246994\n",
            "iteration 542, loss is: 0.20594727993011475, w is: 0.8856695294380188, b is -0.005560774821788073\n",
            "iteration 543, loss is: 0.20594607293605804, w is: 0.8857241272926331, b is -0.005505166947841644\n",
            "iteration 544, loss is: 0.20594488084316254, w is: 0.8857781887054443, b is -0.005450115073472261\n",
            "iteration 545, loss is: 0.2059437334537506, w is: 0.8858317136764526, b is -0.005395614076405764\n",
            "iteration 546, loss is: 0.2059425711631775, w is: 0.885884702205658, b is -0.005341657903045416\n",
            "iteration 547, loss is: 0.20594143867492676, w is: 0.8859371542930603, b is -0.005288241431117058\n",
            "iteration 548, loss is: 0.2059403657913208, w is: 0.8859890699386597, b is -0.005235359072685242\n",
            "iteration 549, loss is: 0.20593930780887604, w is: 0.8860405087471008, b is -0.005183005705475807\n",
            "iteration 550, loss is: 0.20593824982643127, w is: 0.886091411113739, b is -0.005131175741553307\n",
            "iteration 551, loss is: 0.2059372067451477, w is: 0.8861417770385742, b is -0.0050798640586435795\n",
            "iteration 552, loss is: 0.2059362381696701, w is: 0.8861916661262512, b is -0.0050290655344724655\n",
            "iteration 553, loss is: 0.20593520998954773, w is: 0.88624107837677, b is -0.004978775046765804\n",
            "iteration 554, loss is: 0.20593425631523132, w is: 0.8862899541854858, b is -0.004928987473249435\n",
            "iteration 555, loss is: 0.20593330264091492, w is: 0.8863383531570435, b is -0.0048796976916491985\n",
            "iteration 556, loss is: 0.2059323638677597, w is: 0.8863862752914429, b is -0.004830900579690933\n",
            "iteration 557, loss is: 0.20593145489692688, w is: 0.8864337205886841, b is -0.0047825914807617664\n",
            "iteration 558, loss is: 0.20593057572841644, w is: 0.8864806890487671, b is -0.004734765738248825\n",
            "iteration 559, loss is: 0.2059296816587448, w is: 0.8865271806716919, b is -0.004687418229877949\n",
            "iteration 560, loss is: 0.20592884719371796, w is: 0.8865731954574585, b is -0.004640543833374977\n",
            "iteration 561, loss is: 0.2059279829263687, w is: 0.8866187930107117, b is -0.004594138357788324\n",
            "iteration 562, loss is: 0.20592717826366425, w is: 0.8866639137268066, b is -0.004548197146505117\n",
            "iteration 563, loss is: 0.20592635869979858, w is: 0.8867085576057434, b is -0.004502715077251196\n",
            "iteration 564, loss is: 0.2059255838394165, w is: 0.8867527842521667, b is -0.004457687959074974\n",
            "iteration 565, loss is: 0.20592479407787323, w is: 0.8867965340614319, b is -0.004413111135363579\n",
            "iteration 566, loss is: 0.20592404901981354, w is: 0.8868398666381836, b is -0.004368979949504137\n",
            "iteration 567, loss is: 0.20592328906059265, w is: 0.8868827819824219, b is -0.004325290210545063\n",
            "iteration 568, loss is: 0.20592254400253296, w is: 0.8869252800941467, b is -0.004282037261873484\n",
            "iteration 569, loss is: 0.20592184364795685, w is: 0.8869673609733582, b is -0.004239216912537813\n",
            "iteration 570, loss is: 0.20592115819454193, w is: 0.8870089650154114, b is -0.0041968245059251785\n",
            "iteration 571, loss is: 0.20592042803764343, w is: 0.8870501518249512, b is -0.004154856316745281\n",
            "iteration 572, loss is: 0.20591974258422852, w is: 0.8870909810066223, b is -0.004113307688385248\n",
            "iteration 573, loss is: 0.20591911673545837, w is: 0.88713139295578, b is -0.004072174429893494\n",
            "iteration 574, loss is: 0.20591844618320465, w is: 0.8871713876724243, b is -0.004031452815979719\n",
            "iteration 575, loss is: 0.2059178203344345, w is: 0.8872109651565552, b is -0.003991138190031052\n",
            "iteration 576, loss is: 0.20591719448566437, w is: 0.8872501850128174, b is -0.003951226826757193\n",
            "iteration 577, loss is: 0.20591658353805542, w is: 0.8872889876365662, b is -0.003911714535206556\n",
            "iteration 578, loss is: 0.20591601729393005, w is: 0.8873273730278015, b is -0.0038725973572582006\n",
            "iteration 579, loss is: 0.2059154212474823, w is: 0.8873654007911682, b is -0.0038338713347911835\n",
            "iteration 580, loss is: 0.20591481029987335, w is: 0.8874030709266663, b is -0.0037955325096845627\n",
            "iteration 581, loss is: 0.20591425895690918, w is: 0.8874403238296509, b is -0.00375757715664804\n",
            "iteration 582, loss is: 0.2059137374162674, w is: 0.8874772191047668, b is -0.0037200013175606728\n",
            "iteration 583, loss is: 0.20591320097446442, w is: 0.8875137567520142, b is -0.003682801267132163\n",
            "iteration 584, loss is: 0.20591264963150024, w is: 0.8875499367713928, b is -0.0036459732800722122\n",
            "iteration 585, loss is: 0.20591209828853607, w is: 0.8875857591629028, b is -0.003609513631090522\n",
            "iteration 586, loss is: 0.20591160655021667, w is: 0.8876212239265442, b is -0.0035734183620661497\n",
            "iteration 587, loss is: 0.20591109991073608, w is: 0.8876563310623169, b is -0.0035376842133700848\n",
            "iteration 588, loss is: 0.20591062307357788, w is: 0.887691080570221, b is -0.0035023074597120285\n",
            "iteration 589, loss is: 0.20591014623641968, w is: 0.8877254724502563, b is -0.0034672843758016825\n",
            "iteration 590, loss is: 0.20590966939926147, w is: 0.8877595067024231, b is -0.003432611469179392\n",
            "iteration 591, loss is: 0.20590922236442566, w is: 0.887793242931366, b is -0.003398285247385502\n",
            "iteration 592, loss is: 0.20590876042842865, w is: 0.8878266215324402, b is -0.0033643024507910013\n",
            "iteration 593, loss is: 0.20590832829475403, w is: 0.8878596425056458, b is -0.0033306593541055918\n",
            "iteration 594, loss is: 0.2059078812599182, w is: 0.8878923654556274, b is -0.003297352697700262\n",
            "iteration 595, loss is: 0.20590746402740479, w is: 0.8879247307777405, b is -0.003264379221946001\n",
            "iteration 596, loss is: 0.20590706169605255, w is: 0.8879567980766296, b is -0.003231735434383154\n",
            "iteration 597, loss is: 0.20590662956237793, w is: 0.8879885077476501, b is -0.0031994180753827095\n",
            "iteration 598, loss is: 0.2059062123298645, w is: 0.8880199193954468, b is -0.0031674238853156567\n",
            "iteration 599, loss is: 0.20590582489967346, w is: 0.8880510330200195, b is -0.0031357496045529842\n",
            "iteration 600, loss is: 0.2059054672718048, w is: 0.8880818486213684, b is -0.003104391973465681\n",
            "iteration 601, loss is: 0.20590509474277496, w is: 0.8881123065948486, b is -0.0030733479652553797\n",
            "iteration 602, loss is: 0.20590470731258392, w is: 0.888142466545105, b is -0.0030426145531237125\n",
            "iteration 603, loss is: 0.20590434968471527, w is: 0.8881723284721375, b is -0.0030121884774416685\n",
            "iteration 604, loss is: 0.2059040069580078, w is: 0.888201892375946, b is -0.00298206671141088\n",
            "iteration 605, loss is: 0.20590366423130035, w is: 0.8882311582565308, b is -0.002952245995402336\n",
            "iteration 606, loss is: 0.2059033215045929, w is: 0.8882601261138916, b is -0.0029227235354483128\n",
            "iteration 607, loss is: 0.20590299367904663, w is: 0.8882888555526733, b is -0.0028934963047504425\n",
            "iteration 608, loss is: 0.20590265095233917, w is: 0.8883172869682312, b is -0.0028645615093410015\n",
            "iteration 609, loss is: 0.2059023231267929, w is: 0.8883454203605652, b is -0.0028359158895909786\n",
            "iteration 610, loss is: 0.20590201020240784, w is: 0.8883732557296753, b is -0.00280755665153265\n",
            "iteration 611, loss is: 0.20590171217918396, w is: 0.8884008526802063, b is -0.0027794812340289354\n",
            "iteration 612, loss is: 0.2059013992547989, w is: 0.8884281516075134, b is -0.002751686377450824\n",
            "iteration 613, loss is: 0.2059011161327362, w is: 0.8884551525115967, b is -0.002724169520661235\n",
            "iteration 614, loss is: 0.20590083301067352, w is: 0.8884819149971008, b is -0.002696927869692445\n",
            "iteration 615, loss is: 0.20590054988861084, w is: 0.8885083794593811, b is -0.0026699586305767298\n",
            "iteration 616, loss is: 0.20590023696422577, w is: 0.8885346055030823, b is -0.002643259009346366\n",
            "iteration 617, loss is: 0.20590001344680786, w is: 0.8885605931282043, b is -0.002616826444864273\n",
            "iteration 618, loss is: 0.20589973032474518, w is: 0.8885862827301025, b is -0.0025906581431627274\n",
            "iteration 619, loss is: 0.2058994621038437, w is: 0.8886117339134216, b is -0.0025647515431046486\n",
            "iteration 620, loss is: 0.2058991938829422, w is: 0.8886369466781616, b is -0.0025391040835529566\n",
            "iteration 621, loss is: 0.2058989405632019, w is: 0.8886618614196777, b is -0.0025137129705399275\n",
            "iteration 622, loss is: 0.205898717045784, w is: 0.8886865377426147, b is -0.0024885758757591248\n",
            "iteration 623, loss is: 0.2058984637260437, w is: 0.8887109756469727, b is -0.0024636900052428246\n",
            "iteration 624, loss is: 0.2058982104063034, w is: 0.8887351751327515, b is -0.0024390530306845903\n",
            "iteration 625, loss is: 0.2058980017900467, w is: 0.8887591361999512, b is -0.002414662390947342\n",
            "iteration 626, loss is: 0.2058977633714676, w is: 0.8887828588485718, b is -0.0023905157577246428\n",
            "iteration 627, loss is: 0.20589755475521088, w is: 0.8888063430786133, b is -0.0023666105698794127\n",
            "iteration 628, loss is: 0.20589733123779297, w is: 0.8888295888900757, b is -0.002342944499105215\n",
            "iteration 629, loss is: 0.20589712262153625, w is: 0.888852596282959, b is -0.0023195152170956135\n",
            "iteration 630, loss is: 0.20589689910411835, w is: 0.8888753652572632, b is -0.002296319929882884\n",
            "iteration 631, loss is: 0.20589666068553925, w is: 0.8888978958129883, b is -0.0022733567748218775\n",
            "iteration 632, loss is: 0.20589648187160492, w is: 0.888920247554779, b is -0.0022506231907755136\n",
            "iteration 633, loss is: 0.2058962732553482, w is: 0.8889423608779907, b is -0.002228116849437356\n",
            "iteration 634, loss is: 0.20589609444141388, w is: 0.8889642357826233, b is -0.0022058356553316116\n",
            "iteration 635, loss is: 0.20589591562747955, w is: 0.8889858722686768, b is -0.002183777280151844\n",
            "iteration 636, loss is: 0.20589573681354523, w is: 0.8890073299407959, b is -0.0021619393955916166\n",
            "iteration 637, loss is: 0.2058955281972885, w is: 0.8890285491943359, b is -0.0021403199061751366\n",
            "iteration 638, loss is: 0.2058953493833542, w is: 0.8890495896339417, b is -0.002118916716426611\n",
            "iteration 639, loss is: 0.20589518547058105, w is: 0.8890703916549683, b is -0.0020977274980396032\n",
            "iteration 640, loss is: 0.20589500665664673, w is: 0.8890910148620605, b is -0.0020767501555383205\n",
            "iteration 641, loss is: 0.2058948278427124, w is: 0.8891113996505737, b is -0.00205598259344697\n",
            "iteration 642, loss is: 0.20589466392993927, w is: 0.8891316056251526, b is -0.0020354227162897587\n",
            "iteration 643, loss is: 0.20589450001716614, w is: 0.8891515731811523, b is -0.0020150686614215374\n",
            "iteration 644, loss is: 0.2058943659067154, w is: 0.8891713619232178, b is -0.001994917867705226\n",
            "iteration 645, loss is: 0.20589418709278107, w is: 0.8891909718513489, b is -0.0019749687053263187\n",
            "iteration 646, loss is: 0.20589405298233032, w is: 0.8892103433609009, b is -0.001955219078809023\n",
            "iteration 647, loss is: 0.2058938890695572, w is: 0.8892295360565186, b is -0.0019356667762622237\n",
            "iteration 648, loss is: 0.20589375495910645, w is: 0.8892485499382019, b is -0.0019163100514560938\n",
            "iteration 649, loss is: 0.2058935910463333, w is: 0.8892673850059509, b is -0.0018971470417454839\n",
            "iteration 650, loss is: 0.20589345693588257, w is: 0.8892860412597656, b is -0.001878175651654601\n",
            "iteration 651, loss is: 0.20589332282543182, w is: 0.8893044590950012, b is -0.0018593939021229744\n",
            "iteration 652, loss is: 0.20589320361614227, w is: 0.8893226981163025, b is -0.0018407999305054545\n",
            "iteration 653, loss is: 0.20589305460453033, w is: 0.8893407583236694, b is -0.0018223919905722141\n",
            "iteration 654, loss is: 0.2058929055929184, w is: 0.889358639717102, b is -0.001804168103262782\n",
            "iteration 655, loss is: 0.20589281618595123, w is: 0.8893763422966003, b is -0.001786126522347331\n",
            "iteration 656, loss is: 0.2058926820755005, w is: 0.8893938660621643, b is -0.00176826526876539\n",
            "iteration 657, loss is: 0.20589254796504974, w is: 0.889411211013794, b is -0.0017505825962871313\n",
            "iteration 658, loss is: 0.2058924287557602, w is: 0.8894283771514893, b is -0.0017330767586827278\n",
            "iteration 659, loss is: 0.20589232444763184, w is: 0.889445424079895, b is -0.001715746009722352\n",
            "iteration 660, loss is: 0.2058921903371811, w is: 0.8894622921943665, b is -0.0016985886031761765\n",
            "iteration 661, loss is: 0.20589210093021393, w is: 0.8894789814949036, b is -0.0016816026763990521\n",
            "iteration 662, loss is: 0.20589198172092438, w is: 0.8894954919815063, b is -0.001664786715991795\n",
            "iteration 663, loss is: 0.20589186251163483, w is: 0.8895118236541748, b is -0.001648138975724578\n",
            "iteration 664, loss is: 0.20589177310466766, w is: 0.8895280361175537, b is -0.0016316575929522514\n",
            "iteration 665, loss is: 0.2058916687965393, w is: 0.8895440697669983, b is -0.001615341054275632\n",
            "iteration 666, loss is: 0.20589156448841095, w is: 0.8895599246025085, b is -0.0015991877298802137\n",
            "iteration 667, loss is: 0.2058914601802826, w is: 0.8895756006240845, b is -0.0015831958735361695\n",
            "iteration 668, loss is: 0.20589134097099304, w is: 0.8895911574363708, b is -0.0015673638554289937\n",
            "iteration 669, loss is: 0.20589128136634827, w is: 0.8896065354347229, b is -0.0015516902785748243\n",
            "iteration 670, loss is: 0.20589116215705872, w is: 0.8896217942237854, b is -0.001536173396743834\n",
            "iteration 671, loss is: 0.20589107275009155, w is: 0.8896368741989136, b is -0.001520811696536839\n",
            "iteration 672, loss is: 0.2058909833431244, w is: 0.8896518349647522, b is -0.0015056036645546556\n",
            "iteration 673, loss is: 0.20589090883731842, w is: 0.8896666169166565, b is -0.0014905475545674562\n",
            "iteration 674, loss is: 0.20589081943035126, w is: 0.8896812796592712, b is -0.001475642086006701\n",
            "iteration 675, loss is: 0.2058907300233841, w is: 0.8896957635879517, b is -0.0014608856290578842\n",
            "iteration 676, loss is: 0.20589064061641693, w is: 0.8897101283073425, b is -0.0014462766703218222\n",
            "iteration 677, loss is: 0.20589055120944977, w is: 0.8897243142127991, b is -0.0014318140456452966\n",
            "iteration 678, loss is: 0.2058904767036438, w is: 0.8897383809089661, b is -0.0014174958923831582\n",
            "iteration 679, loss is: 0.20589041709899902, w is: 0.8897523283958435, b is -0.001403320929966867\n",
            "iteration 680, loss is: 0.20589031279087067, w is: 0.8897660970687866, b is -0.0013892876449972391\n",
            "iteration 681, loss is: 0.2058902531862259, w is: 0.8897797465324402, b is -0.0013753948733210564\n",
            "iteration 682, loss is: 0.20589017868041992, w is: 0.8897932767868042, b is -0.0013616408687084913\n",
            "iteration 683, loss is: 0.20589011907577515, w is: 0.8898066282272339, b is -0.0013480244670063257\n",
            "iteration 684, loss is: 0.20589002966880798, w is: 0.889819860458374, b is -0.0013345442712306976\n",
            "iteration 685, loss is: 0.2058899849653244, w is: 0.8898329734802246, b is -0.0013211988843977451\n",
            "iteration 686, loss is: 0.20588988065719604, w is: 0.8898459672927856, b is -0.0013079867931082845\n",
            "iteration 687, loss is: 0.20588982105255127, w is: 0.8898587822914124, b is -0.0012949069496244192\n",
            "iteration 688, loss is: 0.2058897465467453, w is: 0.8898714780807495, b is -0.0012819579569622874\n",
            "iteration 689, loss is: 0.20588968694210052, w is: 0.8898840546607971, b is -0.0012691383017227054\n",
            "iteration 690, loss is: 0.20588964223861694, w is: 0.8898965120315552, b is -0.0012564469361677766\n",
            "iteration 691, loss is: 0.20588956773281097, w is: 0.8899088501930237, b is -0.0012438823468983173\n",
            "iteration 692, loss is: 0.2058895081281662, w is: 0.8899210691452026, b is -0.0012314434861764312\n",
            "iteration 693, loss is: 0.20588944852352142, w is: 0.889933168888092, b is -0.001219129073433578\n",
            "iteration 694, loss is: 0.20588940382003784, w is: 0.8899451494216919, b is -0.001206937711685896\n",
            "iteration 695, loss is: 0.20588934421539307, w is: 0.8899570107460022, b is -0.0011948683531954885\n",
            "iteration 696, loss is: 0.20588929951190948, w is: 0.889968752861023, b is -0.0011829196009784937\n",
            "iteration 697, loss is: 0.20588922500610352, w is: 0.8899803757667542, b is -0.0011710904072970152\n",
            "iteration 698, loss is: 0.20588918030261993, w is: 0.8899918794631958, b is -0.0011593794915825129\n",
            "iteration 699, loss is: 0.20588912069797516, w is: 0.8900032639503479, b is -0.0011477856896817684\n",
            "iteration 700, loss is: 0.20588907599449158, w is: 0.8900145292282104, b is -0.0011363078374415636\n",
            "iteration 701, loss is: 0.2058890014886856, w is: 0.8900256752967834, b is -0.0011249447707086802\n",
            "iteration 702, loss is: 0.20588895678520203, w is: 0.8900367021560669, b is -0.0011136953253298998\n",
            "iteration 703, loss is: 0.20588892698287964, w is: 0.8900476098060608, b is -0.001102558453567326\n",
            "iteration 704, loss is: 0.20588886737823486, w is: 0.8900584578514099, b is -0.001091532874852419\n",
            "iteration 705, loss is: 0.20588883757591248, w is: 0.8900691866874695, b is -0.0010806176578626037\n",
            "iteration 706, loss is: 0.2058887779712677, w is: 0.8900797963142395, b is -0.0010698114056140184\n",
            "iteration 707, loss is: 0.20588873326778412, w is: 0.89009028673172, b is -0.0010591134196147323\n",
            "iteration 708, loss is: 0.20588868856430054, w is: 0.8901007175445557, b is -0.001048522419296205\n",
            "iteration 709, loss is: 0.20588865876197815, w is: 0.8901110291481018, b is -0.0010380371240898967\n",
            "iteration 710, loss is: 0.20588861405849457, w is: 0.8901212215423584, b is -0.0010276567190885544\n",
            "iteration 711, loss is: 0.2058885395526886, w is: 0.8901312947273254, b is -0.0010173801565542817\n",
            "iteration 712, loss is: 0.2058885246515274, w is: 0.8901413083076477, b is -0.0010072063887491822\n",
            "iteration 713, loss is: 0.20588849484920502, w is: 0.8901512026786804, b is -0.0009971343679353595\n",
            "iteration 714, loss is: 0.20588843524456024, w is: 0.8901609778404236, b is -0.000987163046374917\n",
            "iteration 715, loss is: 0.20588840544223785, w is: 0.890170693397522, b is -0.0009772913763299584\n",
            "iteration 716, loss is: 0.20588836073875427, w is: 0.8901802897453308, b is -0.00096751848468557\n",
            "iteration 717, loss is: 0.2058883160352707, w is: 0.8901897668838501, b is -0.0009578433819115162\n",
            "iteration 718, loss is: 0.2058883160352707, w is: 0.8901991844177246, b is -0.0009482649620622396\n",
            "iteration 719, loss is: 0.2058882713317871, w is: 0.8902084827423096, b is -0.0009387823520228267\n",
            "iteration 720, loss is: 0.20588821172714233, w is: 0.8902177214622498, b is -0.0009293945622630417\n",
            "iteration 721, loss is: 0.20588821172714233, w is: 0.8902268409729004, b is -0.0009201006614603102\n",
            "iteration 722, loss is: 0.20588816702365875, w is: 0.8902359008789062, b is -0.0009108997182920575\n",
            "iteration 723, loss is: 0.20588813722133636, w is: 0.8902448415756226, b is -0.000901790801435709\n",
            "iteration 724, loss is: 0.20588809251785278, w is: 0.8902537226676941, b is -0.0008927728049457073\n",
            "iteration 725, loss is: 0.2058880627155304, w is: 0.8902624845504761, b is -0.0008838451467454433\n",
            "iteration 726, loss is: 0.2058880478143692, w is: 0.8902711868286133, b is -0.0008750067790970206\n",
            "iteration 727, loss is: 0.20588800311088562, w is: 0.8902797698974609, b is -0.0008662568288855255\n",
            "iteration 728, loss is: 0.20588795840740204, w is: 0.8902882933616638, b is -0.0008575943065807223\n",
            "iteration 729, loss is: 0.20588795840740204, w is: 0.8902966976165771, b is -0.0008490183972753584\n",
            "iteration 730, loss is: 0.20588791370391846, w is: 0.8903050422668457, b is -0.000840528286062181\n",
            "iteration 731, loss is: 0.20588791370391846, w is: 0.8903132677078247, b is -0.0008321229834109545\n",
            "iteration 732, loss is: 0.20588786900043488, w is: 0.8903214335441589, b is -0.0008238017908297479\n",
            "iteration 733, loss is: 0.2058878391981125, w is: 0.8903295397758484, b is -0.0008155637769959867\n",
            "iteration 734, loss is: 0.2058878242969513, w is: 0.8903375267982483, b is -0.000807408185210079\n",
            "iteration 735, loss is: 0.2058877795934677, w is: 0.8903454542160034, b is -0.000799334142357111\n",
            "iteration 736, loss is: 0.2058877795934677, w is: 0.8903533220291138, b is -0.0007913407753221691\n",
            "iteration 737, loss is: 0.20588774979114532, w is: 0.8903610706329346, b is -0.0007834274438209832\n",
            "iteration 738, loss is: 0.20588770508766174, w is: 0.8903687596321106, b is -0.0007755931583233178\n",
            "iteration 739, loss is: 0.20588767528533936, w is: 0.8903763890266418, b is -0.0007678372785449028\n",
            "iteration 740, loss is: 0.20588766038417816, w is: 0.8903839588165283, b is -0.0007601589313708246\n",
            "iteration 741, loss is: 0.20588763058185577, w is: 0.8903914093971252, b is -0.0007525573601014912\n",
            "iteration 742, loss is: 0.20588761568069458, w is: 0.8903988003730774, b is -0.0007450318080373108\n",
            "iteration 743, loss is: 0.2058875858783722, w is: 0.8904061317443848, b is -0.0007375813438557088\n",
            "iteration 744, loss is: 0.205887570977211, w is: 0.8904134035110474, b is -0.0007302055018953979\n",
            "iteration 745, loss is: 0.2058875411748886, w is: 0.8904205560684204, b is -0.0007229034672491252\n",
            "iteration 746, loss is: 0.2058875411748886, w is: 0.8904276490211487, b is -0.0007156745414249599\n",
            "iteration 747, loss is: 0.20588749647140503, w is: 0.8904346823692322, b is -0.0007085177348926663\n",
            "iteration 748, loss is: 0.20588749647140503, w is: 0.8904416561126709, b is -0.0007014326401986182\n",
            "iteration 749, loss is: 0.20588748157024384, w is: 0.8904485702514648, b is -0.0006944183842279017\n",
            "iteration 750, loss is: 0.20588745176792145, w is: 0.8904553651809692, b is -0.000687474210280925\n",
            "iteration 751, loss is: 0.20588745176792145, w is: 0.8904621005058289, b is -0.0006805994198657572\n",
            "iteration 752, loss is: 0.20588743686676025, w is: 0.8904687762260437, b is -0.0006737933726981282\n",
            "iteration 753, loss is: 0.20588740706443787, w is: 0.8904753923416138, b is -0.0006670554867014289\n",
            "iteration 754, loss is: 0.20588739216327667, w is: 0.8904819488525391, b is -0.0006603848887607455\n",
            "iteration 755, loss is: 0.20588736236095428, w is: 0.8904884457588196, b is -0.0006537810550071299\n",
            "iteration 756, loss is: 0.20588736236095428, w is: 0.8904948830604553, b is -0.0006472432869486511\n",
            "iteration 757, loss is: 0.2058873474597931, w is: 0.8905012607574463, b is -0.0006407708860933781\n",
            "iteration 758, loss is: 0.2058873176574707, w is: 0.8905075788497925, b is -0.0006343632121570408\n",
            "iteration 759, loss is: 0.2058873176574707, w is: 0.8905137777328491, b is -0.0006280196248553693\n",
            "iteration 760, loss is: 0.2058873027563095, w is: 0.890519917011261, b is -0.0006217395421117544\n",
            "iteration 761, loss is: 0.20588727295398712, w is: 0.8905259966850281, b is -0.0006155220908112824\n",
            "iteration 762, loss is: 0.20588727295398712, w is: 0.8905320167541504, b is -0.0006093669217079878\n",
            "iteration 763, loss is: 0.20588724315166473, w is: 0.8905379772186279, b is -0.0006032732781022787\n",
            "iteration 764, loss is: 0.20588722825050354, w is: 0.8905438780784607, b is -0.000597240577917546\n",
            "iteration 765, loss is: 0.20588722825050354, w is: 0.8905497193336487, b is -0.0005912681808695197\n",
            "iteration 766, loss is: 0.20588719844818115, w is: 0.8905555009841919, b is -0.0005853554466739297\n",
            "iteration 767, loss is: 0.20588719844818115, w is: 0.8905612230300903, b is -0.0005795019096694887\n",
            "iteration 768, loss is: 0.20588718354701996, w is: 0.8905669450759888, b is -0.0005737069295719266\n",
            "iteration 769, loss is: 0.20588713884353638, w is: 0.8905726075172424, b is -0.0005679698660969734\n",
            "iteration 770, loss is: 0.20588713884353638, w is: 0.8905782103538513, b is -0.0005622902535833418\n",
            "iteration 771, loss is: 0.20588713884353638, w is: 0.8905837535858154, b is -0.00055666733533144\n",
            "iteration 772, loss is: 0.205887109041214, w is: 0.8905892372131348, b is -0.0005511006456799805\n",
            "iteration 773, loss is: 0.205887109041214, w is: 0.8905946612358093, b is -0.0005455896607600152\n",
            "iteration 774, loss is: 0.205887109041214, w is: 0.8906000256538391, b is -0.000540133798494935\n",
            "iteration 775, loss is: 0.2058870941400528, w is: 0.8906053304672241, b is -0.0005347324768081307\n",
            "iteration 776, loss is: 0.2058870941400528, w is: 0.8906105756759644, b is -0.0005293851136229932\n",
            "iteration 777, loss is: 0.2058870494365692, w is: 0.8906157612800598, b is -0.0005240912432782352\n",
            "iteration 778, loss is: 0.2058870494365692, w is: 0.8906208872795105, b is -0.0005188504001125693\n",
            "iteration 779, loss is: 0.2058870494365692, w is: 0.8906259536743164, b is -0.0005136619438417256\n",
            "iteration 780, loss is: 0.2058870494365692, w is: 0.8906310200691223, b is -0.0005085252923890948\n",
            "iteration 781, loss is: 0.20588701963424683, w is: 0.8906360268592834, b is -0.0005034399218857288\n",
            "iteration 782, loss is: 0.20588701963424683, w is: 0.8906409740447998, b is -0.000498405541293323\n",
            "iteration 783, loss is: 0.20588701963424683, w is: 0.8906458616256714, b is -0.0004934215103276074\n",
            "iteration 784, loss is: 0.20588701963424683, w is: 0.8906506896018982, b is -0.0004884873051196337\n",
            "iteration 785, loss is: 0.20588700473308563, w is: 0.8906554579734802, b is -0.00048360240180045366\n",
            "iteration 786, loss is: 0.20588697493076324, w is: 0.8906602263450623, b is -0.0004787663056049496\n",
            "iteration 787, loss is: 0.20588697493076324, w is: 0.8906649351119995, b is -0.0004739787254948169\n",
            "iteration 788, loss is: 0.20588696002960205, w is: 0.890669584274292, b is -0.0004692389629781246\n",
            "iteration 789, loss is: 0.20588696002960205, w is: 0.8906741738319397, b is -0.0004645465814974159\n",
            "iteration 790, loss is: 0.20588693022727966, w is: 0.8906787633895874, b is -0.00045990111539140344\n",
            "iteration 791, loss is: 0.20588693022727966, w is: 0.8906832933425903, b is -0.0004553020989987999\n",
            "iteration 792, loss is: 0.20588696002960205, w is: 0.8906877636909485, b is -0.0004507490957621485\n",
            "iteration 793, loss is: 0.20588693022727966, w is: 0.8906921744346619, b is -0.0004462416109163314\n",
            "iteration 794, loss is: 0.20588691532611847, w is: 0.8906965851783752, b is -0.00044177917880006135\n",
            "iteration 795, loss is: 0.20588691532611847, w is: 0.8907009363174438, b is -0.00043736130464822054\n",
            "iteration 796, loss is: 0.20588688552379608, w is: 0.8907052278518677, b is -0.0004329876974225044\n",
            "iteration 797, loss is: 0.20588691532611847, w is: 0.8907094597816467, b is -0.0004286578914616257\n",
            "iteration 798, loss is: 0.20588688552379608, w is: 0.8907136917114258, b is -0.00042437127558514476\n",
            "iteration 799, loss is: 0.20588688552379608, w is: 0.8907178640365601, b is -0.0004201274423394352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        # b. explicit modeling with automatic differentiation + explicit update\n",
        "        d = 1 # only one regressor.\n",
        "        w = torch.randn(1,d,requires_grad=True)\n",
        "        b = torch.randn(1,requires_grad=True)\n",
        "        \n",
        "        for t in range(niter):\n",
        "            print(t)\n",
        "            y_pred = torch.matmul(xT,w)+b # first dimension in xT is minibatch\n",
        "            # the statement above can be generalized to y = model(x)\n",
        "            err = (y_pred - y)\n",
        "            loss = err.pow(2.0).mean() # mean squared error\n",
        "            loss.backward() # run backpropagation\n",
        "            # we can now verify that the gradients are in agreement with what we can calculate by hand\n",
        "            prod = err * xT\n",
        "            print(w.grad - 2.0*prod.mean()) # verified to be 0\n",
        "            print(b.grad - 2.0*err.mean())  # verified to be 0\n",
        "            print(f\"iteration {t}, loss is: {loss}, w is: {float(w)}, b is {float(b)}\")\n",
        "            with torch.no_grad():\n",
        "              \n",
        "              w -= learning_rate * w.grad # you can also use prod.mean() \n",
        "              # not the same as w = w - learning_rate * w.grad. this won't work here\n",
        "              b -= learning_rate * b.grad # you can also use err.mean()\n",
        "      \n",
        "              w.grad.zero_()\n",
        "              b.grad.zero_()"
      ],
      "metadata": {
        "id": "fzTxf7jyHPaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db393cf4-b3fc-405d-c8d3-aff290515d2c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor([[1.4305e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 0, loss is: 5.6757965087890625, w is: -1.3313122987747192, b is 0.728462815284729\n",
            "1\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 1, loss is: 5.566944599151611, w is: -1.309087872505188, b is 0.7211781740188599\n",
            "2\n",
            "tensor([[1.4305e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 2, loss is: 5.460259914398193, w is: -1.2870856523513794, b is 0.7139663696289062\n",
            "3\n",
            "tensor([[-9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 3, loss is: 5.3556976318359375, w is: -1.2653034925460815, b is 0.7068266868591309\n",
            "4\n",
            "tensor([[9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 4, loss is: 5.2532172203063965, w is: -1.243739128112793, b is 0.6997584104537964\n",
            "5\n",
            "tensor([[1.4305e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 5, loss is: 5.152774810791016, w is: -1.2223904132843018, b is 0.6927608251571655\n",
            "6\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 6, loss is: 5.0543317794799805, w is: -1.201255202293396, b is 0.685833215713501\n",
            "7\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 7, loss is: 4.9578471183776855, w is: -1.1803313493728638, b is 0.6789748668670654\n",
            "8\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 8, loss is: 4.863283157348633, w is: -1.1596167087554932, b is 0.6721851229667664\n",
            "9\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 9, loss is: 4.77060079574585, w is: -1.1391092538833618, b is 0.6654632687568665\n",
            "10\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 10, loss is: 4.679762840270996, w is: -1.1188068389892578, b is 0.6588086485862732\n",
            "11\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 11, loss is: 4.590733051300049, w is: -1.0987074375152588, b is 0.6522205471992493\n",
            "12\n",
            "tensor([[-9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 12, loss is: 4.503474712371826, w is: -1.0788090229034424, b is 0.6456983685493469\n",
            "13\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 13, loss is: 4.417952537536621, w is: -1.0591095685958862, b is 0.6392413973808289\n",
            "14\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 14, loss is: 4.334132194519043, w is: -1.0396071672439575, b is 0.6328489780426025\n",
            "15\n",
            "tensor([[-1.1921e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 15, loss is: 4.251979827880859, w is: -1.0202997922897339, b is 0.6265205144882202\n",
            "16\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 16, loss is: 4.171462535858154, w is: -1.0011855363845825, b is 0.6202552914619446\n",
            "17\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 17, loss is: 4.09254789352417, w is: -0.982262372970581, b is 0.6140527129173279\n",
            "18\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.3842e-07], grad_fn=<SubBackward0>)\n",
            "iteration 18, loss is: 4.015203475952148, w is: -0.9635284543037415, b is 0.6079121828079224\n",
            "19\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 19, loss is: 3.9393978118896484, w is: -0.9449818730354309, b is 0.6018330454826355\n",
            "20\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 20, loss is: 3.865100383758545, w is: -0.9266207218170166, b is 0.5958147048950195\n",
            "21\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 21, loss is: 3.7922825813293457, w is: -0.9084432125091553, b is 0.5898565649986267\n",
            "22\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 22, loss is: 3.7209134101867676, w is: -0.8904474973678589, b is 0.5839579701423645\n",
            "23\n",
            "tensor([[9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 23, loss is: 3.6509640216827393, w is: -0.8726317286491394, b is 0.5781183838844299\n",
            "24\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 24, loss is: 3.5824074745178223, w is: -0.8549941182136536, b is 0.5723372101783752\n",
            "25\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 25, loss is: 3.5152149200439453, w is: -0.8375328779220581, b is 0.5666138529777527\n",
            "26\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 26, loss is: 3.449359178543091, w is: -0.8202462196350098, b is 0.5609477162361145\n",
            "27\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 27, loss is: 3.3848140239715576, w is: -0.8031324744224548, b is 0.5553382635116577\n",
            "28\n",
            "tensor([[9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 28, loss is: 3.3215532302856445, w is: -0.78618985414505, b is 0.5497848987579346\n",
            "29\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 29, loss is: 3.2595512866973877, w is: -0.7694166302680969, b is 0.5442870259284973\n",
            "30\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.3842e-07], grad_fn=<SubBackward0>)\n",
            "iteration 30, loss is: 3.1987836360931396, w is: -0.7528111934661865, b is 0.5388441681861877\n",
            "31\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 31, loss is: 3.1392252445220947, w is: -0.7363717555999756, b is 0.5334557294845581\n",
            "32\n",
            "tensor([[1.1921e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 32, loss is: 3.0808513164520264, w is: -0.7200967073440552, b is 0.5281211733818054\n",
            "33\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 33, loss is: 3.023639678955078, w is: -0.7039844393730164, b is 0.5228399634361267\n",
            "34\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 34, loss is: 2.96756649017334, w is: -0.6880332827568054, b is 0.517611563205719\n",
            "35\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 35, loss is: 2.9126088619232178, w is: -0.6722416281700134, b is 0.5124354362487793\n",
            "36\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 36, loss is: 2.8587450981140137, w is: -0.6566079258918762, b is 0.5073111057281494\n",
            "37\n",
            "tensor([[-1.4305e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 37, loss is: 2.80595326423645, w is: -0.6411305665969849, b is 0.5022379755973816\n",
            "38\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 38, loss is: 2.754211902618408, w is: -0.6258079409599304, b is 0.49721559882164\n",
            "39\n",
            "tensor([[1.1921e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 39, loss is: 2.7035000324249268, w is: -0.6106385588645935, b is 0.4922434389591217\n",
            "40\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 40, loss is: 2.6537978649139404, w is: -0.59562087059021, b is 0.48732101917266846\n",
            "41\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 41, loss is: 2.605084180831909, w is: -0.5807533264160156, b is 0.4824478030204773\n",
            "42\n",
            "tensor([[-1.4305e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 42, loss is: 2.557340145111084, w is: -0.5660344958305359, b is 0.47762331366539\n",
            "43\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 43, loss is: 2.5105462074279785, w is: -0.5514628291130066, b is 0.4728470742702484\n",
            "44\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 44, loss is: 2.464683771133423, w is: -0.5370368957519531, b is 0.4681186079978943\n",
            "45\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 45, loss is: 2.4197332859039307, w is: -0.5227552056312561, b is 0.46343740820884705\n",
            "46\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 46, loss is: 2.3756778240203857, w is: -0.5086163282394409, b is 0.4588030278682709\n",
            "47\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 47, loss is: 2.3324990272521973, w is: -0.49461886286735535, b is 0.45421499013900757\n",
            "48\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 48, loss is: 2.290179491043091, w is: -0.4807613790035248, b is 0.4496728479862213\n",
            "49\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 49, loss is: 2.248702049255371, w is: -0.467042475938797, b is 0.4451761245727539\n",
            "50\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 50, loss is: 2.208050012588501, w is: -0.4534607529640198, b is 0.44072437286376953\n",
            "51\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 51, loss is: 2.1682066917419434, w is: -0.4400148391723633, b is 0.43631711602211\n",
            "52\n",
            "tensor([[9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 52, loss is: 2.1291565895080566, w is: -0.42670339345932007, b is 0.43195393681526184\n",
            "53\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 53, loss is: 2.090883731842041, w is: -0.4135250449180603, b is 0.4276343882083893\n",
            "54\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 54, loss is: 2.0533719062805176, w is: -0.4004784822463989, b is 0.4233580529689789\n",
            "55\n",
            "tensor([[1.1921e-06]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 55, loss is: 2.0166072845458984, w is: -0.3875623941421509, b is 0.4191244840621948\n",
            "56\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 56, loss is: 1.9805738925933838, w is: -0.3747754693031311, b is 0.4149332344532013\n",
            "57\n",
            "tensor([[-9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 57, loss is: 1.9452577829360962, w is: -0.36211639642715454, b is 0.41078391671180725\n",
            "58\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 58, loss is: 1.9106441736221313, w is: -0.3495839238166809, b is 0.4066760838031769\n",
            "59\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 59, loss is: 1.87671959400177, w is: -0.33717676997184753, b is 0.40260931849479675\n",
            "60\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 60, loss is: 1.8434699773788452, w is: -0.3248937129974365, b is 0.39858323335647583\n",
            "61\n",
            "tensor([[9.5367e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 61, loss is: 1.8108818531036377, w is: -0.3127334713935852, b is 0.3945974111557007\n",
            "62\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 62, loss is: 1.7789428234100342, w is: -0.3006948232650757, b is 0.3906514346599579\n",
            "63\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 63, loss is: 1.7476385831832886, w is: -0.28877657651901245, b is 0.3867449164390564\n",
            "64\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 64, loss is: 1.716957926750183, w is: -0.2769775092601776, b is 0.3828774690628052\n",
            "65\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 65, loss is: 1.6868876218795776, w is: -0.26529642939567566, b is 0.3790487051010132\n",
            "66\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 66, loss is: 1.6574156284332275, w is: -0.25373217463493347, b is 0.375258207321167\n",
            "67\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 67, loss is: 1.6285300254821777, w is: -0.24228355288505554, b is 0.37150561809539795\n",
            "68\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 68, loss is: 1.6002197265625, w is: -0.23094941675662994, b is 0.367790549993515\n",
            "69\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 69, loss is: 1.5724724531173706, w is: -0.21972861886024475, b is 0.36411264538764954\n",
            "70\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 70, loss is: 1.5452773571014404, w is: -0.20862002670764923, b is 0.3604715168476105\n",
            "71\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 71, loss is: 1.5186235904693604, w is: -0.19762252271175385, b is 0.3568668067455292\n",
            "72\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 72, loss is: 1.4925000667572021, w is: -0.18673498928546906, b is 0.3532981276512146\n",
            "73\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 73, loss is: 1.4668962955474854, w is: -0.1759563386440277, b is 0.3497651517391205\n",
            "74\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 74, loss is: 1.4418022632598877, w is: -0.16528546810150146, b is 0.3462674915790558\n",
            "75\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 75, loss is: 1.4172075986862183, w is: -0.15472131967544556, b is 0.34280481934547424\n",
            "76\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 76, loss is: 1.3931022882461548, w is: -0.14426280558109283, b is 0.3393767774105072\n",
            "77\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 77, loss is: 1.3694766759872437, w is: -0.13390886783599854, b is 0.335983008146286\n",
            "78\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 78, loss is: 1.3463213443756104, w is: -0.12365847826004028, b is 0.3326231837272644\n",
            "79\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 79, loss is: 1.3236263990402222, w is: -0.11351058632135391, b is 0.32929694652557373\n",
            "80\n",
            "tensor([[-5.9605e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 80, loss is: 1.3013836145401, w is: -0.10346417874097824, b is 0.3260039687156677\n",
            "81\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 81, loss is: 1.2795830965042114, w is: -0.0935182273387909, b is 0.3227439224720001\n",
            "82\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 82, loss is: 1.2582165002822876, w is: -0.08367174118757248, b is 0.31951647996902466\n",
            "83\n",
            "tensor([[-5.9605e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 83, loss is: 1.237275242805481, w is: -0.07392372190952301, b is 0.31632131338119507\n",
            "84\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 84, loss is: 1.2167506217956543, w is: -0.0642731785774231, b is 0.3131580948829651\n",
            "85\n",
            "tensor([[-3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 85, loss is: 1.1966344118118286, w is: -0.05471913889050484, b is 0.31002652645111084\n",
            "86\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 86, loss is: 1.1769185066223145, w is: -0.04526064172387123, b is 0.30692625045776367\n",
            "87\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 87, loss is: 1.157594919204712, w is: -0.03589673340320587, b is 0.3038569986820221\n",
            "88\n",
            "tensor([[3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 88, loss is: 1.1386560201644897, w is: -0.0266264621168375, b is 0.30081844329833984\n",
            "89\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 89, loss is: 1.1200939416885376, w is: -0.017448894679546356, b is 0.29781025648117065\n",
            "90\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 90, loss is: 1.1019010543823242, w is: -0.00836309976875782, b is 0.29483214020729065\n",
            "91\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 91, loss is: 1.0840704441070557, w is: 0.0006318371742963791, b is 0.29188382625579834\n",
            "92\n",
            "tensor([[3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 92, loss is: 1.0665944814682007, w is: 0.009536822326481342, b is 0.28896498680114746\n",
            "93\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 93, loss is: 1.0494664907455444, w is: 0.018352758139371872, b is 0.28607532382011414\n",
            "94\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 94, loss is: 1.0326793193817139, w is: 0.027080532163381577, b is 0.2832145690917969\n",
            "95\n",
            "tensor([[7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 95, loss is: 1.0162261724472046, w is: 0.035721033811569214, b is 0.2803824245929718\n",
            "96\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 96, loss is: 1.0001003742218018, w is: 0.04427512362599373, b is 0.27757859230041504\n",
            "97\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1921e-07], grad_fn=<SubBackward0>)\n",
            "iteration 97, loss is: 0.9842954277992249, w is: 0.05274367332458496, b is 0.2748028039932251\n",
            "98\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 98, loss is: 0.9688051342964172, w is: 0.061127543449401855, b is 0.2720547616481781\n",
            "99\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 99, loss is: 0.9536231160163879, w is: 0.06942757219076157, b is 0.26933422684669495\n",
            "100\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 100, loss is: 0.9387431740760803, w is: 0.07764460146427155, b is 0.26664087176322937\n",
            "101\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 101, loss is: 0.9241592884063721, w is: 0.08577945828437805, b is 0.26397445797920227\n",
            "102\n",
            "tensor([[-3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 102, loss is: 0.9098656177520752, w is: 0.09383296966552734, b is 0.26133471727371216\n",
            "103\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 103, loss is: 0.8958564400672913, w is: 0.10180594772100449, b is 0.25872138142585754\n",
            "104\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 104, loss is: 0.8821260929107666, w is: 0.10969918966293335, b is 0.25613418221473694\n",
            "105\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 105, loss is: 0.8686689138412476, w is: 0.1175135001540184, b is 0.25357285141944885\n",
            "106\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 106, loss is: 0.8554794788360596, w is: 0.12524966895580292, b is 0.2510371208190918\n",
            "107\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([8.9407e-08], grad_fn=<SubBackward0>)\n",
            "iteration 107, loss is: 0.8425526022911072, w is: 0.13290847837924957, b is 0.24852675199508667\n",
            "108\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 108, loss is: 0.8298830389976501, w is: 0.14049069583415985, b is 0.24604147672653198\n",
            "109\n",
            "tensor([[-5.9605e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 109, loss is: 0.8174654245376587, w is: 0.14799709618091583, b is 0.24358105659484863\n",
            "110\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 110, loss is: 0.8052950501441956, w is: 0.1554284393787384, b is 0.24114525318145752\n",
            "111\n",
            "tensor([[-4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 111, loss is: 0.7933667898178101, w is: 0.16278545558452606, b is 0.23873379826545715\n",
            "112\n",
            "tensor([[5.9605e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 112, loss is: 0.7816758751869202, w is: 0.1700689047574997, b is 0.23634645342826843\n",
            "113\n",
            "tensor([[5.9605e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 113, loss is: 0.7702177166938782, w is: 0.1772795170545578, b is 0.23398299515247345\n",
            "114\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 114, loss is: 0.758987545967102, w is: 0.18441802263259888, b is 0.2316431701183319\n",
            "115\n",
            "tensor([[-7.1526e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 115, loss is: 0.7479808330535889, w is: 0.19148515164852142, b is 0.2293267399072647\n",
            "116\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 116, loss is: 0.7371930480003357, w is: 0.19848160445690155, b is 0.22703346610069275\n",
            "117\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 117, loss is: 0.7266201972961426, w is: 0.20540809631347656, b is 0.22476312518119812\n",
            "118\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 118, loss is: 0.7162575125694275, w is: 0.21226531267166138, b is 0.22251549363136292\n",
            "119\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 119, loss is: 0.7061011791229248, w is: 0.2190539687871933, b is 0.22029033303260803\n",
            "120\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 120, loss is: 0.6961469054222107, w is: 0.22577473521232605, b is 0.21808743476867676\n",
            "121\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 121, loss is: 0.6863906979560852, w is: 0.23242829740047455, b is 0.21590656042099\n",
            "122\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 122, loss is: 0.6768286824226379, w is: 0.23901531100273132, b is 0.21374750137329102\n",
            "123\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 123, loss is: 0.6674569249153137, w is: 0.2455364614725113, b is 0.21161001920700073\n",
            "124\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 124, loss is: 0.6582716703414917, w is: 0.251992404460907, b is 0.20949392020702362\n",
            "125\n",
            "tensor([[-3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 125, loss is: 0.649269163608551, w is: 0.25838378071784973, b is 0.20739898085594177\n",
            "126\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 126, loss is: 0.6404458284378052, w is: 0.26471126079559326, b is 0.20532499253749847\n",
            "127\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 127, loss is: 0.6317981481552124, w is: 0.2709754407405853, b is 0.203271746635437\n",
            "128\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 128, loss is: 0.623322606086731, w is: 0.27717697620391846, b is 0.20123903453350067\n",
            "129\n",
            "tensor([[-3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 129, loss is: 0.6150155067443848, w is: 0.2833165228366852, b is 0.19922664761543274\n",
            "130\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 130, loss is: 0.6068738698959351, w is: 0.28939467668533325, b is 0.1972343772649765\n",
            "131\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 131, loss is: 0.5988941192626953, w is: 0.2954120337963104, b is 0.19526202976703644\n",
            "132\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 132, loss is: 0.5910733938217163, w is: 0.30136922001838684, b is 0.19330941140651703\n",
            "133\n",
            "tensor([[4.7684e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 133, loss is: 0.5834081172943115, w is: 0.30726683139801025, b is 0.19137631356716156\n",
            "134\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 134, loss is: 0.5758954286575317, w is: 0.3131054639816284, b is 0.1894625574350357\n",
            "135\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 135, loss is: 0.5685322880744934, w is: 0.3188857138156891, b is 0.18756793439388275\n",
            "136\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 136, loss is: 0.561315655708313, w is: 0.3246081471443176, b is 0.18569225072860718\n",
            "137\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 137, loss is: 0.5542425513267517, w is: 0.3302733600139618, b is 0.18383532762527466\n",
            "138\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 138, loss is: 0.5473103523254395, w is: 0.33588191866874695, b is 0.18199697136878967\n",
            "139\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 139, loss is: 0.5405160188674927, w is: 0.34143438935279846, b is 0.1801770031452179\n",
            "140\n",
            "tensor([[2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 140, loss is: 0.5338569283485413, w is: 0.3469313383102417, b is 0.1783752292394638\n",
            "141\n",
            "tensor([[-3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 141, loss is: 0.5273302793502808, w is: 0.352373331785202, b is 0.17659147083759308\n",
            "142\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 142, loss is: 0.5209335684776306, w is: 0.3577609062194824, b is 0.1748255491256714\n",
            "143\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 143, loss is: 0.5146641135215759, w is: 0.36309459805488586, b is 0.1730773001909256\n",
            "144\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 144, loss is: 0.5085194706916809, w is: 0.36837494373321533, b is 0.1713465303182602\n",
            "145\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 145, loss is: 0.5024970769882202, w is: 0.3736025094985962, b is 0.16963306069374084\n",
            "146\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 146, loss is: 0.49659448862075806, w is: 0.37877780199050903, b is 0.16793672740459442\n",
            "147\n",
            "tensor([[-2.3842e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 147, loss is: 0.49080944061279297, w is: 0.38390132784843445, b is 0.1662573665380478\n",
            "148\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 148, loss is: 0.48513948917388916, w is: 0.3889736235141754, b is 0.16459479928016663\n",
            "149\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 149, loss is: 0.479582279920578, w is: 0.3939951956272125, b is 0.1629488468170166\n",
            "150\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 150, loss is: 0.47413572669029236, w is: 0.39896655082702637, b is 0.16131936013698578\n",
            "151\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 151, loss is: 0.46879756450653076, w is: 0.40388819575309753, b is 0.15970616042613983\n",
            "152\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 152, loss is: 0.4635656476020813, w is: 0.40876060724258423, b is 0.15810909867286682\n",
            "153\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 153, loss is: 0.45843788981437683, w is: 0.41358429193496704, b is 0.15652801096439362\n",
            "154\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 154, loss is: 0.45341208577156067, w is: 0.41835975646972656, b is 0.15496273338794708\n",
            "155\n",
            "tensor([[-1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 155, loss is: 0.448486328125, w is: 0.423087477684021, b is 0.1534131020307541\n",
            "156\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 156, loss is: 0.44365859031677246, w is: 0.42776790261268616, b is 0.1518789678812027\n",
            "157\n",
            "tensor([[2.9802e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 157, loss is: 0.43892690539360046, w is: 0.43240153789520264, b is 0.15036018192768097\n",
            "158\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 158, loss is: 0.43428942561149597, w is: 0.43698883056640625, b is 0.14885658025741577\n",
            "159\n",
            "tensor([[3.5763e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 159, loss is: 0.429744154214859, w is: 0.4415302574634552, b is 0.14736801385879517\n",
            "160\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 160, loss is: 0.42528942227363586, w is: 0.4460262656211853, b is 0.14589433372020721\n",
            "161\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 161, loss is: 0.42092326283454895, w is: 0.4504773020744324, b is 0.14443539083003998\n",
            "162\n",
            "tensor([[1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 162, loss is: 0.41664403676986694, w is: 0.4548838436603546, b is 0.14299103617668152\n",
            "163\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 163, loss is: 0.41244998574256897, w is: 0.45924630761146545, b is 0.1415611207485199\n",
            "164\n",
            "tensor([[1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 164, loss is: 0.40833938121795654, w is: 0.4635651409626007, b is 0.14014551043510437\n",
            "165\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 165, loss is: 0.40431055426597595, w is: 0.4678407907485962, b is 0.138744056224823\n",
            "166\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 166, loss is: 0.4003619849681854, w is: 0.47207367420196533, b is 0.13735660910606384\n",
            "167\n",
            "tensor([[-1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 167, loss is: 0.3964918851852417, w is: 0.47626423835754395, b is 0.13598304986953735\n",
            "168\n",
            "tensor([[-1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 168, loss is: 0.39269882440567017, w is: 0.48041290044784546, b is 0.1346232146024704\n",
            "169\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 169, loss is: 0.38898131251335144, w is: 0.4845200777053833, b is 0.13327698409557343\n",
            "170\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 170, loss is: 0.3853376805782318, w is: 0.4885861873626709, b is 0.1319442093372345\n",
            "171\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 171, loss is: 0.3817666172981262, w is: 0.4926116168498993, b is 0.13062477111816406\n",
            "172\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9605e-08], grad_fn=<SubBackward0>)\n",
            "iteration 172, loss is: 0.3782666027545929, w is: 0.4965968132019043, b is 0.12931852042675018\n",
            "173\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 173, loss is: 0.3748362362384796, w is: 0.500542163848877, b is 0.1280253380537033\n",
            "174\n",
            "tensor([[-1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 174, loss is: 0.37147411704063416, w is: 0.5044480562210083, b is 0.1267450898885727\n",
            "175\n",
            "tensor([[-1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 175, loss is: 0.36817893385887146, w is: 0.5083149075508118, b is 0.1254776418209076\n",
            "176\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 176, loss is: 0.3649492859840393, w is: 0.512143075466156, b is 0.12422286719083786\n",
            "177\n",
            "tensor([[-1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 177, loss is: 0.3617839217185974, w is: 0.5159329771995544, b is 0.12298063933849335\n",
            "178\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 178, loss is: 0.3586815595626831, w is: 0.5196849703788757, b is 0.1217508316040039\n",
            "179\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.4703e-08], grad_fn=<SubBackward0>)\n",
            "iteration 179, loss is: 0.3556409180164337, w is: 0.5233994126319885, b is 0.12053332477807999\n",
            "180\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 180, loss is: 0.3526608347892761, w is: 0.5270767211914062, b is 0.11932799220085144\n",
            "181\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 181, loss is: 0.34973999857902527, w is: 0.5307172536849976, b is 0.11813471466302872\n",
            "182\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 182, loss is: 0.3468773365020752, w is: 0.5343213677406311, b is 0.11695336550474167\n",
            "183\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 183, loss is: 0.3440715968608856, w is: 0.5378894805908203, b is 0.11578383296728134\n",
            "184\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 184, loss is: 0.3413217067718506, w is: 0.5414218902587891, b is 0.11462599784135818\n",
            "185\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 185, loss is: 0.33862656354904175, w is: 0.544918954372406, b is 0.11347974091768265\n",
            "186\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 186, loss is: 0.335985004901886, w is: 0.5483810901641846, b is 0.11234494298696518\n",
            "187\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 187, loss is: 0.3333960771560669, w is: 0.5518085956573486, b is 0.11122149229049683\n",
            "188\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 188, loss is: 0.33085858821868896, w is: 0.5552018284797668, b is 0.11010927706956863\n",
            "189\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 189, loss is: 0.32837167382240295, w is: 0.5585610866546631, b is 0.10900818556547165\n",
            "190\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 190, loss is: 0.32593420147895813, w is: 0.5618867874145508, b is 0.10791810601949692\n",
            "191\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 191, loss is: 0.32354527711868286, w is: 0.5651792287826538, b is 0.10683892667293549\n",
            "192\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 192, loss is: 0.3212038576602936, w is: 0.5684387683868408, b is 0.1057705357670784\n",
            "193\n",
            "tensor([[-1.7881e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 193, loss is: 0.31890901923179626, w is: 0.5716657042503357, b is 0.1047128289937973\n",
            "194\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 194, loss is: 0.3166598677635193, w is: 0.5748603343963623, b is 0.10366570204496384\n",
            "195\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 195, loss is: 0.314455509185791, w is: 0.5780230164527893, b is 0.10262904316186905\n",
            "196\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 196, loss is: 0.31229496002197266, w is: 0.5811541080474854, b is 0.10160275548696518\n",
            "197\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 197, loss is: 0.31017744541168213, w is: 0.5842538475990295, b is 0.10058672726154327\n",
            "198\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 198, loss is: 0.3081020712852478, w is: 0.5873225927352905, b is 0.09958086162805557\n",
            "199\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 199, loss is: 0.30606797337532043, w is: 0.590360701084137, b is 0.09858505427837372\n",
            "200\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 200, loss is: 0.30407437682151794, w is: 0.593368411064148, b is 0.09759920090436935\n",
            "201\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 201, loss is: 0.3021204471588135, w is: 0.5963460206985474, b is 0.09662321209907532\n",
            "202\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 202, loss is: 0.3002053499221802, w is: 0.5992938876152039, b is 0.09565698355436325\n",
            "203\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 203, loss is: 0.2983284294605255, w is: 0.6022122502326965, b is 0.0947004109621048\n",
            "204\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 204, loss is: 0.29648885130882263, w is: 0.6051014065742493, b is 0.0937534049153328\n",
            "205\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 205, loss is: 0.29468584060668945, w is: 0.6079617142677307, b is 0.09281586855649948\n",
            "206\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 206, loss is: 0.29291874170303345, w is: 0.61079341173172, b is 0.0918877124786377\n",
            "207\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 207, loss is: 0.29118677973747253, w is: 0.6135967969894409, b is 0.09096883237361908\n",
            "208\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 208, loss is: 0.2894893288612366, w is: 0.6163721084594727, b is 0.09005914628505707\n",
            "209\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 209, loss is: 0.2878256142139435, w is: 0.6191197037696838, b is 0.0891585573554039\n",
            "210\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 210, loss is: 0.2861950397491455, w is: 0.6218398213386536, b is 0.08826696872711182\n",
            "211\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 211, loss is: 0.28459689021110535, w is: 0.6245326995849609, b is 0.08738429844379425\n",
            "212\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 212, loss is: 0.28303053975105286, w is: 0.6271986961364746, b is 0.08651045709848404\n",
            "213\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 213, loss is: 0.28149542212486267, w is: 0.6298379898071289, b is 0.08564535528421402\n",
            "214\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 214, loss is: 0.2799907922744751, w is: 0.6324509382247925, b is 0.08478890359401703\n",
            "215\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 215, loss is: 0.27851611375808716, w is: 0.6350377202033997, b is 0.0839410126209259\n",
            "216\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 216, loss is: 0.2770708203315735, w is: 0.6375986337661743, b is 0.08310160040855408\n",
            "217\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 217, loss is: 0.2756541967391968, w is: 0.6401339769363403, b is 0.08227058500051498\n",
            "218\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 218, loss is: 0.274265855550766, w is: 0.642643928527832, b is 0.08144787698984146\n",
            "219\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 219, loss is: 0.2729050815105438, w is: 0.6451287865638733, b is 0.08063340187072754\n",
            "220\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 220, loss is: 0.27157145738601685, w is: 0.6475887894630432, b is 0.07982707023620605\n",
            "221\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 221, loss is: 0.27026429772377014, w is: 0.6500241756439209, b is 0.07902880012989044\n",
            "222\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 222, loss is: 0.2689831852912903, w is: 0.6524352431297302, b is 0.07823850959539413\n",
            "223\n",
            "tensor([[8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 223, loss is: 0.2677275836467743, w is: 0.6548221707344055, b is 0.07745612412691116\n",
            "224\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 224, loss is: 0.2664969265460968, w is: 0.6571852564811707, b is 0.07668156176805496\n",
            "225\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 225, loss is: 0.2652907967567444, w is: 0.65952467918396, b is 0.07591474801301956\n",
            "226\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 226, loss is: 0.26410865783691406, w is: 0.6618407368659973, b is 0.0751556009054184\n",
            "227\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 227, loss is: 0.26295006275177, w is: 0.664133608341217, b is 0.0744040459394455\n",
            "228\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 228, loss is: 0.26181450486183167, w is: 0.666403591632843, b is 0.07366000860929489\n",
            "229\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 229, loss is: 0.2607015073299408, w is: 0.6686508655548096, b is 0.07292340695858002\n",
            "230\n",
            "tensor([[1.4901e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 230, loss is: 0.25961071252822876, w is: 0.6708756685256958, b is 0.0721941739320755\n",
            "231\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 231, loss is: 0.2585415542125702, w is: 0.6730782389640808, b is 0.07147223502397537\n",
            "232\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 232, loss is: 0.2574937343597412, w is: 0.6752587556838989, b is 0.07075751572847366\n",
            "233\n",
            "tensor([[8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 233, loss is: 0.2564667761325836, w is: 0.6774174571037292, b is 0.0700499415397644\n",
            "234\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 234, loss is: 0.2554602026939392, w is: 0.6795545816421509, b is 0.06934944540262222\n",
            "235\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 235, loss is: 0.2544736862182617, w is: 0.6816703677177429, b is 0.06865595281124115\n",
            "236\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 236, loss is: 0.25350677967071533, w is: 0.6837649941444397, b is 0.06796939671039581\n",
            "237\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 237, loss is: 0.25255918502807617, w is: 0.6858386397361755, b is 0.06728970259428024\n",
            "238\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 238, loss is: 0.25163039565086365, w is: 0.6878915429115295, b is 0.06661680340766907\n",
            "239\n",
            "tensor([[1.4901e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 239, loss is: 0.2507200837135315, w is: 0.6899239420890808, b is 0.06595063209533691\n",
            "240\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 240, loss is: 0.24982787668704987, w is: 0.6919360160827637, b is 0.06529112905263901\n",
            "241\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 241, loss is: 0.2489534616470337, w is: 0.6939279437065125, b is 0.06463821977376938\n",
            "242\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 242, loss is: 0.24809640645980835, w is: 0.6958999633789062, b is 0.06399183720350266\n",
            "243\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 243, loss is: 0.24725645780563354, w is: 0.6978522539138794, b is 0.06335192173719406\n",
            "244\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 244, loss is: 0.24643316864967346, w is: 0.699785053730011, b is 0.06271839886903763\n",
            "245\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 245, loss is: 0.2456263154745102, w is: 0.7016984820365906, b is 0.06209121644496918\n",
            "246\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2352e-08], grad_fn=<SubBackward0>)\n",
            "iteration 246, loss is: 0.2448354959487915, w is: 0.7035927772521973, b is 0.06147030368447304\n",
            "247\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 247, loss is: 0.24406039714813232, w is: 0.7054681777954102, b is 0.060855600982904434\n",
            "248\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2352e-08], grad_fn=<SubBackward0>)\n",
            "iteration 248, loss is: 0.24330078065395355, w is: 0.7073248028755188, b is 0.06024704501032829\n",
            "249\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 249, loss is: 0.24255625903606415, w is: 0.7091628313064575, b is 0.05964457616209984\n",
            "250\n",
            "tensor([[1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 250, loss is: 0.24182648956775665, w is: 0.7109825015068054, b is 0.059048131108284\n",
            "251\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9802e-08], grad_fn=<SubBackward0>)\n",
            "iteration 251, loss is: 0.24111130833625793, w is: 0.7127839922904968, b is 0.05845765024423599\n",
            "252\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 252, loss is: 0.24041029810905457, w is: 0.7145674824714661, b is 0.05787307396531105\n",
            "253\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 253, loss is: 0.23972328007221222, w is: 0.7163330912590027, b is 0.057294342666864395\n",
            "254\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 254, loss is: 0.23904992640018463, w is: 0.7180810570716858, b is 0.05672140046954155\n",
            "255\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 255, loss is: 0.2383899986743927, w is: 0.7198115587234497, b is 0.05615418776869774\n",
            "256\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2352e-08], grad_fn=<SubBackward0>)\n",
            "iteration 256, loss is: 0.23774316906929016, w is: 0.7215247750282288, b is 0.05559264495968819\n",
            "257\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 257, loss is: 0.2371092140674591, w is: 0.7232208251953125, b is 0.055036719888448715\n",
            "258\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 258, loss is: 0.23648788034915924, w is: 0.72489994764328, b is 0.05448635295033455\n",
            "259\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 259, loss is: 0.23587891459465027, w is: 0.7265622615814209, b is 0.05394148826599121\n",
            "260\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 260, loss is: 0.2352820634841919, w is: 0.7282079458236694, b is 0.05340207368135452\n",
            "261\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 261, loss is: 0.23469707369804382, w is: 0.72983717918396, b is 0.05286805331707001\n",
            "262\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 262, loss is: 0.23412373661994934, w is: 0.7314501404762268, b is 0.05233937129378319\n",
            "263\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 263, loss is: 0.23356182873249054, w is: 0.7330469489097595, b is 0.051815979182720184\n",
            "264\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 264, loss is: 0.23301108181476593, w is: 0.7346277832984924, b is 0.05129782110452652\n",
            "265\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 265, loss is: 0.2324712872505188, w is: 0.7361928224563599, b is 0.05078484117984772\n",
            "266\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 266, loss is: 0.23194225132465363, w is: 0.7377421855926514, b is 0.050276994705200195\n",
            "267\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 267, loss is: 0.2314237803220749, w is: 0.7392760515213013, b is 0.04977422580122948\n",
            "268\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 268, loss is: 0.23091554641723633, w is: 0.7407945990562439, b is 0.049276482313871384\n",
            "269\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 269, loss is: 0.23041753470897675, w is: 0.7422979474067688, b is 0.048783715814352036\n",
            "270\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 270, loss is: 0.22992932796478271, w is: 0.7437862753868103, b is 0.04829587787389755\n",
            "271\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 271, loss is: 0.22945085167884827, w is: 0.745259702205658, b is 0.047812920063734055\n",
            "272\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 272, loss is: 0.22898194193840027, w is: 0.7467184066772461, b is 0.04733479022979736\n",
            "273\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 273, loss is: 0.22852236032485962, w is: 0.7481625080108643, b is 0.0468614436686039\n",
            "274\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 274, loss is: 0.2280719131231308, w is: 0.7495921850204468, b is 0.04639282822608948\n",
            "275\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 275, loss is: 0.22763043642044067, w is: 0.7510075569152832, b is 0.04592889919877052\n",
            "276\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 276, loss is: 0.22719770669937134, w is: 0.7524088025093079, b is 0.04546961188316345\n",
            "277\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 277, loss is: 0.22677361965179443, w is: 0.7537960410118103, b is 0.045014914125204086\n",
            "278\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 278, loss is: 0.22635796666145325, w is: 0.7551693916320801, b is 0.04456476494669914\n",
            "279\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 279, loss is: 0.22595058381557465, w is: 0.7565289735794067, b is 0.04411911591887474\n",
            "280\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 280, loss is: 0.2255513221025467, w is: 0.7578749656677246, b is 0.0436779260635376\n",
            "281\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 281, loss is: 0.22515995800495148, w is: 0.759207546710968, b is 0.043241146951913834\n",
            "282\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 282, loss is: 0.22477644681930542, w is: 0.7605267763137817, b is 0.04280873388051987\n",
            "283\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 283, loss is: 0.22440052032470703, w is: 0.7618328332901001, b is 0.042380645871162415\n",
            "284\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 284, loss is: 0.22403208911418915, w is: 0.7631258368492126, b is 0.041956838220357895\n",
            "285\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 285, loss is: 0.22367098927497864, w is: 0.7644059062004089, b is 0.041537269949913025\n",
            "286\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 286, loss is: 0.22331705689430237, w is: 0.7656731605529785, b is 0.04112189635634422\n",
            "287\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 287, loss is: 0.22297020256519318, w is: 0.7669277191162109, b is 0.040710676461458206\n",
            "288\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 288, loss is: 0.22263026237487793, w is: 0.7681697607040405, b is 0.04030356928706169\n",
            "289\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 289, loss is: 0.2222970426082611, w is: 0.7693993449211121, b is 0.039900533854961395\n",
            "290\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 290, loss is: 0.22197049856185913, w is: 0.7706166505813599, b is 0.039501529186964035\n",
            "291\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 291, loss is: 0.2216503918170929, w is: 0.7718217968940735, b is 0.03910651430487633\n",
            "292\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 292, loss is: 0.22133669257164001, w is: 0.7730149030685425, b is 0.03871544823050499\n",
            "293\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 293, loss is: 0.22102925181388855, w is: 0.7741960287094116, b is 0.03832829371094704\n",
            "294\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 294, loss is: 0.22072790563106537, w is: 0.7753653526306152, b is 0.037945009768009186\n",
            "295\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 295, loss is: 0.2204325795173645, w is: 0.7765229940414429, b is 0.03756555914878845\n",
            "296\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 296, loss is: 0.22014309465885162, w is: 0.7776690721511841, b is 0.03718990460038185\n",
            "297\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 297, loss is: 0.21985936164855957, w is: 0.7788037061691284, b is 0.0368180051445961\n",
            "298\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 298, loss is: 0.21958133578300476, w is: 0.7799269556999207, b is 0.036449823528528214\n",
            "299\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 299, loss is: 0.2193087935447693, w is: 0.7810389995574951, b is 0.036085326224565506\n",
            "300\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 300, loss is: 0.21904172003269196, w is: 0.7821398973464966, b is 0.035724472254514694\n",
            "301\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 301, loss is: 0.21877990663051605, w is: 0.7832298278808594, b is 0.03536722809076309\n",
            "302\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 302, loss is: 0.21852335333824158, w is: 0.7843088507652283, b is 0.035013556480407715\n",
            "303\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 303, loss is: 0.218271866440773, w is: 0.7853770852088928, b is 0.03466342017054558\n",
            "304\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 304, loss is: 0.21802540123462677, w is: 0.7864345908164978, b is 0.034316785633563995\n",
            "305\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 305, loss is: 0.2177838236093521, w is: 0.7874815464019775, b is 0.03397361934185028\n",
            "306\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 306, loss is: 0.21754707396030426, w is: 0.7885180115699768, b is 0.03363388404250145\n",
            "307\n",
            "tensor([[-1.3411e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 307, loss is: 0.21731501817703247, w is: 0.7895441651344299, b is 0.033297546207904816\n",
            "308\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 308, loss is: 0.21708761155605316, w is: 0.7905600070953369, b is 0.03296457231044769\n",
            "309\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 309, loss is: 0.2168646901845932, w is: 0.7915657162666321, b is 0.0326349250972271\n",
            "310\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 310, loss is: 0.2166462391614914, w is: 0.7925613522529602, b is 0.03230857476592064\n",
            "311\n",
            "tensor([[7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 311, loss is: 0.216432124376297, w is: 0.7935470342636108, b is 0.031985487788915634\n",
            "312\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 312, loss is: 0.21622227132320404, w is: 0.7945228815078735, b is 0.031665634363889694\n",
            "313\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 313, loss is: 0.21601659059524536, w is: 0.7954889535903931, b is 0.031348977237939835\n",
            "314\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 314, loss is: 0.2158149927854538, w is: 0.796445369720459, b is 0.031035486608743668\n",
            "315\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 315, loss is: 0.21561741828918457, w is: 0.7973922491073608, b is 0.030725132673978806\n",
            "316\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 316, loss is: 0.21542376279830933, w is: 0.7983296513557434, b is 0.030417881906032562\n",
            "317\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 317, loss is: 0.21523398160934448, w is: 0.7992576360702515, b is 0.0301137026399374\n",
            "318\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.6077e-08], grad_fn=<SubBackward0>)\n",
            "iteration 318, loss is: 0.2150479555130005, w is: 0.8001763820648193, b is 0.029812565073370934\n",
            "319\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 319, loss is: 0.21486562490463257, w is: 0.8010859489440918, b is 0.029514439404010773\n",
            "320\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 320, loss is: 0.21468695998191833, w is: 0.8019863963127136, b is 0.02921929582953453\n",
            "321\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 321, loss is: 0.21451182663440704, w is: 0.8028778433799744, b is 0.02892710268497467\n",
            "322\n",
            "tensor([[7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 322, loss is: 0.2143401801586151, w is: 0.8037603497505188, b is 0.028637832030653954\n",
            "323\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 323, loss is: 0.21417196094989777, w is: 0.8046340346336365, b is 0.028351454064249992\n",
            "324\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 324, loss is: 0.21400707960128784, w is: 0.8054990172386169, b is 0.0280679389834404\n",
            "325\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 325, loss is: 0.21384549140930176, w is: 0.806355357170105, b is 0.027787258848547935\n",
            "326\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 326, loss is: 0.21368710696697235, w is: 0.8072031140327454, b is 0.027509385719895363\n",
            "327\n",
            "tensor([[-1.1921e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 327, loss is: 0.21353185176849365, w is: 0.8080424070358276, b is 0.027234291657805443\n",
            "328\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 328, loss is: 0.21337969601154327, w is: 0.8088732957839966, b is 0.026961948722600937\n",
            "329\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 329, loss is: 0.21323060989379883, w is: 0.809695839881897, b is 0.026692328974604607\n",
            "330\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 330, loss is: 0.21308445930480957, w is: 0.8105101585388184, b is 0.026425406336784363\n",
            "331\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.4901e-08], grad_fn=<SubBackward0>)\n",
            "iteration 331, loss is: 0.2129412293434143, w is: 0.8113163709640503, b is 0.026161152869462967\n",
            "332\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 332, loss is: 0.21280083060264587, w is: 0.8121145367622375, b is 0.02589954063296318\n",
            "333\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 333, loss is: 0.21266324818134308, w is: 0.8129047155380249, b is 0.025640545412898064\n",
            "334\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 334, loss is: 0.21252834796905518, w is: 0.8136869668960571, b is 0.025384139269590378\n",
            "335\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 335, loss is: 0.21239620447158813, w is: 0.8144614100456238, b is 0.025130297988653183\n",
            "336\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 336, loss is: 0.21226665377616882, w is: 0.8152281045913696, b is 0.02487899549305439\n",
            "337\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 337, loss is: 0.21213969588279724, w is: 0.8159871101379395, b is 0.02463020570576191\n",
            "338\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 338, loss is: 0.21201525628566742, w is: 0.8167385458946228, b is 0.0243839044123888\n",
            "339\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2352e-08], grad_fn=<SubBackward0>)\n",
            "iteration 339, loss is: 0.21189327538013458, w is: 0.8174824714660645, b is 0.024140065535902977\n",
            "340\n",
            "tensor([[1.0431e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 340, loss is: 0.21177378296852112, w is: 0.8182189464569092, b is 0.023898664861917496\n",
            "341\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 341, loss is: 0.2116566002368927, w is: 0.8189480900764465, b is 0.023659678176045418\n",
            "342\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 342, loss is: 0.2115417867898941, w is: 0.8196699023246765, b is 0.023423081263899803\n",
            "343\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 343, loss is: 0.21142923831939697, w is: 0.8203845024108887, b is 0.023188849911093712\n",
            "344\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 344, loss is: 0.2113189399242401, w is: 0.8210919499397278, b is 0.022956961765885353\n",
            "345\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 345, loss is: 0.21121081709861755, w is: 0.8217923641204834, b is 0.022727392613887787\n",
            "346\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 346, loss is: 0.2111048698425293, w is: 0.8224857449531555, b is 0.022500118240714073\n",
            "347\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 347, loss is: 0.21100103855133057, w is: 0.8231722116470337, b is 0.02227511629462242\n",
            "348\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 348, loss is: 0.21089927852153778, w is: 0.8238517642021179, b is 0.02205236442387104\n",
            "349\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 349, loss is: 0.21079951524734497, w is: 0.8245245218276978, b is 0.02183184027671814\n",
            "350\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 350, loss is: 0.21070173382759094, w is: 0.8251906037330627, b is 0.02161352150142193\n",
            "351\n",
            "tensor([[-1.0431e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 351, loss is: 0.2106059044599533, w is: 0.8258500099182129, b is 0.021397385746240616\n",
            "352\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 352, loss is: 0.2105119824409485, w is: 0.826502799987793, b is 0.02118341252207756\n",
            "353\n",
            "tensor([[-1.4901e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 353, loss is: 0.21041995286941528, w is: 0.8271490931510925, b is 0.020971577614545822\n",
            "354\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 354, loss is: 0.21032971143722534, w is: 0.8277888894081116, b is 0.02076186239719391\n",
            "355\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 355, loss is: 0.21024133265018463, w is: 0.8284223079681396, b is 0.02055424451828003\n",
            "356\n",
            "tensor([[-2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 356, loss is: 0.2101546674966812, w is: 0.8290494084358215, b is 0.020348701626062393\n",
            "357\n",
            "tensor([[-2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 357, loss is: 0.21006973087787628, w is: 0.8296701908111572, b is 0.020145215094089508\n",
            "358\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 358, loss is: 0.20998647809028625, w is: 0.8302847743034363, b is 0.019943762570619583\n",
            "359\n",
            "tensor([[-2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 359, loss is: 0.20990490913391113, w is: 0.8308932185173035, b is 0.019744325429201126\n",
            "360\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 360, loss is: 0.20982490479946136, w is: 0.8314955830574036, b is 0.019546881318092346\n",
            "361\n",
            "tensor([[3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 361, loss is: 0.2097465693950653, w is: 0.8320919275283813, b is 0.01935141161084175\n",
            "362\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 362, loss is: 0.20966972410678864, w is: 0.8326823115348816, b is 0.01915789768099785\n",
            "363\n",
            "tensor([[8.1956e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 363, loss is: 0.2095944583415985, w is: 0.8332667946815491, b is 0.018966319039463997\n",
            "364\n",
            "tensor([[2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 364, loss is: 0.20952066779136658, w is: 0.8338454365730286, b is 0.018776655197143555\n",
            "365\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 365, loss is: 0.20944835245609283, w is: 0.8344182968139648, b is 0.01858888939023018\n",
            "366\n",
            "tensor([[-8.1956e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 366, loss is: 0.2093774378299713, w is: 0.8349854350090027, b is 0.018403001129627228\n",
            "367\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 367, loss is: 0.20930799841880798, w is: 0.8355469107627869, b is 0.01821897178888321\n",
            "368\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 368, loss is: 0.2092399150133133, w is: 0.8361027240753174, b is 0.01803678274154663\n",
            "369\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 369, loss is: 0.20917318761348724, w is: 0.8366529941558838, b is 0.017856415361166\n",
            "370\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 370, loss is: 0.20910777151584625, w is: 0.8371977806091309, b is 0.017677851021289825\n",
            "371\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 371, loss is: 0.2090436816215515, w is: 0.8377370834350586, b is 0.017501072958111763\n",
            "372\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 372, loss is: 0.20898082852363586, w is: 0.8382710218429565, b is 0.01732606254518032\n",
            "373\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 373, loss is: 0.20891928672790527, w is: 0.8387995958328247, b is 0.017152801156044006\n",
            "374\n",
            "tensor([[-3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 374, loss is: 0.208858922123909, w is: 0.8393229246139526, b is 0.016981273889541626\n",
            "375\n",
            "tensor([[3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 375, loss is: 0.20879976451396942, w is: 0.8398410081863403, b is 0.016811462119221687\n",
            "376\n",
            "tensor([[6.7055e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 376, loss is: 0.20874178409576416, w is: 0.8403539061546326, b is 0.016643347218632698\n",
            "377\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 377, loss is: 0.2086849808692932, w is: 0.8408616781234741, b is 0.016476914286613464\n",
            "378\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 378, loss is: 0.2086292803287506, w is: 0.8413643836975098, b is 0.016312144696712494\n",
            "379\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 379, loss is: 0.20857471227645874, w is: 0.8418620228767395, b is 0.016149023547768593\n",
            "380\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 380, loss is: 0.20852121710777283, w is: 0.8423547148704529, b is 0.015987534075975418\n",
            "381\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 381, loss is: 0.20846876502037048, w is: 0.8428424596786499, b is 0.015827659517526627\n",
            "382\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 382, loss is: 0.20841741561889648, w is: 0.8433253169059753, b is 0.015669383108615875\n",
            "383\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 383, loss is: 0.20836703479290009, w is: 0.843803346157074, b is 0.015512689016759396\n",
            "384\n",
            "tensor([[-2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 384, loss is: 0.20831766724586487, w is: 0.8442766070365906, b is 0.015357562340795994\n",
            "385\n",
            "tensor([[-2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 385, loss is: 0.20826928317546844, w is: 0.8447451591491699, b is 0.015203986316919327\n",
            "386\n",
            "tensor([[2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 386, loss is: 0.208221897482872, w is: 0.845209002494812, b is 0.0150519460439682\n",
            "387\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 387, loss is: 0.2081754058599472, w is: 0.8456681966781616, b is 0.014901426620781422\n",
            "388\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 388, loss is: 0.2081298530101776, w is: 0.8461228013038635, b is 0.014752412214875221\n",
            "389\n",
            "tensor([[-6.7055e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 389, loss is: 0.20808522403240204, w is: 0.8465728759765625, b is 0.014604887925088406\n",
            "390\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 390, loss is: 0.20804142951965332, w is: 0.8470184803009033, b is 0.014458838850259781\n",
            "391\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 391, loss is: 0.20799857378005981, w is: 0.847459614276886, b is 0.014314250089228153\n",
            "392\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 392, loss is: 0.20795653760433197, w is: 0.8478963375091553, b is 0.014171107672154903\n",
            "393\n",
            "tensor([[-2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.6764e-08], grad_fn=<SubBackward0>)\n",
            "iteration 393, loss is: 0.20791535079479218, w is: 0.8483286499977112, b is 0.014029396697878838\n",
            "394\n",
            "tensor([[-6.7055e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 394, loss is: 0.20787498354911804, w is: 0.8487566709518433, b is 0.013889102265238762\n",
            "395\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 395, loss is: 0.207835391163826, w is: 0.8491804003715515, b is 0.013750211335718632\n",
            "396\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 396, loss is: 0.2077966034412384, w is: 0.8495998978614807, b is 0.013612709008157253\n",
            "397\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 397, loss is: 0.2077586054801941, w is: 0.8500152230262756, b is 0.013476582244038582\n",
            "398\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 398, loss is: 0.20772135257720947, w is: 0.8504263758659363, b is 0.013341816142201424\n",
            "399\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 399, loss is: 0.20768485963344574, w is: 0.8508334159851074, b is 0.013208397664129734\n",
            "400\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-09], grad_fn=<SubBackward0>)\n",
            "iteration 400, loss is: 0.20764903724193573, w is: 0.8512364029884338, b is 0.013076313771307468\n",
            "401\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 401, loss is: 0.2076139748096466, w is: 0.8516353368759155, b is 0.012945550493896008\n",
            "402\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 402, loss is: 0.2075796127319336, w is: 0.8520302772521973, b is 0.012816094793379307\n",
            "403\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 403, loss is: 0.2075459063053131, w is: 0.8524212837219238, b is 0.012687933631241322\n",
            "404\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 404, loss is: 0.20751288533210754, w is: 0.8528083562850952, b is 0.012561053968966007\n",
            "405\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 405, loss is: 0.2074805349111557, w is: 0.8531915545463562, b is 0.012435443699359894\n",
            "406\n",
            "tensor([[5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 406, loss is: 0.2074488252401352, w is: 0.8535709381103516, b is 0.012311088852584362\n",
            "407\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 407, loss is: 0.20741772651672363, w is: 0.8539465069770813, b is 0.012187978252768517\n",
            "408\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 408, loss is: 0.20738725364208221, w is: 0.8543183207511902, b is 0.012066098861396313\n",
            "409\n",
            "tensor([[-1.1176e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 409, loss is: 0.20735739171504974, w is: 0.854686439037323, b is 0.011945437639951706\n",
            "410\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 410, loss is: 0.20732812583446503, w is: 0.8550508618354797, b is 0.0118259834125638\n",
            "411\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 411, loss is: 0.20729942619800568, w is: 0.8554116487503052, b is 0.011707723140716553\n",
            "412\n",
            "tensor([[-3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 412, loss is: 0.20727132260799408, w is: 0.8557688593864441, b is 0.011590645648539066\n",
            "413\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 413, loss is: 0.20724375545978546, w is: 0.8561224937438965, b is 0.011474738828837872\n",
            "414\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 414, loss is: 0.20721673965454102, w is: 0.8564725518226624, b is 0.011359991505742073\n",
            "415\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 415, loss is: 0.20719027519226074, w is: 0.8568191528320312, b is 0.0112463915720582\n",
            "416\n",
            "tensor([[-5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 416, loss is: 0.20716434717178345, w is: 0.8571622371673584, b is 0.01113392785191536\n",
            "417\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 417, loss is: 0.20713892579078674, w is: 0.8575019240379333, b is 0.011022588238120079\n",
            "418\n",
            "tensor([[-8.9407e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 418, loss is: 0.20711398124694824, w is: 0.8578382134437561, b is 0.010912362486124039\n",
            "419\n",
            "tensor([[6.7055e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 419, loss is: 0.20708955824375153, w is: 0.8581711649894714, b is 0.010803238488733768\n",
            "420\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 420, loss is: 0.2070656269788742, w is: 0.8585007786750793, b is 0.010695206001400948\n",
            "421\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 421, loss is: 0.2070421725511551, w is: 0.8588270545005798, b is 0.01058825384825468\n",
            "422\n",
            "tensor([[5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 422, loss is: 0.2070191502571106, w is: 0.8591501116752625, b is 0.010482371784746647\n",
            "423\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 423, loss is: 0.2069965898990631, w is: 0.8594698905944824, b is 0.010377547703683376\n",
            "424\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 424, loss is: 0.20697453618049622, w is: 0.8597865104675293, b is 0.010273772291839123\n",
            "425\n",
            "tensor([[-8.1956e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 425, loss is: 0.20695286989212036, w is: 0.8600999712944031, b is 0.010171034373342991\n",
            "426\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 426, loss is: 0.20693165063858032, w is: 0.8604102730751038, b is 0.01006932370364666\n",
            "427\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 427, loss is: 0.2069108635187149, w is: 0.8607174754142761, b is 0.009968630038201809\n",
            "428\n",
            "tensor([[-9.3132e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 428, loss is: 0.20689046382904053, w is: 0.8610215783119202, b is 0.009868944063782692\n",
            "429\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 429, loss is: 0.20687048137187958, w is: 0.8613226413726807, b is 0.009770254604518414\n",
            "430\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 430, loss is: 0.20685091614723206, w is: 0.8616207242012024, b is 0.009672552347183228\n",
            "431\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 431, loss is: 0.20683172345161438, w is: 0.8619158267974854, b is 0.009575827047228813\n",
            "432\n",
            "tensor([[-1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 432, loss is: 0.20681294798851013, w is: 0.8622079491615295, b is 0.00948006846010685\n",
            "433\n",
            "tensor([[7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 433, loss is: 0.20679447054862976, w is: 0.8624971508979797, b is 0.009385268203914165\n",
            "434\n",
            "tensor([[6.3330e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.3039e-08], grad_fn=<SubBackward0>)\n",
            "iteration 434, loss is: 0.20677639544010162, w is: 0.8627834916114807, b is 0.009291415102779865\n",
            "435\n",
            "tensor([[-2.6077e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 435, loss is: 0.20675870776176453, w is: 0.8630669713020325, b is 0.009198500774800777\n",
            "436\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 436, loss is: 0.2067413628101349, w is: 0.863347589969635, b is 0.009106515906751156\n",
            "437\n",
            "tensor([[-6.3330e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 437, loss is: 0.20672431588172913, w is: 0.8636254072189331, b is 0.009015451185405254\n",
            "438\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-09], grad_fn=<SubBackward0>)\n",
            "iteration 438, loss is: 0.2067076563835144, w is: 0.8639004826545715, b is 0.008925296366214752\n",
            "439\n",
            "tensor([[-7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 439, loss is: 0.20669135451316833, w is: 0.8641727566719055, b is 0.008836043067276478\n",
            "440\n",
            "tensor([[9.3132e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 440, loss is: 0.20667532086372375, w is: 0.8644423484802246, b is 0.00874768290668726\n",
            "441\n",
            "tensor([[-5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 441, loss is: 0.20665960013866425, w is: 0.8647092580795288, b is 0.008660205639898777\n",
            "442\n",
            "tensor([[-7.0781e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 442, loss is: 0.2066442370414734, w is: 0.8649734854698181, b is 0.008573603816330433\n",
            "443\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 443, loss is: 0.20662912726402283, w is: 0.8652350306510925, b is 0.008487868122756481\n",
            "444\n",
            "tensor([[-1.2666e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 444, loss is: 0.20661437511444092, w is: 0.8654940128326416, b is 0.008402989245951176\n",
            "445\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 445, loss is: 0.2065998911857605, w is: 0.8657503724098206, b is 0.00831895973533392\n",
            "446\n",
            "tensor([[3.7253e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 446, loss is: 0.20658570528030396, w is: 0.8660041689872742, b is 0.008235770277678967\n",
            "447\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 447, loss is: 0.2065717577934265, w is: 0.8662554025650024, b is 0.008153412491083145\n",
            "448\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 448, loss is: 0.20655815303325653, w is: 0.8665041327476501, b is 0.008071877993643284\n",
            "449\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 449, loss is: 0.20654477179050446, w is: 0.8667504191398621, b is 0.007991159334778786\n",
            "450\n",
            "tensor([[4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 450, loss is: 0.20653168857097626, w is: 0.8669942021369934, b is 0.00791124813258648\n",
            "451\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.3039e-08], grad_fn=<SubBackward0>)\n",
            "iteration 451, loss is: 0.20651884377002716, w is: 0.867235541343689, b is 0.007832136005163193\n",
            "452\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-09], grad_fn=<SubBackward0>)\n",
            "iteration 452, loss is: 0.20650626718997955, w is: 0.8674744963645935, b is 0.007753814570605755\n",
            "453\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 453, loss is: 0.20649394392967224, w is: 0.867711067199707, b is 0.007676276378333569\n",
            "454\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 454, loss is: 0.20648184418678284, w is: 0.8679452538490295, b is 0.00759951351210475\n",
            "455\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 455, loss is: 0.20646996796131134, w is: 0.8681771159172058, b is 0.007523518521338701\n",
            "456\n",
            "tensor([[-4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 456, loss is: 0.20645837485790253, w is: 0.8684066534042358, b is 0.007448283489793539\n",
            "457\n",
            "tensor([[3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 457, loss is: 0.20644700527191162, w is: 0.8686338663101196, b is 0.007373800501227379\n",
            "458\n",
            "tensor([[7.0781e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 458, loss is: 0.20643584430217743, w is: 0.868858814239502, b is 0.007300062570720911\n",
            "459\n",
            "tensor([[5.5879e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 459, loss is: 0.20642492175102234, w is: 0.8690815567970276, b is 0.007227061782032251\n",
            "460\n",
            "tensor([[-3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 460, loss is: 0.20641420781612396, w is: 0.8693020343780518, b is 0.00715479115024209\n",
            "461\n",
            "tensor([[-7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 461, loss is: 0.2064036875963211, w is: 0.8695203065872192, b is 0.007083243224769831\n",
            "462\n",
            "tensor([[1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 462, loss is: 0.20639342069625854, w is: 0.8697364330291748, b is 0.007012410555034876\n",
            "463\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 463, loss is: 0.2063833326101303, w is: 0.8699503540992737, b is 0.006942286621779203\n",
            "464\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 464, loss is: 0.2063734382390976, w is: 0.8701621294021606, b is 0.006872863974422216\n",
            "465\n",
            "tensor([[-1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 465, loss is: 0.2063637673854828, w is: 0.8703718185424805, b is 0.006804135162383318\n",
            "466\n",
            "tensor([[1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 466, loss is: 0.2063542753458023, w is: 0.8705794215202332, b is 0.006736093666404486\n",
            "467\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 467, loss is: 0.20634494721889496, w is: 0.8707849383354187, b is 0.006668732967227697\n",
            "468\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 468, loss is: 0.20633584260940552, w is: 0.8709883689880371, b is 0.006602045614272356\n",
            "469\n",
            "tensor([[-1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 469, loss is: 0.20632688701152802, w is: 0.8711897730827332, b is 0.006536025088280439\n",
            "470\n",
            "tensor([[1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 470, loss is: 0.20631814002990723, w is: 0.8713891506195068, b is 0.006470664869993925\n",
            "471\n",
            "tensor([[2.0489e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 471, loss is: 0.20630954205989838, w is: 0.8715865612030029, b is 0.006405957974493504\n",
            "472\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 472, loss is: 0.20630110800266266, w is: 0.8717820048332214, b is 0.00634189834818244\n",
            "473\n",
            "tensor([[-1.3784e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 473, loss is: 0.20629285275936127, w is: 0.8719754815101624, b is 0.0062784794718027115\n",
            "474\n",
            "tensor([[-3.3528e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 474, loss is: 0.2062847763299942, w is: 0.8721670508384705, b is 0.006215694826096296\n",
            "475\n",
            "tensor([[4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 475, loss is: 0.20627683401107788, w is: 0.8723567128181458, b is 0.006153537891805172\n",
            "476\n",
            "tensor([[7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 476, loss is: 0.20626908540725708, w is: 0.8725444674491882, b is 0.0060920026153326035\n",
            "477\n",
            "tensor([[2.6077e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3970e-08], grad_fn=<SubBackward0>)\n",
            "iteration 477, loss is: 0.20626148581504822, w is: 0.8727303147315979, b is 0.0060310824774205685\n",
            "478\n",
            "tensor([[8.5682e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 478, loss is: 0.2062540054321289, w is: 0.8729143142700195, b is 0.005970771890133619\n",
            "479\n",
            "tensor([[-6.3330e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 479, loss is: 0.20624670386314392, w is: 0.8730964660644531, b is 0.005911064334213734\n",
            "480\n",
            "tensor([[-5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 480, loss is: 0.2062395215034485, w is: 0.8732768297195435, b is 0.0058519537560641766\n",
            "481\n",
            "tensor([[7.8231e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 481, loss is: 0.20623251795768738, w is: 0.8734553456306458, b is 0.005793434102088213\n",
            "482\n",
            "tensor([[-2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-09], grad_fn=<SubBackward0>)\n",
            "iteration 482, loss is: 0.20622563362121582, w is: 0.8736320734024048, b is 0.005735499784350395\n",
            "483\n",
            "tensor([[4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.5832e-08], grad_fn=<SubBackward0>)\n",
            "iteration 483, loss is: 0.2062188982963562, w is: 0.8738070726394653, b is 0.005678144749253988\n",
            "484\n",
            "tensor([[-4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 484, loss is: 0.20621225237846375, w is: 0.8739802837371826, b is 0.0056213634088635445\n",
            "485\n",
            "tensor([[-1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 485, loss is: 0.20620577037334442, w is: 0.8741517663002014, b is 0.005565149709582329\n",
            "486\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 486, loss is: 0.20619940757751465, w is: 0.8743215799331665, b is 0.005509498063474894\n",
            "487\n",
            "tensor([[-1.0803e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 487, loss is: 0.20619317889213562, w is: 0.8744896650314331, b is 0.005454402882605791\n",
            "488\n",
            "tensor([[5.5879e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0245e-08], grad_fn=<SubBackward0>)\n",
            "iteration 488, loss is: 0.20618709921836853, w is: 0.874656081199646, b is 0.005399859044700861\n",
            "489\n",
            "tensor([[4.0978e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 489, loss is: 0.2061811238527298, w is: 0.8748208284378052, b is 0.005345860496163368\n",
            "490\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 490, loss is: 0.20617525279521942, w is: 0.8749839067459106, b is 0.005292401649057865\n",
            "491\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 491, loss is: 0.2061694860458374, w is: 0.8751453757286072, b is 0.005239477381110191\n",
            "492\n",
            "tensor([[1.0058e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 492, loss is: 0.20616388320922852, w is: 0.8753052353858948, b is 0.0051870825700461864\n",
            "493\n",
            "tensor([[-1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 493, loss is: 0.2061583548784256, w is: 0.8754634857177734, b is 0.005135211627930403\n",
            "494\n",
            "tensor([[-3.7253e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 494, loss is: 0.20615294575691223, w is: 0.8756201267242432, b is 0.00508385943248868\n",
            "495\n",
            "tensor([[-3.5390e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 495, loss is: 0.20614765584468842, w is: 0.8757752180099487, b is 0.0050330208614468575\n",
            "496\n",
            "tensor([[1.2666e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 496, loss is: 0.20614244043827057, w is: 0.8759287595748901, b is 0.004982690792530775\n",
            "497\n",
            "tensor([[-1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 497, loss is: 0.20613734424114227, w is: 0.8760807514190674, b is 0.004932863637804985\n",
            "498\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 498, loss is: 0.20613236725330353, w is: 0.8762312531471252, b is 0.004883534740656614\n",
            "499\n",
            "tensor([[-4.0978e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0245e-08], grad_fn=<SubBackward0>)\n",
            "iteration 499, loss is: 0.20612750947475433, w is: 0.8763802647590637, b is 0.00483469944447279\n",
            "500\n",
            "tensor([[-1.6764e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 500, loss is: 0.20612268149852753, w is: 0.8765277862548828, b is 0.004786352626979351\n",
            "501\n",
            "tensor([[-6.1467e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 501, loss is: 0.20611797273159027, w is: 0.8766738176345825, b is 0.004738489165902138\n",
            "502\n",
            "tensor([[-1.3597e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 502, loss is: 0.20611338317394257, w is: 0.8768183588981628, b is 0.004691104404628277\n",
            "503\n",
            "tensor([[3.3528e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 503, loss is: 0.20610888302326202, w is: 0.8769614696502686, b is 0.004644193220883608\n",
            "504\n",
            "tensor([[5.5879e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 504, loss is: 0.20610444247722626, w is: 0.8771031498908997, b is 0.004597751423716545\n",
            "505\n",
            "tensor([[-2.4214e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 505, loss is: 0.20610010623931885, w is: 0.8772433996200562, b is 0.004551773890852928\n",
            "506\n",
            "tensor([[-3.3528e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 506, loss is: 0.2060958445072174, w is: 0.8773822784423828, b is 0.004506255965679884\n",
            "507\n",
            "tensor([[1.0245e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 507, loss is: 0.20609168708324432, w is: 0.8775197863578796, b is 0.004461193457245827\n",
            "508\n",
            "tensor([[5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 508, loss is: 0.2060876190662384, w is: 0.8776558637619019, b is 0.004416581708937883\n",
            "509\n",
            "tensor([[5.7742e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 509, loss is: 0.20608359575271606, w is: 0.877790629863739, b is 0.004372416064143181\n",
            "510\n",
            "tensor([[-2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 510, loss is: 0.2060796618461609, w is: 0.8779240250587463, b is 0.004328691866248846\n",
            "511\n",
            "tensor([[3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 511, loss is: 0.20607583224773407, w is: 0.8780561089515686, b is 0.004285404924303293\n",
            "512\n",
            "tensor([[3.3528e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 512, loss is: 0.20607206225395203, w is: 0.8781868815422058, b is 0.004242551047354937\n",
            "513\n",
            "tensor([[4.2841e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 513, loss is: 0.20606838166713715, w is: 0.878316342830658, b is 0.004200125578790903\n",
            "514\n",
            "tensor([[3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 514, loss is: 0.20606476068496704, w is: 0.878444492816925, b is 0.004158124327659607\n",
            "515\n",
            "tensor([[-1.2480e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 515, loss is: 0.2060612142086029, w is: 0.8785713315010071, b is 0.004116543103009462\n",
            "516\n",
            "tensor([[-8.7544e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 516, loss is: 0.20605772733688354, w is: 0.8786969184875488, b is 0.004075377713888884\n",
            "517\n",
            "tensor([[-5.4017e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 517, loss is: 0.20605434477329254, w is: 0.8788212537765503, b is 0.004034623969346285\n",
            "518\n",
            "tensor([[-2.6077e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 518, loss is: 0.20605097711086273, w is: 0.8789443373680115, b is 0.00399427767843008\n",
            "519\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 519, loss is: 0.20604772865772247, w is: 0.8790661692619324, b is 0.003954335115849972\n",
            "520\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 520, loss is: 0.2060445249080658, w is: 0.8791868090629578, b is 0.003914791624993086\n",
            "521\n",
            "tensor([[2.6077e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 521, loss is: 0.2060413658618927, w is: 0.8793062567710876, b is 0.003875643713399768\n",
            "522\n",
            "tensor([[1.6764e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 522, loss is: 0.20603826642036438, w is: 0.879424512386322, b is 0.003836887190118432\n",
            "523\n",
            "tensor([[1.6019e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 523, loss is: 0.20603527128696442, w is: 0.8795415759086609, b is 0.00379851832985878\n",
            "524\n",
            "tensor([[4.6566e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 524, loss is: 0.20603229105472565, w is: 0.8796574473381042, b is 0.0037605331744998693\n",
            "525\n",
            "tensor([[-6.7055e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 525, loss is: 0.20602940022945404, w is: 0.8797721862792969, b is 0.0037229277659207582\n",
            "526\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([9.7789e-09], grad_fn=<SubBackward0>)\n",
            "iteration 526, loss is: 0.20602655410766602, w is: 0.8798857927322388, b is 0.003685698611661792\n",
            "527\n",
            "tensor([[1.6764e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 527, loss is: 0.20602378249168396, w is: 0.8799982666969299, b is 0.0036488415207713842\n",
            "528\n",
            "tensor([[1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.1910e-09], grad_fn=<SubBackward0>)\n",
            "iteration 528, loss is: 0.20602105557918549, w is: 0.8801096081733704, b is 0.0036123530007898808\n",
            "529\n",
            "tensor([[5.5879e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.1223e-09], grad_fn=<SubBackward0>)\n",
            "iteration 529, loss is: 0.2060183435678482, w is: 0.8802198171615601, b is 0.0035762295592576265\n",
            "530\n",
            "tensor([[8.5682e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.7789e-09], grad_fn=<SubBackward0>)\n",
            "iteration 530, loss is: 0.20601573586463928, w is: 0.880328893661499, b is 0.0035404672380536795\n",
            "531\n",
            "tensor([[1.0803e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 531, loss is: 0.20601315796375275, w is: 0.880436897277832, b is 0.0035050625447183847\n",
            "532\n",
            "tensor([[7.4506e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 532, loss is: 0.20601066946983337, w is: 0.8805438280105591, b is 0.0034700119867920876\n",
            "533\n",
            "tensor([[4.0978e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 533, loss is: 0.206008180975914, w is: 0.8806496858596802, b is 0.0034353118389844894\n",
            "534\n",
            "tensor([[-4.6566e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.9849e-09], grad_fn=<SubBackward0>)\n",
            "iteration 534, loss is: 0.2060057669878006, w is: 0.8807544708251953, b is 0.0034009588416665792\n",
            "535\n",
            "tensor([[-8.1956e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 535, loss is: 0.20600338280200958, w is: 0.8808582425117493, b is 0.0033669492695480585\n",
            "536\n",
            "tensor([[-5.5879e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.3283e-09], grad_fn=<SubBackward0>)\n",
            "iteration 536, loss is: 0.20600107312202454, w is: 0.8809609413146973, b is 0.003333279862999916\n",
            "537\n",
            "tensor([[3.1665e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 537, loss is: 0.20599879324436188, w is: 0.8810626268386841, b is 0.003299947129562497\n",
            "538\n",
            "tensor([[5.5879e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 538, loss is: 0.2059965580701828, w is: 0.8811632990837097, b is 0.003266947576776147\n",
            "539\n",
            "tensor([[-3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 539, loss is: 0.2059943526983261, w is: 0.8812629580497742, b is 0.003234278177842498\n",
            "540\n",
            "tensor([[-8.5682e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.2107e-08], grad_fn=<SubBackward0>)\n",
            "iteration 540, loss is: 0.2059922218322754, w is: 0.8813616037368774, b is 0.003201935440301895\n",
            "541\n",
            "tensor([[1.1735e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 541, loss is: 0.20599010586738586, w is: 0.8814592957496643, b is 0.003169915871694684\n",
            "542\n",
            "tensor([[-7.6368e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 542, loss is: 0.20598803460597992, w is: 0.8815560340881348, b is 0.0031382166780531406\n",
            "543\n",
            "tensor([[-3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 543, loss is: 0.20598603785037994, w is: 0.881651759147644, b is 0.00310683436691761\n",
            "544\n",
            "tensor([[-3.9116e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 544, loss is: 0.20598405599594116, w is: 0.8817465305328369, b is 0.0030757661443203688\n",
            "545\n",
            "tensor([[4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 545, loss is: 0.20598210394382477, w is: 0.8818403482437134, b is 0.0030450085178017616\n",
            "546\n",
            "tensor([[9.3132e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.1223e-09], grad_fn=<SubBackward0>)\n",
            "iteration 546, loss is: 0.20598019659519196, w is: 0.8819332718849182, b is 0.0030145584605634212\n",
            "547\n",
            "tensor([[2.0489e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 547, loss is: 0.20597831904888153, w is: 0.8820252418518066, b is 0.0029844127129763365\n",
            "548\n",
            "tensor([[-1.3039e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 548, loss is: 0.20597650110721588, w is: 0.8821163177490234, b is 0.0029545684810727835\n",
            "549\n",
            "tensor([[-1.1548e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 549, loss is: 0.2059747278690338, w is: 0.8822064399719238, b is 0.0029250227380543947\n",
            "550\n",
            "tensor([[-4.0978e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 550, loss is: 0.20597298443317413, w is: 0.8822956681251526, b is 0.0028957724571228027\n",
            "551\n",
            "tensor([[7.2643e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.9162e-09], grad_fn=<SubBackward0>)\n",
            "iteration 551, loss is: 0.20597125589847565, w is: 0.8823840022087097, b is 0.00286681461147964\n",
            "552\n",
            "tensor([[2.6077e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 552, loss is: 0.20596954226493835, w is: 0.8824714422225952, b is 0.0028381464071571827\n",
            "553\n",
            "tensor([[5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.1910e-09], grad_fn=<SubBackward0>)\n",
            "iteration 553, loss is: 0.20596790313720703, w is: 0.8825580477714539, b is 0.002809765050187707\n",
            "554\n",
            "tensor([[1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 554, loss is: 0.20596632361412048, w is: 0.8826437592506409, b is 0.0027816675137728453\n",
            "555\n",
            "tensor([[7.8231e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.3283e-09], grad_fn=<SubBackward0>)\n",
            "iteration 555, loss is: 0.20596469938755035, w is: 0.882728636264801, b is 0.00275385077111423\n",
            "556\n",
            "tensor([[-1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 556, loss is: 0.2059631645679474, w is: 0.8828126788139343, b is 0.0027263122610747814\n",
            "557\n",
            "tensor([[-1.2852e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 557, loss is: 0.20596162974834442, w is: 0.882895827293396, b is 0.002699049189686775\n",
            "558\n",
            "tensor([[-1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 558, loss is: 0.20596013963222504, w is: 0.8829782009124756, b is 0.002672058530151844\n",
            "559\n",
            "tensor([[1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 559, loss is: 0.20595867931842804, w is: 0.8830597400665283, b is 0.0026453379541635513\n",
            "560\n",
            "tensor([[8.0094e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.0536e-09], grad_fn=<SubBackward0>)\n",
            "iteration 560, loss is: 0.20595724880695343, w is: 0.8831404447555542, b is 0.0026188846677541733\n",
            "561\n",
            "tensor([[4.6566e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 561, loss is: 0.20595581829547882, w is: 0.8832203149795532, b is 0.002592695876955986\n",
            "562\n",
            "tensor([[-4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 562, loss is: 0.205954447388649, w is: 0.8832994103431702, b is 0.0025667690206319094\n",
            "563\n",
            "tensor([[1.6764e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 563, loss is: 0.20595307648181915, w is: 0.883377730846405, b is 0.0025411013048142195\n",
            "564\n",
            "tensor([[8.3819e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 564, loss is: 0.2059517651796341, w is: 0.8834552764892578, b is 0.00251569040119648\n",
            "565\n",
            "tensor([[3.6322e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 565, loss is: 0.20595046877861023, w is: 0.8835320472717285, b is 0.0024905335158109665\n",
            "566\n",
            "tensor([[-2.5146e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 566, loss is: 0.20594920217990875, w is: 0.8836080431938171, b is 0.002465628320351243\n",
            "567\n",
            "tensor([[-9.3132e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([7.9162e-09], grad_fn=<SubBackward0>)\n",
            "iteration 567, loss is: 0.20594796538352966, w is: 0.8836832642555237, b is 0.0024409720208495855\n",
            "568\n",
            "tensor([[-2.5146e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 568, loss is: 0.20594671368598938, w is: 0.8837577104568481, b is 0.0024165622889995575\n",
            "569\n",
            "tensor([[4.0978e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 569, loss is: 0.20594553649425507, w is: 0.8838314414024353, b is 0.0023923965636640787\n",
            "570\n",
            "tensor([[7.5437e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 570, loss is: 0.20594437420368195, w is: 0.8839044570922852, b is 0.0023684725165367126\n",
            "571\n",
            "tensor([[7.9162e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 571, loss is: 0.20594321191310883, w is: 0.8839766979217529, b is 0.0023447878193110228\n",
            "572\n",
            "tensor([[1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.1910e-09], grad_fn=<SubBackward0>)\n",
            "iteration 572, loss is: 0.2059420794248581, w is: 0.8840482234954834, b is 0.002321339910849929\n",
            "573\n",
            "tensor([[4.3772e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 573, loss is: 0.20594099164009094, w is: 0.8841190338134766, b is 0.0022981264628469944\n",
            "574\n",
            "tensor([[-3.9116e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 574, loss is: 0.2059398889541626, w is: 0.8841891288757324, b is 0.002275145146995783\n",
            "575\n",
            "tensor([[-4.7497e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.3039e-08], grad_fn=<SubBackward0>)\n",
            "iteration 575, loss is: 0.20593884587287903, w is: 0.8842585682868958, b is 0.0022523936349898577\n",
            "576\n",
            "tensor([[3.9116e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 576, loss is: 0.20593777298927307, w is: 0.8843272924423218, b is 0.002229869831353426\n",
            "577\n",
            "tensor([[4.9360e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 577, loss is: 0.20593677461147308, w is: 0.8843953013420105, b is 0.002207570942118764\n",
            "578\n",
            "tensor([[-5.4948e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 578, loss is: 0.2059357613325119, w is: 0.8844626545906067, b is 0.002185495337471366\n",
            "579\n",
            "tensor([[0.]], grad_fn=<SubBackward0>)\n",
            "tensor([5.1223e-09], grad_fn=<SubBackward0>)\n",
            "iteration 579, loss is: 0.2059347778558731, w is: 0.8845293521881104, b is 0.002163640223443508\n",
            "580\n",
            "tensor([[1.4901e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 580, loss is: 0.2059338241815567, w is: 0.8845953345298767, b is 0.0021420037373900414\n",
            "581\n",
            "tensor([[-2.3283e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.9849e-09], grad_fn=<SubBackward0>)\n",
            "iteration 581, loss is: 0.2059328854084015, w is: 0.8846606612205505, b is 0.0021205837838351727\n",
            "582\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3504e-08], grad_fn=<SubBackward0>)\n",
            "iteration 582, loss is: 0.20593196153640747, w is: 0.8847253322601318, b is 0.0020993780344724655\n",
            "583\n",
            "tensor([[-2.7940e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 583, loss is: 0.20593105256557465, w is: 0.8847894072532654, b is 0.002078384393826127\n",
            "584\n",
            "tensor([[-4.7497e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 584, loss is: 0.2059302031993866, w is: 0.8848528265953064, b is 0.0020576005335897207\n",
            "585\n",
            "tensor([[-4.9360e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1176e-08], grad_fn=<SubBackward0>)\n",
            "iteration 585, loss is: 0.20592929422855377, w is: 0.8849155902862549, b is 0.0020370245911180973\n",
            "586\n",
            "tensor([[2.2352e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.9162e-09], grad_fn=<SubBackward0>)\n",
            "iteration 586, loss is: 0.2059284746646881, w is: 0.8849777579307556, b is 0.0020166542381048203\n",
            "587\n",
            "tensor([[3.5390e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 587, loss is: 0.20592761039733887, w is: 0.8850392699241638, b is 0.0019964876119047403\n",
            "588\n",
            "tensor([[-2.3283e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.3283e-09], grad_fn=<SubBackward0>)\n",
            "iteration 588, loss is: 0.2059268206357956, w is: 0.8851001858711243, b is 0.0019765226170420647\n",
            "589\n",
            "tensor([[-5.6811e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 589, loss is: 0.20592601597309113, w is: 0.885160505771637, b is 0.001956757390871644\n",
            "590\n",
            "tensor([[1.2759e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 590, loss is: 0.20592525601387024, w is: 0.8852202296257019, b is 0.0019371898379176855\n",
            "591\n",
            "tensor([[-4.0978e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.5193e-09], grad_fn=<SubBackward0>)\n",
            "iteration 591, loss is: 0.20592443645000458, w is: 0.8852793574333191, b is 0.001917817979119718\n",
            "592\n",
            "tensor([[6.1467e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1642e-09], grad_fn=<SubBackward0>)\n",
            "iteration 592, loss is: 0.20592370629310608, w is: 0.8853378891944885, b is 0.0018986397190019488\n",
            "593\n",
            "tensor([[2.0489e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.9849e-10], grad_fn=<SubBackward0>)\n",
            "iteration 593, loss is: 0.2059229463338852, w is: 0.8853958249092102, b is 0.0018796533113345504\n",
            "594\n",
            "tensor([[-1.3039e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.6298e-09], grad_fn=<SubBackward0>)\n",
            "iteration 594, loss is: 0.20592224597930908, w is: 0.8854531645774841, b is 0.0018608567770570517\n",
            "595\n",
            "tensor([[1.0058e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 595, loss is: 0.20592151582241058, w is: 0.8855099081993103, b is 0.0018422482535243034\n",
            "596\n",
            "tensor([[-3.1665e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 596, loss is: 0.20592081546783447, w is: 0.8855661153793335, b is 0.0018238256452605128\n",
            "597\n",
            "tensor([[1.5087e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 597, loss is: 0.20592015981674194, w is: 0.8856217861175537, b is 0.001805587438866496\n",
            "598\n",
            "tensor([[-3.1665e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 598, loss is: 0.20591948926448822, w is: 0.8856768608093262, b is 0.0017875316552817822\n",
            "599\n",
            "tensor([[7.4506e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.6298e-09], grad_fn=<SubBackward0>)\n",
            "iteration 599, loss is: 0.2059188336133957, w is: 0.8857313990592957, b is 0.0017696563154459\n",
            "600\n",
            "tensor([[7.2643e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.3283e-10], grad_fn=<SubBackward0>)\n",
            "iteration 600, loss is: 0.20591817796230316, w is: 0.8857854008674622, b is 0.001751959789544344\n",
            "601\n",
            "tensor([[4.6566e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([1.0943e-08], grad_fn=<SubBackward0>)\n",
            "iteration 601, loss is: 0.20591752231121063, w is: 0.8858388662338257, b is 0.001734440098516643\n",
            "602\n",
            "tensor([[-3.4459e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.3283e-10], grad_fn=<SubBackward0>)\n",
            "iteration 602, loss is: 0.20591692626476288, w is: 0.8858917951583862, b is 0.0017170956125482917\n",
            "603\n",
            "tensor([[-6.9849e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.7789e-09], grad_fn=<SubBackward0>)\n",
            "iteration 603, loss is: 0.20591631531715393, w is: 0.8859441876411438, b is 0.0016999247018247843\n",
            "604\n",
            "tensor([[4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.2177e-09], grad_fn=<SubBackward0>)\n",
            "iteration 604, loss is: 0.20591574907302856, w is: 0.8859960436820984, b is 0.0016829255037009716\n",
            "605\n",
            "tensor([[1.6764e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.0268e-09], grad_fn=<SubBackward0>)\n",
            "iteration 605, loss is: 0.2059151530265808, w is: 0.88604736328125, b is 0.0016660962719470263\n",
            "606\n",
            "tensor([[4.2841e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 606, loss is: 0.20591460168361664, w is: 0.8860982060432434, b is 0.0016494353767484426\n",
            "607\n",
            "tensor([[-5.4948e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.6298e-09], grad_fn=<SubBackward0>)\n",
            "iteration 607, loss is: 0.20591402053833008, w is: 0.8861485123634338, b is 0.0016329410718753934\n",
            "608\n",
            "tensor([[2.0396e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.9162e-09], grad_fn=<SubBackward0>)\n",
            "iteration 608, loss is: 0.2059134691953659, w is: 0.8861983418464661, b is 0.001616611727513373\n",
            "609\n",
            "tensor([[1.3039e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.3283e-10], grad_fn=<SubBackward0>)\n",
            "iteration 609, loss is: 0.20591291785240173, w is: 0.8862476348876953, b is 0.0016004457138478756\n",
            "610\n",
            "tensor([[2.3283e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.3551e-09], grad_fn=<SubBackward0>)\n",
            "iteration 610, loss is: 0.20591239631175995, w is: 0.8862964510917664, b is 0.0015844411682337523\n",
            "611\n",
            "tensor([[-6.9849e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 611, loss is: 0.20591190457344055, w is: 0.8863447904586792, b is 0.001568596693687141\n",
            "612\n",
            "tensor([[2.1420e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0943e-08], grad_fn=<SubBackward0>)\n",
            "iteration 612, loss is: 0.20591139793395996, w is: 0.8863926529884338, b is 0.001552910776808858\n",
            "613\n",
            "tensor([[5.8673e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.1223e-09], grad_fn=<SubBackward0>)\n",
            "iteration 613, loss is: 0.20591090619564056, w is: 0.8864400386810303, b is 0.0015373816713690758\n",
            "614\n",
            "tensor([[2.9802e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.3551e-09], grad_fn=<SubBackward0>)\n",
            "iteration 614, loss is: 0.20591039955615997, w is: 0.8864869475364685, b is 0.0015220079803839326\n",
            "615\n",
            "tensor([[-1.7695e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 615, loss is: 0.20590993762016296, w is: 0.8865333795547485, b is 0.0015067878412082791\n",
            "616\n",
            "tensor([[2.7008e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.9849e-10], grad_fn=<SubBackward0>)\n",
            "iteration 616, loss is: 0.20590949058532715, w is: 0.8865793347358704, b is 0.0014917199732735753\n",
            "617\n",
            "tensor([[-4.0978e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.4925e-09], grad_fn=<SubBackward0>)\n",
            "iteration 617, loss is: 0.20590901374816895, w is: 0.8866248726844788, b is 0.0014768027467653155\n",
            "618\n",
            "tensor([[-1.5832e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.4238e-09], grad_fn=<SubBackward0>)\n",
            "iteration 618, loss is: 0.20590858161449432, w is: 0.886669933795929, b is 0.0014620347646996379\n",
            "619\n",
            "tensor([[3.6322e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.3283e-10], grad_fn=<SubBackward0>)\n",
            "iteration 619, loss is: 0.2059081494808197, w is: 0.886714518070221, b is 0.0014474145136773586\n",
            "620\n",
            "tensor([[-9.3132e-10]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 620, loss is: 0.2059076875448227, w is: 0.8867586851119995, b is 0.0014329402474686503\n",
            "621\n",
            "tensor([[-4.4703e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 621, loss is: 0.20590728521347046, w is: 0.8868023753166199, b is 0.0014186109183356166\n",
            "622\n",
            "tensor([[1.8626e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1642e-09], grad_fn=<SubBackward0>)\n",
            "iteration 622, loss is: 0.20590685307979584, w is: 0.8868456482887268, b is 0.0014044248964637518\n",
            "623\n",
            "tensor([[5.9605e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.5611e-09], grad_fn=<SubBackward0>)\n",
            "iteration 623, loss is: 0.2059064358472824, w is: 0.8868885040283203, b is 0.0013903806684538722\n",
            "624\n",
            "tensor([[-7.5437e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.0955e-09], grad_fn=<SubBackward0>)\n",
            "iteration 624, loss is: 0.20590606331825256, w is: 0.8869309425354004, b is 0.001376476837322116\n",
            "625\n",
            "tensor([[1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 625, loss is: 0.20590569078922272, w is: 0.886972963809967, b is 0.0013627121224999428\n",
            "626\n",
            "tensor([[3.8184e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 626, loss is: 0.20590533316135406, w is: 0.8870145678520203, b is 0.0013490848941728473\n",
            "627\n",
            "tensor([[-5.6811e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.0536e-09], grad_fn=<SubBackward0>)\n",
            "iteration 627, loss is: 0.20590491592884064, w is: 0.8870557546615601, b is 0.001335594104602933\n",
            "628\n",
            "tensor([[7.0781e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 628, loss is: 0.20590455830097198, w is: 0.8870965242385864, b is 0.0013222380075603724\n",
            "629\n",
            "tensor([[8.3819e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 629, loss is: 0.20590421557426453, w is: 0.8871368765830994, b is 0.001309015671722591\n",
            "630\n",
            "tensor([[6.4261e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.4925e-09], grad_fn=<SubBackward0>)\n",
            "iteration 630, loss is: 0.2059038281440735, w is: 0.8871768116950989, b is 0.001295925467275083\n",
            "631\n",
            "tensor([[1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 631, loss is: 0.20590351521968842, w is: 0.887216329574585, b is 0.0012829661136493087\n",
            "632\n",
            "tensor([[-7.2177e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.9849e-10], grad_fn=<SubBackward0>)\n",
            "iteration 632, loss is: 0.20590317249298096, w is: 0.8872554898262024, b is 0.0012701364466920495\n",
            "633\n",
            "tensor([[-1.1176e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.8894e-09], grad_fn=<SubBackward0>)\n",
            "iteration 633, loss is: 0.2059028297662735, w is: 0.8872942328453064, b is 0.0012574350694194436\n",
            "634\n",
            "tensor([[-5.4482e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.8894e-09], grad_fn=<SubBackward0>)\n",
            "iteration 634, loss is: 0.20590250194072723, w is: 0.8873326182365417, b is 0.0012448608176782727\n",
            "635\n",
            "tensor([[6.9384e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.7940e-09], grad_fn=<SubBackward0>)\n",
            "iteration 635, loss is: 0.20590218901634216, w is: 0.8873705863952637, b is 0.001232412178069353\n",
            "636\n",
            "tensor([[1.2107e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 636, loss is: 0.2059018760919571, w is: 0.8874081969261169, b is 0.0012200881028547883\n",
            "637\n",
            "tensor([[6.5193e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([9.5461e-09], grad_fn=<SubBackward0>)\n",
            "iteration 637, loss is: 0.2059015929698944, w is: 0.8874453902244568, b is 0.0012078871950507164\n",
            "638\n",
            "tensor([[-8.3819e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 638, loss is: 0.20590128004550934, w is: 0.887482225894928, b is 0.001195808406919241\n",
            "639\n",
            "tensor([[-6.0536e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.9162e-09], grad_fn=<SubBackward0>)\n",
            "iteration 639, loss is: 0.20590096712112427, w is: 0.8875187039375305, b is 0.0011838503414765\n",
            "640\n",
            "tensor([[-1.3364e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.2177e-09], grad_fn=<SubBackward0>)\n",
            "iteration 640, loss is: 0.20590068399906158, w is: 0.8875548243522644, b is 0.0011720118345692754\n",
            "641\n",
            "tensor([[8.3819e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 641, loss is: 0.2059004157781601, w is: 0.8875905871391296, b is 0.0011602916056290269\n",
            "642\n",
            "tensor([[-1.4435e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 642, loss is: 0.20590011775493622, w is: 0.8876259922981262, b is 0.00114868872333318\n",
            "643\n",
            "tensor([[2.8871e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.4506e-09], grad_fn=<SubBackward0>)\n",
            "iteration 643, loss is: 0.20589986443519592, w is: 0.8876610398292542, b is 0.0011372017906978726\n",
            "644\n",
            "tensor([[-2.7474e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([8.1491e-09], grad_fn=<SubBackward0>)\n",
            "iteration 644, loss is: 0.20589959621429443, w is: 0.8876957297325134, b is 0.0011258297599852085\n",
            "645\n",
            "tensor([[-1.0710e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.9581e-09], grad_fn=<SubBackward0>)\n",
            "iteration 645, loss is: 0.20589932799339294, w is: 0.887730062007904, b is 0.0011145714670419693\n",
            "646\n",
            "tensor([[-3.8184e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0012e-08], grad_fn=<SubBackward0>)\n",
            "iteration 646, loss is: 0.20589910447597504, w is: 0.887764036655426, b is 0.0011034257477149367\n",
            "647\n",
            "tensor([[2.8405e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.1910e-09], grad_fn=<SubBackward0>)\n",
            "iteration 647, loss is: 0.20589880645275116, w is: 0.8877977132797241, b is 0.0010923916706815362\n",
            "648\n",
            "tensor([[-9.3598e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.6298e-09], grad_fn=<SubBackward0>)\n",
            "iteration 648, loss is: 0.20589856803417206, w is: 0.8878310322761536, b is 0.001081467722542584\n",
            "649\n",
            "tensor([[-6.8918e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.4238e-09], grad_fn=<SubBackward0>)\n",
            "iteration 649, loss is: 0.20589834451675415, w is: 0.8878640532493591, b is 0.0010706530883908272\n",
            "650\n",
            "tensor([[2.8405e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 650, loss is: 0.20589812099933624, w is: 0.887896716594696, b is 0.0010599464876577258\n",
            "651\n",
            "tensor([[-4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 651, loss is: 0.20589789748191833, w is: 0.8879290819168091, b is 0.001049347105436027\n",
            "652\n",
            "tensor([[6.6124e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.4925e-09], grad_fn=<SubBackward0>)\n",
            "iteration 652, loss is: 0.20589764416217804, w is: 0.8879610896110535, b is 0.0010388536611571908\n",
            "653\n",
            "tensor([[1.1455e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-09], grad_fn=<SubBackward0>)\n",
            "iteration 653, loss is: 0.20589742064476013, w is: 0.887992799282074, b is 0.0010284651070833206\n",
            "654\n",
            "tensor([[-2.5146e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 654, loss is: 0.20589721202850342, w is: 0.8880241513252258, b is 0.00101818039547652\n",
            "655\n",
            "tensor([[-6.4261e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.8894e-09], grad_fn=<SubBackward0>)\n",
            "iteration 655, loss is: 0.2058970034122467, w is: 0.8880552053451538, b is 0.0010079985950142145\n",
            "656\n",
            "tensor([[1.3039e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.7521e-09], grad_fn=<SubBackward0>)\n",
            "iteration 656, loss is: 0.20589679479599, w is: 0.8880859613418579, b is 0.0009979185415431857\n",
            "657\n",
            "tensor([[2.7940e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 657, loss is: 0.20589660108089447, w is: 0.8881164193153381, b is 0.0009879394201561809\n",
            "658\n",
            "tensor([[-3.0734e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 658, loss is: 0.20589639246463776, w is: 0.8881465792655945, b is 0.0009780600666999817\n",
            "659\n",
            "tensor([[-6.9849e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([6.9849e-10], grad_fn=<SubBackward0>)\n",
            "iteration 659, loss is: 0.20589618384838104, w is: 0.888176441192627, b is 0.0009682794334366918\n",
            "660\n",
            "tensor([[-5.5414e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.9640e-09], grad_fn=<SubBackward0>)\n",
            "iteration 660, loss is: 0.20589600503444672, w is: 0.8882060050964355, b is 0.0009585965890437365\n",
            "661\n",
            "tensor([[-8.8476e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 661, loss is: 0.2058958262205124, w is: 0.8882352709770203, b is 0.0009490106021985412\n",
            "662\n",
            "tensor([[8.7079e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.3760e-09], grad_fn=<SubBackward0>)\n",
            "iteration 662, loss is: 0.20589564740657806, w is: 0.8882642388343811, b is 0.0009395204833708704\n",
            "663\n",
            "tensor([[4.6566e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.8208e-10], grad_fn=<SubBackward0>)\n",
            "iteration 663, loss is: 0.20589545369148254, w is: 0.8882929086685181, b is 0.0009301252430304885\n",
            "664\n",
            "tensor([[1.1642e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.0245e-08], grad_fn=<SubBackward0>)\n",
            "iteration 664, loss is: 0.20589527487754822, w is: 0.8883212804794312, b is 0.0009208240662701428\n",
            "665\n",
            "tensor([[7.5903e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.0268e-09], grad_fn=<SubBackward0>)\n",
            "iteration 665, loss is: 0.2058950960636139, w is: 0.8883493542671204, b is 0.0009116158471442759\n",
            "666\n",
            "tensor([[2.4680e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.7521e-09], grad_fn=<SubBackward0>)\n",
            "iteration 666, loss is: 0.20589491724967957, w is: 0.8883771896362305, b is 0.0009024997125379741\n",
            "667\n",
            "tensor([[-9.5461e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.8208e-10], grad_fn=<SubBackward0>)\n",
            "iteration 667, loss is: 0.20589475333690643, w is: 0.8884047269821167, b is 0.0008934747893363237\n",
            "668\n",
            "tensor([[2.3283e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.6357e-09], grad_fn=<SubBackward0>)\n",
            "iteration 668, loss is: 0.2058945894241333, w is: 0.888431966304779, b is 0.0008845399715937674\n",
            "669\n",
            "tensor([[1.2573e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.5611e-09], grad_fn=<SubBackward0>)\n",
            "iteration 669, loss is: 0.20589441061019897, w is: 0.8884589672088623, b is 0.0008756946190260351\n",
            "670\n",
            "tensor([[9.6392e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.9849e-10], grad_fn=<SubBackward0>)\n",
            "iteration 670, loss is: 0.20589430630207062, w is: 0.8884856700897217, b is 0.0008669376256875694\n",
            "671\n",
            "tensor([[-7.7765e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.9791e-09], grad_fn=<SubBackward0>)\n",
            "iteration 671, loss is: 0.20589414238929749, w is: 0.888512134552002, b is 0.0008582682930864394\n",
            "672\n",
            "tensor([[7.9162e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.1432e-09], grad_fn=<SubBackward0>)\n",
            "iteration 672, loss is: 0.20589396357536316, w is: 0.8885383009910583, b is 0.0008496855734847486\n",
            "673\n",
            "tensor([[-1.3970e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.7044e-09], grad_fn=<SubBackward0>)\n",
            "iteration 673, loss is: 0.2058938443660736, w is: 0.8885642290115356, b is 0.0008411887101829052\n",
            "674\n",
            "tensor([[-3.3993e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.6089e-09], grad_fn=<SubBackward0>)\n",
            "iteration 674, loss is: 0.20589366555213928, w is: 0.8885899186134338, b is 0.0008327768882736564\n",
            "675\n",
            "tensor([[1.0291e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([5.7044e-09], grad_fn=<SubBackward0>)\n",
            "iteration 675, loss is: 0.20589354634284973, w is: 0.8886153101921082, b is 0.0008244490600191057\n",
            "676\n",
            "tensor([[5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1642e-09], grad_fn=<SubBackward0>)\n",
            "iteration 676, loss is: 0.2058933973312378, w is: 0.8886404633522034, b is 0.0008162045269273221\n",
            "677\n",
            "tensor([[7.0781e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.5611e-09], grad_fn=<SubBackward0>)\n",
            "iteration 677, loss is: 0.20589324831962585, w is: 0.8886653780937195, b is 0.0008080424158833921\n",
            "678\n",
            "tensor([[-6.5193e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([3.3760e-09], grad_fn=<SubBackward0>)\n",
            "iteration 678, loss is: 0.2058931440114975, w is: 0.8886900544166565, b is 0.0007999620283953846\n",
            "679\n",
            "tensor([[6.8452e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.5461e-09], grad_fn=<SubBackward0>)\n",
            "iteration 679, loss is: 0.20589300990104675, w is: 0.8887144327163696, b is 0.000791962374933064\n",
            "680\n",
            "tensor([[4.6566e-10]], grad_fn=<SubBackward0>)\n",
            "tensor([8.0327e-09], grad_fn=<SubBackward0>)\n",
            "iteration 680, loss is: 0.20589286088943481, w is: 0.8887385725975037, b is 0.0007840427570044994\n",
            "681\n",
            "tensor([[-2.3749e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([8.1491e-10], grad_fn=<SubBackward0>)\n",
            "iteration 681, loss is: 0.20589272677898407, w is: 0.8887624740600586, b is 0.0007762023014947772\n",
            "682\n",
            "tensor([[-3.4925e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.8208e-09], grad_fn=<SubBackward0>)\n",
            "iteration 682, loss is: 0.2058926224708557, w is: 0.8887861371040344, b is 0.0007684401934966445\n",
            "683\n",
            "tensor([[-5.8673e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.6089e-09], grad_fn=<SubBackward0>)\n",
            "iteration 683, loss is: 0.20589250326156616, w is: 0.8888095617294312, b is 0.0007607558509334922\n",
            "684\n",
            "tensor([[-4.9360e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.3283e-09], grad_fn=<SubBackward0>)\n",
            "iteration 684, loss is: 0.2058923840522766, w is: 0.8888327479362488, b is 0.0007531482842750847\n",
            "685\n",
            "tensor([[2.5611e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 685, loss is: 0.20589227974414825, w is: 0.8888556957244873, b is 0.0007456167950294912\n",
            "686\n",
            "tensor([[9.1735e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.8894e-09], grad_fn=<SubBackward0>)\n",
            "iteration 686, loss is: 0.2058921605348587, w is: 0.8888784646987915, b is 0.0007381606264971197\n",
            "687\n",
            "tensor([[4.3306e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.5134e-09], grad_fn=<SubBackward0>)\n",
            "iteration 687, loss is: 0.20589204132556915, w is: 0.8889009952545166, b is 0.0007307790801860392\n",
            "688\n",
            "tensor([[-2.7940e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.2806e-09], grad_fn=<SubBackward0>)\n",
            "iteration 688, loss is: 0.2058919370174408, w is: 0.8889232873916626, b is 0.0007234713993966579\n",
            "689\n",
            "tensor([[1.1595e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.8417e-09], grad_fn=<SubBackward0>)\n",
            "iteration 689, loss is: 0.20589181780815125, w is: 0.8889453411102295, b is 0.0007162367110140622\n",
            "690\n",
            "tensor([[2.5146e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1758e-08], grad_fn=<SubBackward0>)\n",
            "iteration 690, loss is: 0.2058917135000229, w is: 0.8889672160148621, b is 0.0007090744329616427\n",
            "691\n",
            "tensor([[2.2817e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.9849e-10], grad_fn=<SubBackward0>)\n",
            "iteration 691, loss is: 0.20589160919189453, w is: 0.8889888525009155, b is 0.000701983692124486\n",
            "692\n",
            "tensor([[4.6566e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([3.4925e-09], grad_fn=<SubBackward0>)\n",
            "iteration 692, loss is: 0.20589150488376617, w is: 0.8890102505683899, b is 0.0006949638482183218\n",
            "693\n",
            "tensor([[2.6077e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.9581e-09], grad_fn=<SubBackward0>)\n",
            "iteration 693, loss is: 0.205891415476799, w is: 0.8890314698219299, b is 0.0006880140863358974\n",
            "694\n",
            "tensor([[-1.3225e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.6298e-09], grad_fn=<SubBackward0>)\n",
            "iteration 694, loss is: 0.20589132606983185, w is: 0.8890524506568909, b is 0.0006811339990235865\n",
            "695\n",
            "tensor([[-2.7474e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.2806e-09], grad_fn=<SubBackward0>)\n",
            "iteration 695, loss is: 0.20589123666286469, w is: 0.8890732526779175, b is 0.0006743225385434926\n",
            "696\n",
            "tensor([[7.0315e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1642e-09], grad_fn=<SubBackward0>)\n",
            "iteration 696, loss is: 0.20589113235473633, w is: 0.889093816280365, b is 0.0006675792974419892\n",
            "697\n",
            "tensor([[-4.9360e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1642e-09], grad_fn=<SubBackward0>)\n",
            "iteration 697, loss is: 0.20589102804660797, w is: 0.8891142010688782, b is 0.0006609035772271454\n",
            "698\n",
            "tensor([[-1.8626e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9372e-09], grad_fn=<SubBackward0>)\n",
            "iteration 698, loss is: 0.2058909386396408, w is: 0.8891343474388123, b is 0.0006542946211993694\n",
            "699\n",
            "tensor([[-5.8673e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.9372e-09], grad_fn=<SubBackward0>)\n",
            "iteration 699, loss is: 0.20589086413383484, w is: 0.889154314994812, b is 0.0006477516726590693\n",
            "700\n",
            "tensor([[5.6345e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 700, loss is: 0.20589077472686768, w is: 0.8891741037368774, b is 0.0006412741495296359\n",
            "701\n",
            "tensor([[-1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.3283e-10], grad_fn=<SubBackward0>)\n",
            "iteration 701, loss is: 0.2058906853199005, w is: 0.8891936540603638, b is 0.0006348614115267992\n",
            "702\n",
            "tensor([[4.6566e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([3.3760e-09], grad_fn=<SubBackward0>)\n",
            "iteration 702, loss is: 0.20589061081409454, w is: 0.8892130255699158, b is 0.0006285127601586282\n",
            "703\n",
            "tensor([[1.6531e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([6.2864e-09], grad_fn=<SubBackward0>)\n",
            "iteration 703, loss is: 0.20589052140712738, w is: 0.8892322182655334, b is 0.0006222277297638357\n",
            "704\n",
            "tensor([[4.4471e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.0955e-09], grad_fn=<SubBackward0>)\n",
            "iteration 704, loss is: 0.20589043200016022, w is: 0.889251172542572, b is 0.0006160054472275078\n",
            "705\n",
            "tensor([[7.2876e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.5611e-09], grad_fn=<SubBackward0>)\n",
            "iteration 705, loss is: 0.20589037239551544, w is: 0.8892699480056763, b is 0.0006098454468883574\n",
            "706\n",
            "tensor([[2.9104e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.6834e-09], grad_fn=<SubBackward0>)\n",
            "iteration 706, loss is: 0.20589029788970947, w is: 0.8892885446548462, b is 0.0006037468556314707\n",
            "707\n",
            "tensor([[8.5216e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.3551e-09], grad_fn=<SubBackward0>)\n",
            "iteration 707, loss is: 0.2058902233839035, w is: 0.8893069624900818, b is 0.0005977093824185431\n",
            "708\n",
            "tensor([[5.4482e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.3283e-10], grad_fn=<SubBackward0>)\n",
            "iteration 708, loss is: 0.20589013397693634, w is: 0.8893252015113831, b is 0.0005917322705499828\n",
            "709\n",
            "tensor([[-4.0745e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.1223e-09], grad_fn=<SubBackward0>)\n",
            "iteration 709, loss is: 0.20589007437229156, w is: 0.88934326171875, b is 0.0005858148797415197\n",
            "710\n",
            "tensor([[9.3831e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.9372e-09], grad_fn=<SubBackward0>)\n",
            "iteration 710, loss is: 0.2058899998664856, w is: 0.8893611431121826, b is 0.0005799568025395274\n",
            "711\n",
            "tensor([[-9.5461e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([1.9791e-09], grad_fn=<SubBackward0>)\n",
            "iteration 711, loss is: 0.20588994026184082, w is: 0.8893788456916809, b is 0.0005741572240367532\n",
            "712\n",
            "tensor([[1.9791e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 712, loss is: 0.20588986575603485, w is: 0.8893963694572449, b is 0.0005684156785719097\n",
            "713\n",
            "tensor([[5.9139e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.2596e-09], grad_fn=<SubBackward0>)\n",
            "iteration 713, loss is: 0.2058897763490677, w is: 0.8894137144088745, b is 0.0005627315840683877\n",
            "714\n",
            "tensor([[8.7311e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.4715e-09], grad_fn=<SubBackward0>)\n",
            "iteration 714, loss is: 0.2058897465467453, w is: 0.8894308805465698, b is 0.0005571042420342565\n",
            "715\n",
            "tensor([[3.0966e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0245e-08], grad_fn=<SubBackward0>)\n",
            "iteration 715, loss is: 0.20588968694210052, w is: 0.8894478678703308, b is 0.0005515332450158894\n",
            "716\n",
            "tensor([[-2.8638e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.3342e-09], grad_fn=<SubBackward0>)\n",
            "iteration 716, loss is: 0.20588959753513336, w is: 0.8894646763801575, b is 0.0005460179527290165\n",
            "717\n",
            "tensor([[-1.1036e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.0955e-09], grad_fn=<SubBackward0>)\n",
            "iteration 717, loss is: 0.20588955283164978, w is: 0.8894813060760498, b is 0.0005405577830970287\n",
            "718\n",
            "tensor([[2.8405e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.6089e-09], grad_fn=<SubBackward0>)\n",
            "iteration 718, loss is: 0.2058894783258438, w is: 0.8894978165626526, b is 0.0005351521540433168\n",
            "719\n",
            "tensor([[8.1491e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-4.5402e-09], grad_fn=<SubBackward0>)\n",
            "iteration 719, loss is: 0.20588943362236023, w is: 0.889514148235321, b is 0.0005298006581142545\n",
            "720\n",
            "tensor([[-2.9337e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.7462e-09], grad_fn=<SubBackward0>)\n",
            "iteration 720, loss is: 0.20588935911655426, w is: 0.8895303010940552, b is 0.0005245026550255716\n",
            "721\n",
            "tensor([[-6.2864e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([8.3819e-09], grad_fn=<SubBackward0>)\n",
            "iteration 721, loss is: 0.20588931441307068, w is: 0.889546275138855, b is 0.000519257562700659\n",
            "722\n",
            "tensor([[-6.0303e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.6357e-09], grad_fn=<SubBackward0>)\n",
            "iteration 722, loss is: 0.2058892548084259, w is: 0.8895621299743652, b is 0.0005140649154782295\n",
            "723\n",
            "tensor([[8.1258e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 723, loss is: 0.20588921010494232, w is: 0.8895778059959412, b is 0.0005089243059046566\n",
            "724\n",
            "tensor([[-7.7998e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.3132e-10], grad_fn=<SubBackward0>)\n",
            "iteration 724, loss is: 0.20588916540145874, w is: 0.8895933032035828, b is 0.0005038350936956704\n",
            "725\n",
            "tensor([[-3.6322e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.1642e-10], grad_fn=<SubBackward0>)\n",
            "iteration 725, loss is: 0.20588912069797516, w is: 0.8896086812019348, b is 0.0004987968131899834\n",
            "726\n",
            "tensor([[6.7754e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-8.6147e-09], grad_fn=<SubBackward0>)\n",
            "iteration 726, loss is: 0.2058890461921692, w is: 0.8896238803863525, b is 0.0004938089405186474\n",
            "727\n",
            "tensor([[-1.6810e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0477e-09], grad_fn=<SubBackward0>)\n",
            "iteration 727, loss is: 0.205889031291008, w is: 0.8896389603614807, b is 0.0004888709518127143\n",
            "728\n",
            "tensor([[-2.7008e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.2119e-09], grad_fn=<SubBackward0>)\n",
            "iteration 728, loss is: 0.20588892698287964, w is: 0.8896538615226746, b is 0.0004839822358917445\n",
            "729\n",
            "tensor([[1.9558e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.7416e-09], grad_fn=<SubBackward0>)\n",
            "iteration 729, loss is: 0.20588891208171844, w is: 0.8896686434745789, b is 0.0004791423853021115\n",
            "730\n",
            "tensor([[8.5449e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2119e-09], grad_fn=<SubBackward0>)\n",
            "iteration 730, loss is: 0.20588883757591248, w is: 0.8896832466125488, b is 0.00047435093438252807\n",
            "731\n",
            "tensor([[4.4005e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1642e-08], grad_fn=<SubBackward0>)\n",
            "iteration 731, loss is: 0.2058887928724289, w is: 0.8896977305412292, b is 0.00046960741747170687\n",
            "732\n",
            "tensor([[-6.7055e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.7044e-09], grad_fn=<SubBackward0>)\n",
            "iteration 732, loss is: 0.2058887481689453, w is: 0.8897120356559753, b is 0.0004649113689083606\n",
            "733\n",
            "tensor([[1.2806e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.9581e-09], grad_fn=<SubBackward0>)\n",
            "iteration 733, loss is: 0.20588873326778412, w is: 0.8897262215614319, b is 0.00046026220661588013\n",
            "734\n",
            "tensor([[-5.1223e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.2282e-09], grad_fn=<SubBackward0>)\n",
            "iteration 734, loss is: 0.20588868856430054, w is: 0.8897402882575989, b is 0.00045565961045213044\n",
            "735\n",
            "tensor([[1.1176e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.3074e-09], grad_fn=<SubBackward0>)\n",
            "iteration 735, loss is: 0.20588864386081696, w is: 0.8897541761398315, b is 0.00045110302744433284\n",
            "736\n",
            "tensor([[1.8626e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-09], grad_fn=<SubBackward0>)\n",
            "iteration 736, loss is: 0.20588859915733337, w is: 0.8897679448127747, b is 0.0004465920210350305\n",
            "737\n",
            "tensor([[4.5402e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.5670e-10], grad_fn=<SubBackward0>)\n",
            "iteration 737, loss is: 0.2058885395526886, w is: 0.8897815942764282, b is 0.0004421260964591056\n",
            "738\n",
            "tensor([[3.2596e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0477e-09], grad_fn=<SubBackward0>)\n",
            "iteration 738, loss is: 0.20588849484920502, w is: 0.8897950649261475, b is 0.00043770481715910137\n",
            "739\n",
            "tensor([[-5.8208e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.6461e-09], grad_fn=<SubBackward0>)\n",
            "iteration 739, loss is: 0.20588845014572144, w is: 0.8898084163665771, b is 0.00043332777568139136\n",
            "740\n",
            "tensor([[5.5414e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.7462e-10], grad_fn=<SubBackward0>)\n",
            "iteration 740, loss is: 0.20588843524456024, w is: 0.8898216485977173, b is 0.00042899444815702736\n",
            "741\n",
            "tensor([[-5.1921e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.8522e-09], grad_fn=<SubBackward0>)\n",
            "iteration 741, loss is: 0.20588839054107666, w is: 0.8898347616195679, b is 0.0004247044853400439\n",
            "742\n",
            "tensor([[-3.5157e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<SubBackward0>)\n",
            "iteration 742, loss is: 0.20588834583759308, w is: 0.8898476958274841, b is 0.0004204574506729841\n",
            "743\n",
            "tensor([[2.1188e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.1327e-09], grad_fn=<SubBackward0>)\n",
            "iteration 743, loss is: 0.2058883160352707, w is: 0.8898605108261108, b is 0.0004162528784945607\n",
            "744\n",
            "tensor([[4.0745e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.2224e-09], grad_fn=<SubBackward0>)\n",
            "iteration 744, loss is: 0.2058882713317871, w is: 0.889873206615448, b is 0.00041209018672816455\n",
            "745\n",
            "tensor([[-3.0268e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([8.4401e-09], grad_fn=<SubBackward0>)\n",
            "iteration 745, loss is: 0.20588825643062592, w is: 0.8898857831954956, b is 0.00040796928806230426\n",
            "746\n",
            "tensor([[9.4762e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([7.0431e-09], grad_fn=<SubBackward0>)\n",
            "iteration 746, loss is: 0.20588822662830353, w is: 0.8898982405662537, b is 0.00040388957131654024\n",
            "747\n",
            "tensor([[-1.4668e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.7626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 747, loss is: 0.20588818192481995, w is: 0.8899105787277222, b is 0.0003998507163487375\n",
            "748\n",
            "tensor([[-1.4016e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([4.0745e-10], grad_fn=<SubBackward0>)\n",
            "iteration 748, loss is: 0.20588813722133636, w is: 0.8899227976799011, b is 0.0003958523157052696\n",
            "749\n",
            "tensor([[1.3271e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.2806e-09], grad_fn=<SubBackward0>)\n",
            "iteration 749, loss is: 0.20588810741901398, w is: 0.8899348974227905, b is 0.0003918937873095274\n",
            "750\n",
            "tensor([[-5.4250e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.5507e-09], grad_fn=<SubBackward0>)\n",
            "iteration 750, loss is: 0.20588809251785278, w is: 0.8899468779563904, b is 0.0003879748401232064\n",
            "751\n",
            "tensor([[-6.5425e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.5716e-09], grad_fn=<SubBackward0>)\n",
            "iteration 751, loss is: 0.2058880478143692, w is: 0.8899587392807007, b is 0.0003840950957965106\n",
            "752\n",
            "tensor([[-5.7509e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.8953e-10], grad_fn=<SubBackward0>)\n",
            "iteration 752, loss is: 0.20588801801204681, w is: 0.8899704813957214, b is 0.0003802541468758136\n",
            "753\n",
            "tensor([[-6.8452e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.7148e-09], grad_fn=<SubBackward0>)\n",
            "iteration 753, loss is: 0.20588800311088562, w is: 0.8899821043014526, b is 0.00037645152769982815\n",
            "754\n",
            "tensor([[8.6147e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([5.8208e-10], grad_fn=<SubBackward0>)\n",
            "iteration 754, loss is: 0.20588795840740204, w is: 0.8899936079978943, b is 0.0003726869763340801\n",
            "755\n",
            "tensor([[-3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.0477e-09], grad_fn=<SubBackward0>)\n",
            "iteration 755, loss is: 0.20588792860507965, w is: 0.8900049924850464, b is 0.0003689601144287735\n",
            "756\n",
            "tensor([[-6.9849e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.0641e-09], grad_fn=<SubBackward0>)\n",
            "iteration 756, loss is: 0.20588791370391846, w is: 0.8900162577629089, b is 0.00036527050542645156\n",
            "757\n",
            "tensor([[-2.3283e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.1223e-09], grad_fn=<SubBackward0>)\n",
            "iteration 757, loss is: 0.20588786900043488, w is: 0.8900274038314819, b is 0.00036161791649647057\n",
            "758\n",
            "tensor([[-8.1491e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.1805e-09], grad_fn=<SubBackward0>)\n",
            "iteration 758, loss is: 0.20588786900043488, w is: 0.8900384306907654, b is 0.00035800179466605186\n",
            "759\n",
            "tensor([[7.2876e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([8.1491e-10], grad_fn=<SubBackward0>)\n",
            "iteration 759, loss is: 0.2058878242969513, w is: 0.8900493383407593, b is 0.00035442179068922997\n",
            "760\n",
            "tensor([[-3.1665e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.6566e-10], grad_fn=<SubBackward0>)\n",
            "iteration 760, loss is: 0.2058878242969513, w is: 0.8900601267814636, b is 0.00035087758442386985\n",
            "761\n",
            "tensor([[-6.5658e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.4552e-09], grad_fn=<SubBackward0>)\n",
            "iteration 761, loss is: 0.2058877795934677, w is: 0.8900708556175232, b is 0.00034736882662400603\n",
            "762\n",
            "tensor([[1.8859e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.0641e-09], grad_fn=<SubBackward0>)\n",
            "iteration 762, loss is: 0.2058877795934677, w is: 0.8900814652442932, b is 0.0003438951098360121\n",
            "763\n",
            "tensor([[-2.0722e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.7521e-09], grad_fn=<SubBackward0>)\n",
            "iteration 763, loss is: 0.20588773488998413, w is: 0.8900919556617737, b is 0.0003404561139177531\n",
            "764\n",
            "tensor([[3.2829e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2119e-09], grad_fn=<SubBackward0>)\n",
            "iteration 764, loss is: 0.20588773488998413, w is: 0.8901023268699646, b is 0.00033705151872709394\n",
            "765\n",
            "tensor([[-8.2189e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([9.3132e-09], grad_fn=<SubBackward0>)\n",
            "iteration 765, loss is: 0.20588767528533936, w is: 0.890112578868866, b is 0.00033368097501806915\n",
            "766\n",
            "tensor([[4.8429e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.2806e-09], grad_fn=<SubBackward0>)\n",
            "iteration 766, loss is: 0.20588766038417816, w is: 0.8901227712631226, b is 0.00033034419175237417\n",
            "767\n",
            "tensor([[-1.1805e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.2014e-09], grad_fn=<SubBackward0>)\n",
            "iteration 767, loss is: 0.20588763058185577, w is: 0.8901328444480896, b is 0.00032704073237255216\n",
            "768\n",
            "tensor([[-3.5856e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1933e-08], grad_fn=<SubBackward0>)\n",
            "iteration 768, loss is: 0.20588761568069458, w is: 0.8901427984237671, b is 0.00032377030584029853\n",
            "769\n",
            "tensor([[-3.0035e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.4343e-09], grad_fn=<SubBackward0>)\n",
            "iteration 769, loss is: 0.2058875858783722, w is: 0.8901526927947998, b is 0.00032053262111730874\n",
            "770\n",
            "tensor([[5.8790e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.6776e-09], grad_fn=<SubBackward0>)\n",
            "iteration 770, loss is: 0.205887570977211, w is: 0.890162467956543, b is 0.0003173272998537868\n",
            "771\n",
            "tensor([[3.6089e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.0373e-09], grad_fn=<SubBackward0>)\n",
            "iteration 771, loss is: 0.2058875411748886, w is: 0.8901721239089966, b is 0.0003141539928037673\n",
            "772\n",
            "tensor([[-3.0617e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.9104e-09], grad_fn=<SubBackward0>)\n",
            "iteration 772, loss is: 0.2058875411748886, w is: 0.8901817202568054, b is 0.00031101240892894566\n",
            "773\n",
            "tensor([[3.2131e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([4.4820e-09], grad_fn=<SubBackward0>)\n",
            "iteration 773, loss is: 0.20588749647140503, w is: 0.8901911973953247, b is 0.0003079023153986782\n",
            "774\n",
            "tensor([[-3.6089e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.5134e-09], grad_fn=<SubBackward0>)\n",
            "iteration 774, loss is: 0.20588749647140503, w is: 0.8902006149291992, b is 0.0003048233047593385\n",
            "775\n",
            "tensor([[-9.8953e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.8208e-09], grad_fn=<SubBackward0>)\n",
            "iteration 775, loss is: 0.20588748157024384, w is: 0.8902099132537842, b is 0.00030177508597262204\n",
            "776\n",
            "tensor([[-7.5670e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.5134e-09], grad_fn=<SubBackward0>)\n",
            "iteration 776, loss is: 0.20588748157024384, w is: 0.8902190923690796, b is 0.00029875742620788515\n",
            "777\n",
            "tensor([[-9.3598e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.6461e-09], grad_fn=<SubBackward0>)\n",
            "iteration 777, loss is: 0.20588743686676025, w is: 0.8902282118797302, b is 0.0002957698598038405\n",
            "778\n",
            "tensor([[7.2177e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([7.5670e-10], grad_fn=<SubBackward0>)\n",
            "iteration 778, loss is: 0.20588740706443787, w is: 0.8902372121810913, b is 0.0002928121539298445\n",
            "779\n",
            "tensor([[1.7695e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([2.9104e-09], grad_fn=<SubBackward0>)\n",
            "iteration 779, loss is: 0.20588740706443787, w is: 0.8902461528778076, b is 0.000289884046651423\n",
            "780\n",
            "tensor([[2.4098e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.9791e-09], grad_fn=<SubBackward0>)\n",
            "iteration 780, loss is: 0.20588739216327667, w is: 0.8902549743652344, b is 0.00028698515961878\n",
            "781\n",
            "tensor([[-3.0966e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.7462e-10], grad_fn=<SubBackward0>)\n",
            "iteration 781, loss is: 0.20588736236095428, w is: 0.8902637362480164, b is 0.0002841152891051024\n",
            "782\n",
            "tensor([[-1.4319e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-9.8953e-10], grad_fn=<SubBackward0>)\n",
            "iteration 782, loss is: 0.2058873474597931, w is: 0.8902723789215088, b is 0.00028127411496825516\n",
            "783\n",
            "tensor([[5.7044e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.1642e-08], grad_fn=<SubBackward0>)\n",
            "iteration 783, loss is: 0.2058873176574707, w is: 0.8902809619903564, b is 0.00027846143348142505\n",
            "784\n",
            "tensor([[-5.2154e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.0361e-08], grad_fn=<SubBackward0>)\n",
            "iteration 784, loss is: 0.2058873176574707, w is: 0.8902894854545593, b is 0.0002756767498794943\n",
            "785\n",
            "tensor([[-4.4820e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.3551e-09], grad_fn=<SubBackward0>)\n",
            "iteration 785, loss is: 0.2058873176574707, w is: 0.8902978897094727, b is 0.00027292000595480204\n",
            "786\n",
            "tensor([[3.6904e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-6.4028e-10], grad_fn=<SubBackward0>)\n",
            "iteration 786, loss is: 0.20588727295398712, w is: 0.8903062343597412, b is 0.0002701908233575523\n",
            "787\n",
            "tensor([[1.9209e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.7462e-10], grad_fn=<SubBackward0>)\n",
            "iteration 787, loss is: 0.20588727295398712, w is: 0.8903144598007202, b is 0.00026748888194561005\n",
            "788\n",
            "tensor([[-2.8871e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.9581e-09], grad_fn=<SubBackward0>)\n",
            "iteration 788, loss is: 0.20588727295398712, w is: 0.8903226256370544, b is 0.00026481394888833165\n",
            "789\n",
            "tensor([[-6.4145e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-2.8522e-09], grad_fn=<SubBackward0>)\n",
            "iteration 789, loss is: 0.20588722825050354, w is: 0.8903307318687439, b is 0.0002621658204589039\n",
            "790\n",
            "tensor([[7.0082e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-1.8626e-09], grad_fn=<SubBackward0>)\n",
            "iteration 790, loss is: 0.20588719844818115, w is: 0.8903387188911438, b is 0.0002595442347228527\n",
            "791\n",
            "tensor([[-4.4471e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-5.5879e-09], grad_fn=<SubBackward0>)\n",
            "iteration 791, loss is: 0.20588719844818115, w is: 0.8903466463088989, b is 0.00025694884243421257\n",
            "792\n",
            "tensor([[-1.3469e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([-3.7253e-09], grad_fn=<SubBackward0>)\n",
            "iteration 792, loss is: 0.20588719844818115, w is: 0.8903545141220093, b is 0.00025437941076233983\n",
            "793\n",
            "tensor([[1.1642e-10]], grad_fn=<SubBackward0>)\n",
            "tensor([1.9791e-09], grad_fn=<SubBackward0>)\n",
            "iteration 793, loss is: 0.20588719844818115, w is: 0.8903622627258301, b is 0.00025183556135743856\n",
            "794\n",
            "tensor([[1.4494e-07]], grad_fn=<SubBackward0>)\n",
            "tensor([1.3970e-09], grad_fn=<SubBackward0>)\n",
            "iteration 794, loss is: 0.20588715374469757, w is: 0.8903699517250061, b is 0.0002493172069080174\n",
            "795\n",
            "tensor([[-1.3039e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([5.2969e-09], grad_fn=<SubBackward0>)\n",
            "iteration 795, loss is: 0.20588713884353638, w is: 0.8903775811195374, b is 0.0002468240272719413\n",
            "796\n",
            "tensor([[5.9372e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2119e-09], grad_fn=<SubBackward0>)\n",
            "iteration 796, loss is: 0.20588715374469757, w is: 0.890385091304779, b is 0.0002443556732032448\n",
            "797\n",
            "tensor([[8.8476e-09]], grad_fn=<SubBackward0>)\n",
            "tensor([2.2410e-09], grad_fn=<SubBackward0>)\n",
            "iteration 797, loss is: 0.20588713884353638, w is: 0.890392541885376, b is 0.0002419120428385213\n",
            "798\n",
            "tensor([[-3.7253e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([1.7462e-10], grad_fn=<SubBackward0>)\n",
            "iteration 798, loss is: 0.205887109041214, w is: 0.8903999328613281, b is 0.00023949290334712714\n",
            "799\n",
            "tensor([[4.6450e-08]], grad_fn=<SubBackward0>)\n",
            "tensor([-7.5961e-09], grad_fn=<SubBackward0>)\n",
            "iteration 799, loss is: 0.205887109041214, w is: 0.8904072642326355, b is 0.0002370979345869273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        # c. \n",
        "        d = 1 # only one regressor.\n",
        "        w = torch.randn(1,d,requires_grad=True)\n",
        "        b = torch.randn(1,requires_grad=True)\n",
        "        optim = torch.optim.SGD([w,b],lr=3e-3)\n",
        "\n",
        "        for t in range(niter):\n",
        "            y_pred = torch.matmul(xT,w)+b # first dimension in xT is minibatch\n",
        "            # the statement above can be generalized to y = model(x)\n",
        "            err = (y_pred - y)\n",
        "            loss = err.pow(2.0).mean() # mean squared error\n",
        "            loss.backward() # run backpropagation\n",
        "\n",
        "            optim.step() # update the parameters\n",
        "            optim.zero_grad()\n",
        "            print(f\"iteration {t}, loss is: {loss}, w is: {float(w)}, b is {float(b)}\")"
      ],
      "metadata": {
        "id": "ZY94yQmgHT9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e7b045-f87c-4fce-ea55-c9e25e437a1b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0, loss is: 0.240524023771286, w is: 0.7176312208175659, b is -0.06419597566127777\n",
            "iteration 1, loss is: 0.2401096373796463, w is: 0.7186722159385681, b is -0.0638108029961586\n",
            "iteration 2, loss is: 0.23970019817352295, w is: 0.7197069525718689, b is -0.06342794001102448\n",
            "iteration 3, loss is: 0.23929563164710999, w is: 0.720735490322113, b is -0.0630473718047142\n",
            "iteration 4, loss is: 0.2388959676027298, w is: 0.7217578887939453, b is -0.06266909092664719\n",
            "iteration 5, loss is: 0.23850099742412567, w is: 0.7227741479873657, b is -0.062293075025081635\n",
            "iteration 6, loss is: 0.23811081051826477, w is: 0.7237842679023743, b is -0.06191931664943695\n",
            "iteration 7, loss is: 0.23772528767585754, w is: 0.7247883677482605, b is -0.06154780089855194\n",
            "iteration 8, loss is: 0.23734432458877563, w is: 0.7257864475250244, b is -0.06117851287126541\n",
            "iteration 9, loss is: 0.23696796596050262, w is: 0.726778507232666, b is -0.06081144139170647\n",
            "iteration 10, loss is: 0.23659609258174896, w is: 0.7277646064758301, b is -0.06044657155871391\n",
            "iteration 11, loss is: 0.23622870445251465, w is: 0.7287448048591614, b is -0.060083892196416855\n",
            "iteration 12, loss is: 0.23586569726467133, w is: 0.7297191023826599, b is -0.0597233884036541\n",
            "iteration 13, loss is: 0.2355070561170578, w is: 0.7306875586509705, b is -0.05936504900455475\n",
            "iteration 14, loss is: 0.23515267670154572, w is: 0.7316502332687378, b is -0.05900885909795761\n",
            "iteration 15, loss is: 0.2348025143146515, w is: 0.7326071262359619, b is -0.05865480750799179\n",
            "iteration 16, loss is: 0.23445656895637512, w is: 0.7335582375526428, b is -0.058302879333496094\n",
            "iteration 17, loss is: 0.23411476612091064, w is: 0.7345036864280701, b is -0.057953063398599625\n",
            "iteration 18, loss is: 0.2337770164012909, w is: 0.7354434728622437, b is -0.05760534480214119\n",
            "iteration 19, loss is: 0.23344331979751587, w is: 0.7363775968551636, b is -0.05725971236824989\n",
            "iteration 20, loss is: 0.23311366140842438, w is: 0.7373061180114746, b is -0.05691615492105484\n",
            "iteration 21, loss is: 0.23278790712356567, w is: 0.7382290363311768, b is -0.056574657559394836\n",
            "iteration 22, loss is: 0.23246605694293976, w is: 0.7391464710235596, b is -0.05623520910739899\n",
            "iteration 23, loss is: 0.23214805126190186, w is: 0.7400583624839783, b is -0.055897798389196396\n",
            "iteration 24, loss is: 0.23183386027812958, w is: 0.7409647703170776, b is -0.05556241050362587\n",
            "iteration 25, loss is: 0.23152345418930054, w is: 0.7418657541275024, b is -0.05522903427481651\n",
            "iteration 26, loss is: 0.23121672868728638, w is: 0.7427613139152527, b is -0.05489765852689743\n",
            "iteration 27, loss is: 0.2309136837720871, w is: 0.7436515092849731, b is -0.054568272083997726\n",
            "iteration 28, loss is: 0.2306142896413803, w is: 0.7445363998413086, b is -0.054240863770246506\n",
            "iteration 29, loss is: 0.23031841218471527, w is: 0.745415985584259, b is -0.053915418684482574\n",
            "iteration 30, loss is: 0.23002611100673676, w is: 0.7462902665138245, b is -0.05359192565083504\n",
            "iteration 31, loss is: 0.2297373116016388, w is: 0.7471593022346497, b is -0.053270373493433\n",
            "iteration 32, loss is: 0.2294519543647766, w is: 0.7480231523513794, b is -0.05295075103640556\n",
            "iteration 33, loss is: 0.22916999459266663, w is: 0.7488818168640137, b is -0.052633047103881836\n",
            "iteration 34, loss is: 0.22889146208763123, w is: 0.7497352957725525, b is -0.05231725051999092\n",
            "iteration 35, loss is: 0.22861620783805847, w is: 0.7505836486816406, b is -0.052003346383571625\n",
            "iteration 36, loss is: 0.22834430634975433, w is: 0.7514269351959229, b is -0.05169132724404335\n",
            "iteration 37, loss is: 0.22807559370994568, w is: 0.7522651553153992, b is -0.051381178200244904\n",
            "iteration 38, loss is: 0.22781012952327728, w is: 0.7530983686447144, b is -0.05107289180159569\n",
            "iteration 39, loss is: 0.22754783928394318, w is: 0.7539265751838684, b is -0.05076645314693451\n",
            "iteration 40, loss is: 0.22728866338729858, w is: 0.7547497749328613, b is -0.05046185478568077\n",
            "iteration 41, loss is: 0.2270326167345047, w is: 0.7555680871009827, b is -0.05015908181667328\n",
            "iteration 42, loss is: 0.22677959501743317, w is: 0.7563814520835876, b is -0.049858126789331436\n",
            "iteration 43, loss is: 0.22652961313724518, w is: 0.757189929485321, b is -0.04955897852778435\n",
            "iteration 44, loss is: 0.22628268599510193, w is: 0.7579935789108276, b is -0.04926162585616112\n",
            "iteration 45, loss is: 0.22603864967823029, w is: 0.7587924003601074, b is -0.04896605759859085\n",
            "iteration 46, loss is: 0.22579757869243622, w is: 0.7595864534378052, b is -0.04867226257920265\n",
            "iteration 47, loss is: 0.2255593091249466, w is: 0.7603757381439209, b is -0.048380229622125626\n",
            "iteration 48, loss is: 0.22532397508621216, w is: 0.7611602544784546, b is -0.048089947551488876\n",
            "iteration 49, loss is: 0.22509144246578217, w is: 0.761940062046051, b is -0.04780140891671181\n",
            "iteration 50, loss is: 0.22486165165901184, w is: 0.762715220451355, b is -0.047514598816633224\n",
            "iteration 51, loss is: 0.22463463246822357, w is: 0.7634857296943665, b is -0.04722950980067253\n",
            "iteration 52, loss is: 0.22441032528877258, w is: 0.7642515897750854, b is -0.04694613441824913\n",
            "iteration 53, loss is: 0.22418873012065887, w is: 0.7650128602981567, b is -0.04666445776820183\n",
            "iteration 54, loss is: 0.2239697277545929, w is: 0.7657695412635803, b is -0.04638447239995003\n",
            "iteration 55, loss is: 0.2237534075975418, w is: 0.766521692276001, b is -0.04610616713762283\n",
            "iteration 56, loss is: 0.22353965044021606, w is: 0.7672693729400635, b is -0.04582953080534935\n",
            "iteration 57, loss is: 0.22332845628261566, w is: 0.768012523651123, b is -0.04555455222725868\n",
            "iteration 58, loss is: 0.2231197953224182, w is: 0.7687512040138245, b is -0.04528122395277023\n",
            "iteration 59, loss is: 0.22291357815265656, w is: 0.7694854736328125, b is -0.04500953480601311\n",
            "iteration 60, loss is: 0.22270989418029785, w is: 0.7702153325080872, b is -0.04473947733640671\n",
            "iteration 61, loss is: 0.22250862419605255, w is: 0.7709408402442932, b is -0.04447104036808014\n",
            "iteration 62, loss is: 0.22230973839759827, w is: 0.7716619968414307, b is -0.044204212725162506\n",
            "iteration 63, loss is: 0.2221132516860962, w is: 0.7723788022994995, b is -0.04393898695707321\n",
            "iteration 64, loss is: 0.22191911935806274, w is: 0.7730913162231445, b is -0.04367535188794136\n",
            "iteration 65, loss is: 0.22172728180885315, w is: 0.7737995386123657, b is -0.043413300067186356\n",
            "iteration 66, loss is: 0.221537783741951, w is: 0.7745035290718079, b is -0.0431528203189373\n",
            "iteration 67, loss is: 0.2213505208492279, w is: 0.775203287601471, b is -0.0428939051926136\n",
            "iteration 68, loss is: 0.2211655229330063, w is: 0.7758988738059998, b is -0.04263654351234436\n",
            "iteration 69, loss is: 0.22098271548748016, w is: 0.7765902876853943, b is -0.04238072410225868\n",
            "iteration 70, loss is: 0.22080211341381073, w is: 0.7772775292396545, b is -0.04212643951177597\n",
            "iteration 71, loss is: 0.2206236720085144, w is: 0.7779606580734253, b is -0.04187368229031563\n",
            "iteration 72, loss is: 0.2204473465681076, w is: 0.7786396741867065, b is -0.04162244126200676\n",
            "iteration 73, loss is: 0.22027312219142914, w is: 0.7793146371841431, b is -0.04137270525097847\n",
            "iteration 74, loss is: 0.220100998878479, w is: 0.7799855470657349, b is -0.04112447053194046\n",
            "iteration 75, loss is: 0.21993093192577362, w is: 0.7806524038314819, b is -0.04087772220373154\n",
            "iteration 76, loss is: 0.219762921333313, w is: 0.781315267086029, b is -0.0406324565410614\n",
            "iteration 77, loss is: 0.21959692239761353, w is: 0.7819741368293762, b is -0.04038866236805916\n",
            "iteration 78, loss is: 0.21943287551403046, w is: 0.7826290726661682, b is -0.04014633223414421\n",
            "iteration 79, loss is: 0.21927082538604736, w is: 0.783280074596405, b is -0.03990545496344566\n",
            "iteration 80, loss is: 0.21911071240901947, w is: 0.7839272022247314, b is -0.03966602310538292\n",
            "iteration 81, loss is: 0.2189524620771408, w is: 0.7845703959465027, b is -0.03942802548408508\n",
            "iteration 82, loss is: 0.21879613399505615, w is: 0.7852097749710083, b is -0.039191458374261856\n",
            "iteration 83, loss is: 0.21864168345928192, w is: 0.7858452796936035, b is -0.03895631060004234\n",
            "iteration 84, loss is: 0.21848909556865692, w is: 0.7864769697189331, b is -0.03872257098555565\n",
            "iteration 85, loss is: 0.21833831071853638, w is: 0.7871049046516418, b is -0.038490235805511475\n",
            "iteration 86, loss is: 0.21818935871124268, w is: 0.7877290844917297, b is -0.038259293884038925\n",
            "iteration 87, loss is: 0.21804213523864746, w is: 0.7883495092391968, b is -0.038029737770557404\n",
            "iteration 88, loss is: 0.2178967148065567, w is: 0.788966178894043, b is -0.03780156001448631\n",
            "iteration 89, loss is: 0.21775302290916443, w is: 0.7895791530609131, b is -0.03757474943995476\n",
            "iteration 90, loss is: 0.21761107444763184, w is: 0.7901884317398071, b is -0.03734930232167244\n",
            "iteration 91, loss is: 0.21747078001499176, w is: 0.7907940745353699, b is -0.03712520748376846\n",
            "iteration 92, loss is: 0.21733219921588898, w is: 0.7913960814476013, b is -0.03690245747566223\n",
            "iteration 93, loss is: 0.21719525754451752, w is: 0.7919945120811462, b is -0.03668104112148285\n",
            "iteration 94, loss is: 0.21705995500087738, w is: 0.7925893068313599, b is -0.03646095469594002\n",
            "iteration 95, loss is: 0.21692626178264618, w is: 0.793180525302887, b is -0.03624219074845314\n",
            "iteration 96, loss is: 0.2167942076921463, w is: 0.7937682271003723, b is -0.03602473810315132\n",
            "iteration 97, loss is: 0.21666373312473297, w is: 0.7943524122238159, b is -0.035808589309453964\n",
            "iteration 98, loss is: 0.21653476357460022, w is: 0.7949330806732178, b is -0.03559373691678047\n",
            "iteration 99, loss is: 0.21640737354755402, w is: 0.7955102920532227, b is -0.03538017347455025\n",
            "iteration 100, loss is: 0.2162814885377884, w is: 0.7960839867591858, b is -0.035167891532182693\n",
            "iteration 101, loss is: 0.21615713834762573, w is: 0.7966542840003967, b is -0.034956883639097214\n",
            "iteration 102, loss is: 0.21603427827358246, w is: 0.7972211241722107, b is -0.03474714234471321\n",
            "iteration 103, loss is: 0.2159128487110138, w is: 0.7977845668792725, b is -0.03453866019845009\n",
            "iteration 104, loss is: 0.21579290926456451, w is: 0.7983446717262268, b is -0.03433142974972725\n",
            "iteration 105, loss is: 0.21567437052726746, w is: 0.798901379108429, b is -0.0341254398226738\n",
            "iteration 106, loss is: 0.2155572772026062, w is: 0.7994547486305237, b is -0.033920686691999435\n",
            "iteration 107, loss is: 0.21544156968593597, w is: 0.800004780292511, b is -0.033717162907123566\n",
            "iteration 108, loss is: 0.21532727777957916, w is: 0.8005515336990356, b is -0.03351486101746559\n",
            "iteration 109, loss is: 0.2152142971754074, w is: 0.8010950088500977, b is -0.033313773572444916\n",
            "iteration 110, loss is: 0.21510271728038788, w is: 0.801635205745697, b is -0.03311388939619064\n",
            "iteration 111, loss is: 0.2149924784898758, w is: 0.8021721839904785, b is -0.032915204763412476\n",
            "iteration 112, loss is: 0.21488350629806519, w is: 0.8027059435844421, b is -0.032717712223529816\n",
            "iteration 113, loss is: 0.21477587521076202, w is: 0.8032364845275879, b is -0.03252140432596207\n",
            "iteration 114, loss is: 0.21466954052448273, w is: 0.8037638664245605, b is -0.03232627734541893\n",
            "iteration 115, loss is: 0.2145644575357437, w is: 0.8042880892753601, b is -0.03213232010602951\n",
            "iteration 116, loss is: 0.2144605964422226, w is: 0.8048091530799866, b is -0.03193952515721321\n",
            "iteration 117, loss is: 0.21435803174972534, w is: 0.8053270578384399, b is -0.03174788877367973\n",
            "iteration 118, loss is: 0.21425668895244598, w is: 0.805841863155365, b is -0.03155739977955818\n",
            "iteration 119, loss is: 0.21415653824806213, w is: 0.8063535690307617, b is -0.03136805444955826\n",
            "iteration 120, loss is: 0.21405760943889618, w is: 0.8068622350692749, b is -0.031179845333099365\n",
            "iteration 121, loss is: 0.21395985782146454, w is: 0.8073678612709045, b is -0.030992766842246056\n",
            "iteration 122, loss is: 0.21386325359344482, w is: 0.8078704476356506, b is -0.030806809663772583\n",
            "iteration 123, loss is: 0.21376781165599823, w is: 0.8083699941635132, b is -0.0306219682097435\n",
            "iteration 124, loss is: 0.21367354691028595, w is: 0.808866560459137, b is -0.030438236892223358\n",
            "iteration 125, loss is: 0.21358034014701843, w is: 0.809360146522522, b is -0.03025560826063156\n",
            "iteration 126, loss is: 0.21348831057548523, w is: 0.8098507523536682, b is -0.030074074864387512\n",
            "iteration 127, loss is: 0.21339736878871918, w is: 0.8103384375572205, b is -0.029893631115555763\n",
            "iteration 128, loss is: 0.21330752968788147, w is: 0.8108232021331787, b is -0.029714269563555717\n",
            "iteration 129, loss is: 0.21321871876716614, w is: 0.811305046081543, b is -0.029535984620451927\n",
            "iteration 130, loss is: 0.21313101053237915, w is: 0.8117839694023132, b is -0.029358768835663795\n",
            "iteration 131, loss is: 0.21304433047771454, w is: 0.8122600317001343, b is -0.029182616621255875\n",
            "iteration 132, loss is: 0.2129587084054947, w is: 0.8127332329750061, b is -0.029007520526647568\n",
            "iteration 133, loss is: 0.21287409961223602, w is: 0.8132036328315735, b is -0.028833474963903427\n",
            "iteration 134, loss is: 0.21279050409793854, w is: 0.8136711716651917, b is -0.028660474345088005\n",
            "iteration 135, loss is: 0.21270787715911865, w is: 0.8141359090805054, b is -0.028488511219620705\n",
            "iteration 136, loss is: 0.21262626349925995, w is: 0.8145979046821594, b is -0.028317579999566078\n",
            "iteration 137, loss is: 0.21254563331604004, w is: 0.815057098865509, b is -0.028147675096988678\n",
            "iteration 138, loss is: 0.21246598660945892, w is: 0.815513551235199, b is -0.027978789061307907\n",
            "iteration 139, loss is: 0.21238726377487183, w is: 0.8159672617912292, b is -0.027810916304588318\n",
            "iteration 140, loss is: 0.21230946481227875, w is: 0.8164182305335999, b is -0.027644051238894463\n",
            "iteration 141, loss is: 0.21223263442516327, w is: 0.8168665170669556, b is -0.027478186413645744\n",
            "iteration 142, loss is: 0.2121567279100418, w is: 0.8173121213912964, b is -0.027313318103551865\n",
            "iteration 143, loss is: 0.2120816856622696, w is: 0.8177550435066223, b is -0.027149438858032227\n",
            "iteration 144, loss is: 0.2120075672864914, w is: 0.8181952834129333, b is -0.026986543089151382\n",
            "iteration 145, loss is: 0.21193434298038483, w is: 0.8186329007148743, b is -0.026824623346328735\n",
            "iteration 146, loss is: 0.2118619829416275, w is: 0.8190678954124451, b is -0.026663675904273987\n",
            "iteration 147, loss is: 0.21179048717021942, w is: 0.8195002675056458, b is -0.02650369331240654\n",
            "iteration 148, loss is: 0.21171988546848297, w is: 0.8199300765991211, b is -0.026344671845436096\n",
            "iteration 149, loss is: 0.2116500735282898, w is: 0.8203572630882263, b is -0.02618660405278206\n",
            "iteration 150, loss is: 0.21158114075660706, w is: 0.8207818865776062, b is -0.02602948434650898\n",
            "iteration 151, loss is: 0.2115129977464676, w is: 0.8212040066719055, b is -0.025873307138681412\n",
            "iteration 152, loss is: 0.2114456743001938, w is: 0.8216235637664795, b is -0.025718066841363907\n",
            "iteration 153, loss is: 0.21137917041778564, w is: 0.8220406174659729, b is -0.025563757866621017\n",
            "iteration 154, loss is: 0.21131344139575958, w is: 0.8224551677703857, b is -0.025410374626517296\n",
            "iteration 155, loss is: 0.211248517036438, w is: 0.822867214679718, b is -0.025257911533117294\n",
            "iteration 156, loss is: 0.21118435263633728, w is: 0.8232768177986145, b is -0.025106364861130714\n",
            "iteration 157, loss is: 0.21112096309661865, w is: 0.8236839175224304, b is -0.02495572715997696\n",
            "iteration 158, loss is: 0.2110583484172821, w is: 0.8240885734558105, b is -0.02480599284172058\n",
            "iteration 159, loss is: 0.21099650859832764, w is: 0.8244908452033997, b is -0.024657156318426132\n",
            "iteration 160, loss is: 0.2109353393316269, w is: 0.824890673160553, b is -0.024509213864803314\n",
            "iteration 161, loss is: 0.21087494492530823, w is: 0.8252881169319153, b is -0.02436215803027153\n",
            "iteration 162, loss is: 0.21081525087356567, w is: 0.8256831765174866, b is -0.02421598508954048\n",
            "iteration 163, loss is: 0.21075628697872162, w is: 0.8260758519172668, b is -0.02407068945467472\n",
            "iteration 164, loss is: 0.21069803833961487, w is: 0.8264662027359009, b is -0.0239262655377388\n",
            "iteration 165, loss is: 0.21064046025276184, w is: 0.8268541693687439, b is -0.02378270775079727\n",
            "iteration 166, loss is: 0.21058359742164612, w is: 0.8272398114204407, b is -0.023640012368559837\n",
            "iteration 167, loss is: 0.21052740514278412, w is: 0.8276231288909912, b is -0.0234981719404459\n",
            "iteration 168, loss is: 0.21047186851501465, w is: 0.8280041813850403, b is -0.02335718274116516\n",
            "iteration 169, loss is: 0.2104170173406601, w is: 0.8283829092979431, b is -0.023217039182782173\n",
            "iteration 170, loss is: 0.21036282181739807, w is: 0.8287593722343445, b is -0.023077737540006638\n",
            "iteration 171, loss is: 0.21030928194522858, w is: 0.8291335701942444, b is -0.022939270362257957\n",
            "iteration 172, loss is: 0.21025635302066803, w is: 0.8295055627822876, b is -0.022801633924245834\n",
            "iteration 173, loss is: 0.2102040797472, w is: 0.8298752903938293, b is -0.02266482450067997\n",
            "iteration 174, loss is: 0.21015243232250214, w is: 0.8302428126335144, b is -0.022528834640979767\n",
            "iteration 175, loss is: 0.2101014107465744, w is: 0.8306081295013428, b is -0.022393662482500076\n",
            "iteration 176, loss is: 0.21005097031593323, w is: 0.8309712409973145, b is -0.0222593005746603\n",
            "iteration 177, loss is: 0.210001140832901, w is: 0.8313322067260742, b is -0.022125745192170143\n",
            "iteration 178, loss is: 0.20995190739631653, w is: 0.8316909670829773, b is -0.021992990747094154\n",
            "iteration 179, loss is: 0.209903284907341, w is: 0.8320475816726685, b is -0.021861033514142036\n",
            "iteration 180, loss is: 0.20985524356365204, w is: 0.8324020504951477, b is -0.02172986790537834\n",
            "iteration 181, loss is: 0.20980773866176605, w is: 0.8327544331550598, b is -0.021599488332867622\n",
            "iteration 182, loss is: 0.20976081490516663, w is: 0.83310467004776, b is -0.02146989107131958\n",
            "iteration 183, loss is: 0.20971447229385376, w is: 0.8334528207778931, b is -0.021341072395443916\n",
            "iteration 184, loss is: 0.20966868102550507, w is: 0.833798885345459, b is -0.021213026717305183\n",
            "iteration 185, loss is: 0.20962344110012054, w is: 0.8341428637504578, b is -0.021085748448967934\n",
            "iteration 186, loss is: 0.2095787227153778, w is: 0.8344848155975342, b is -0.02095923386514187\n",
            "iteration 187, loss is: 0.20953455567359924, w is: 0.8348246812820435, b is -0.02083347924053669\n",
            "iteration 188, loss is: 0.20949091017246246, w is: 0.8351625204086304, b is -0.02070847898721695\n",
            "iteration 189, loss is: 0.20944780111312866, w is: 0.8354983329772949, b is -0.0205842275172472\n",
            "iteration 190, loss is: 0.20940518379211426, w is: 0.8358321189880371, b is -0.020460722967982292\n",
            "iteration 191, loss is: 0.20936307311058044, w is: 0.8361639380455017, b is -0.02033795788884163\n",
            "iteration 192, loss is: 0.20932148396968842, w is: 0.836493730545044, b is -0.02021593041718006\n",
            "iteration 193, loss is: 0.20928038656711578, w is: 0.8368215560913086, b is -0.02009463496506214\n",
            "iteration 194, loss is: 0.20923979580402374, w is: 0.8371474146842957, b is -0.01997406780719757\n",
            "iteration 195, loss is: 0.20919965207576752, w is: 0.8374713063240051, b is -0.019854223355650902\n",
            "iteration 196, loss is: 0.20916001498699188, w is: 0.8377932906150818, b is -0.019735097885131836\n",
            "iteration 197, loss is: 0.20912083983421326, w is: 0.8381133079528809, b is -0.019616687670350075\n",
            "iteration 198, loss is: 0.20908215641975403, w is: 0.8384314179420471, b is -0.01949898712337017\n",
            "iteration 199, loss is: 0.209043949842453, w is: 0.8387476205825806, b is -0.019381992518901825\n",
            "iteration 200, loss is: 0.2090061604976654, w is: 0.8390619158744812, b is -0.01926570013165474\n",
            "iteration 201, loss is: 0.20896881818771362, w is: 0.839374303817749, b is -0.019150106236338615\n",
            "iteration 202, loss is: 0.20893195271492004, w is: 0.8396848440170288, b is -0.019035205245018005\n",
            "iteration 203, loss is: 0.20889553427696228, w is: 0.8399935364723206, b is -0.01892099343240261\n",
            "iteration 204, loss is: 0.20885950326919556, w is: 0.8403003811836243, b is -0.018807467073202133\n",
            "iteration 205, loss is: 0.20882394909858704, w is: 0.8406053781509399, b is -0.018694622442126274\n",
            "iteration 206, loss is: 0.20878881216049194, w is: 0.8409085273742676, b is -0.018582453951239586\n",
            "iteration 207, loss is: 0.20875407755374908, w is: 0.841209888458252, b is -0.01847095973789692\n",
            "iteration 208, loss is: 0.20871977508068085, w is: 0.8415094017982483, b is -0.018360134214162827\n",
            "iteration 209, loss is: 0.20868588984012604, w is: 0.8418071269989014, b is -0.01824997365474701\n",
            "iteration 210, loss is: 0.20865239202976227, w is: 0.8421030640602112, b is -0.01814047433435917\n",
            "iteration 211, loss is: 0.20861928164958954, w is: 0.8423972129821777, b is -0.018031630665063858\n",
            "iteration 212, loss is: 0.20858657360076904, w is: 0.8426896333694458, b is -0.017923440784215927\n",
            "iteration 213, loss is: 0.20855428278446198, w is: 0.8429802656173706, b is -0.017815900966525078\n",
            "iteration 214, loss is: 0.20852237939834595, w is: 0.8432691693305969, b is -0.017709005624055862\n",
            "iteration 215, loss is: 0.20849086344242096, w is: 0.8435563445091248, b is -0.017602751031517982\n",
            "iteration 216, loss is: 0.20845969021320343, w is: 0.8438417911529541, b is -0.01749713532626629\n",
            "iteration 217, loss is: 0.20842888951301575, w is: 0.844125509262085, b is -0.017392152920365334\n",
            "iteration 218, loss is: 0.2083984762430191, w is: 0.8444075584411621, b is -0.01728780008852482\n",
            "iteration 219, loss is: 0.2083684355020523, w is: 0.8446878790855408, b is -0.017184073105454445\n",
            "iteration 220, loss is: 0.20833873748779297, w is: 0.8449665307998657, b is -0.017080968245863914\n",
            "iteration 221, loss is: 0.20830939710140228, w is: 0.845243513584137, b is -0.01697848178446293\n",
            "iteration 222, loss is: 0.20828039944171906, w is: 0.8455188274383545, b is -0.01687660999596119\n",
            "iteration 223, loss is: 0.20825178921222687, w is: 0.8457924723625183, b is -0.016775351017713547\n",
            "iteration 224, loss is: 0.20822346210479736, w is: 0.8460645079612732, b is -0.016674699261784554\n",
            "iteration 225, loss is: 0.2081955075263977, w is: 0.8463348746299744, b is -0.01657465100288391\n",
            "iteration 226, loss is: 0.2081678807735443, w is: 0.8466036319732666, b is -0.01647520251572132\n",
            "iteration 227, loss is: 0.20814059674739838, w is: 0.8468707799911499, b is -0.016376351937651634\n",
            "iteration 228, loss is: 0.2081136256456375, w is: 0.8471363186836243, b is -0.016278093680739403\n",
            "iteration 229, loss is: 0.2080869972705841, w is: 0.8474003076553345, b is -0.016180425882339478\n",
            "iteration 230, loss is: 0.20806066691875458, w is: 0.8476626873016357, b is -0.01608334295451641\n",
            "iteration 231, loss is: 0.20803464949131012, w is: 0.8479235172271729, b is -0.015986843034625053\n",
            "iteration 232, loss is: 0.20800894498825073, w is: 0.848182737827301, b is -0.015890922397375107\n",
            "iteration 233, loss is: 0.2079835683107376, w is: 0.848440408706665, b is -0.015795577317476273\n",
            "iteration 234, loss is: 0.20795845985412598, w is: 0.8486965298652649, b is -0.015700804069638252\n",
            "iteration 235, loss is: 0.2079336792230606, w is: 0.8489511609077454, b is -0.015606598928570747\n",
            "iteration 236, loss is: 0.20790918171405792, w is: 0.8492042422294617, b is -0.015512959100306034\n",
            "iteration 237, loss is: 0.2078849971294403, w is: 0.8494557738304138, b is -0.015419881790876389\n",
            "iteration 238, loss is: 0.20786109566688538, w is: 0.8497058153152466, b is -0.015327362343668938\n",
            "iteration 239, loss is: 0.20783744752407074, w is: 0.84995436668396, b is -0.015235397964715958\n",
            "iteration 240, loss is: 0.20781412720680237, w is: 0.850201427936554, b is -0.015143985860049725\n",
            "iteration 241, loss is: 0.2077910304069519, w is: 0.8504469990730286, b is -0.01505312230437994\n",
            "iteration 242, loss is: 0.2077682912349701, w is: 0.8506910800933838, b is -0.014962803572416306\n",
            "iteration 243, loss is: 0.2077457457780838, w is: 0.8509337306022644, b is -0.014873026870191097\n",
            "iteration 244, loss is: 0.20772351324558258, w is: 0.8511748909950256, b is -0.014783788472414017\n",
            "iteration 245, loss is: 0.20770154893398285, w is: 0.8514146208763123, b is -0.01469508558511734\n",
            "iteration 246, loss is: 0.20767982304096222, w is: 0.8516529202461243, b is -0.014606915414333344\n",
            "iteration 247, loss is: 0.20765838027000427, w is: 0.8518897891044617, b is -0.014519274234771729\n",
            "iteration 248, loss is: 0.20763716101646423, w is: 0.8521252274513245, b is -0.014432158321142197\n",
            "iteration 249, loss is: 0.20761622488498688, w is: 0.8523592352867126, b is -0.014345565810799599\n",
            "iteration 250, loss is: 0.207595556974411, w is: 0.852591872215271, b is -0.014259492047131062\n",
            "iteration 251, loss is: 0.20757511258125305, w is: 0.8528230786323547, b is -0.014173935167491436\n",
            "iteration 252, loss is: 0.207554891705513, w is: 0.8530529141426086, b is -0.014088891446590424\n",
            "iteration 253, loss is: 0.20753492414951324, w is: 0.8532813787460327, b is -0.0140043580904603\n",
            "iteration 254, loss is: 0.20751523971557617, w is: 0.853508472442627, b is -0.013920332305133343\n",
            "iteration 255, loss is: 0.20749571919441223, w is: 0.8537341952323914, b is -0.013836810365319252\n",
            "iteration 256, loss is: 0.20747646689414978, w is: 0.8539585471153259, b is -0.013753789477050304\n",
            "iteration 257, loss is: 0.20745745301246643, w is: 0.8541815876960754, b is -0.013671266846358776\n",
            "iteration 258, loss is: 0.207438662648201, w is: 0.8544032573699951, b is -0.013589239679276943\n",
            "iteration 259, loss is: 0.20742008090019226, w is: 0.8546236157417297, b is -0.013507704250514507\n",
            "iteration 260, loss is: 0.20740173757076263, w is: 0.8548426628112793, b is -0.013426657766103745\n",
            "iteration 261, loss is: 0.20738358795642853, w is: 0.8550603985786438, b is -0.013346097432076931\n",
            "iteration 262, loss is: 0.2073657214641571, w is: 0.8552768230438232, b is -0.013266020454466343\n",
            "iteration 263, loss is: 0.20734800398349762, w is: 0.8554919362068176, b is -0.013186424039304256\n",
            "iteration 264, loss is: 0.20733051002025604, w is: 0.855705738067627, b is -0.013107305392622948\n",
            "iteration 265, loss is: 0.20731325447559357, w is: 0.855918288230896, b is -0.013028661720454693\n",
            "iteration 266, loss is: 0.20729617774486542, w is: 0.8561295866966248, b is -0.012950489297509193\n",
            "iteration 267, loss is: 0.2072792947292328, w is: 0.8563395738601685, b is -0.0128727862611413\n",
            "iteration 268, loss is: 0.20726265013217926, w is: 0.8565483093261719, b is -0.01279554981738329\n",
            "iteration 269, loss is: 0.20724616944789886, w is: 0.856755793094635, b is -0.012718776240944862\n",
            "iteration 270, loss is: 0.20722991228103638, w is: 0.8569620251655579, b is -0.01264246366918087\n",
            "iteration 271, loss is: 0.2072138488292694, w is: 0.8571670651435852, b is -0.012566609308123589\n",
            "iteration 272, loss is: 0.20719796419143677, w is: 0.8573708534240723, b is -0.01249120943248272\n",
            "iteration 273, loss is: 0.20718227326869965, w is: 0.857573390007019, b is -0.012416262179613113\n",
            "iteration 274, loss is: 0.20716677606105804, w is: 0.8577747344970703, b is -0.012341764755547047\n",
            "iteration 275, loss is: 0.20715145766735077, w is: 0.8579748868942261, b is -0.012267714366316795\n",
            "iteration 276, loss is: 0.207136332988739, w is: 0.8581738471984863, b is -0.012194108217954636\n",
            "iteration 277, loss is: 0.2071213722229004, w is: 0.8583716154098511, b is -0.012120943516492844\n",
            "iteration 278, loss is: 0.2071065902709961, w is: 0.8585681915283203, b is -0.012048217467963696\n",
            "iteration 279, loss is: 0.2070920169353485, w is: 0.858763575553894, b is -0.011975928209722042\n",
            "iteration 280, loss is: 0.20707757771015167, w is: 0.8589577674865723, b is -0.01190407294780016\n",
            "iteration 281, loss is: 0.20706331729888916, w is: 0.8591508269309998, b is -0.011832648888230324\n",
            "iteration 282, loss is: 0.20704923570156097, w is: 0.8593426942825317, b is -0.011761653237044811\n",
            "iteration 283, loss is: 0.2070353478193283, w is: 0.859533429145813, b is -0.011691083200275898\n",
            "iteration 284, loss is: 0.2070215791463852, w is: 0.8597230315208435, b is -0.011620936915278435\n",
            "iteration 285, loss is: 0.2070080041885376, w is: 0.8599115014076233, b is -0.011551211588084698\n",
            "iteration 286, loss is: 0.20699459314346313, w is: 0.8600988388061523, b is -0.011481904424726963\n",
            "iteration 287, loss is: 0.20698131620883942, w is: 0.8602850437164307, b is -0.011413012631237507\n",
            "iteration 288, loss is: 0.20696821808815002, w is: 0.8604701161384583, b is -0.01134453434497118\n",
            "iteration 289, loss is: 0.20695529878139496, w is: 0.8606540560722351, b is -0.011276466771960258\n",
            "iteration 290, loss is: 0.20694249868392944, w is: 0.860836923122406, b is -0.011208808049559593\n",
            "iteration 291, loss is: 0.20692987740039825, w is: 0.8610186576843262, b is -0.01114155538380146\n",
            "iteration 292, loss is: 0.2069173902273178, w is: 0.8611993193626404, b is -0.011074705980718136\n",
            "iteration 293, loss is: 0.2069050669670105, w is: 0.8613789081573486, b is -0.01100825797766447\n",
            "iteration 294, loss is: 0.20689287781715393, w is: 0.8615574240684509, b is -0.010942208580672741\n",
            "iteration 295, loss is: 0.2068808227777481, w is: 0.8617348670959473, b is -0.010876554995775223\n",
            "iteration 296, loss is: 0.20686893165111542, w is: 0.8619112372398376, b is -0.010811295360326767\n",
            "iteration 297, loss is: 0.20685717463493347, w is: 0.8620865345001221, b is -0.010746427811682224\n",
            "iteration 298, loss is: 0.20684558153152466, w is: 0.8622608184814453, b is -0.01068194955587387\n",
            "iteration 299, loss is: 0.2068340927362442, w is: 0.8624340295791626, b is -0.010617857798933983\n",
            "iteration 300, loss is: 0.20682276785373688, w is: 0.8626062273979187, b is -0.010554150678217411\n",
            "iteration 301, loss is: 0.2068115621805191, w is: 0.8627773523330688, b is -0.010490825399756432\n",
            "iteration 302, loss is: 0.20680047571659088, w is: 0.8629474639892578, b is -0.010427880100905895\n",
            "iteration 303, loss is: 0.2067895382642746, w is: 0.8631165623664856, b is -0.010365312919020653\n",
            "iteration 304, loss is: 0.20677876472473145, w is: 0.8632846474647522, b is -0.01030312106013298\n",
            "iteration 305, loss is: 0.20676805078983307, w is: 0.8634517192840576, b is -0.010241302661597729\n",
            "iteration 306, loss is: 0.2067575454711914, w is: 0.8636177778244019, b is -0.010179854929447174\n",
            "iteration 307, loss is: 0.2067471146583557, w is: 0.8637828826904297, b is -0.010118776001036167\n",
            "iteration 308, loss is: 0.20673680305480957, w is: 0.8639469742774963, b is -0.010058063082396984\n",
            "iteration 309, loss is: 0.20672664046287537, w is: 0.8641100525856018, b is -0.009997714310884476\n",
            "iteration 310, loss is: 0.20671656727790833, w is: 0.8642721772193909, b is -0.009937727823853493\n",
            "iteration 311, loss is: 0.20670665800571442, w is: 0.8644333481788635, b is -0.009878101758658886\n",
            "iteration 312, loss is: 0.20669685304164886, w is: 0.864593505859375, b is -0.009818833321332932\n",
            "iteration 313, loss is: 0.20668713748455048, w is: 0.8647527098655701, b is -0.00975992064923048\n",
            "iteration 314, loss is: 0.20667757093906403, w is: 0.8649109601974487, b is -0.009701360948383808\n",
            "iteration 315, loss is: 0.20666810870170593, w is: 0.865068256855011, b is -0.009643152356147766\n",
            "iteration 316, loss is: 0.2066587656736374, w is: 0.8652246594429016, b is -0.009585293009877205\n",
            "iteration 317, loss is: 0.2066495269536972, w is: 0.8653801083564758, b is -0.009527781046926975\n",
            "iteration 318, loss is: 0.20664039254188538, w is: 0.8655346035957336, b is -0.009470614604651928\n",
            "iteration 319, loss is: 0.2066313773393631, w is: 0.8656882047653198, b is -0.00941379088908434\n",
            "iteration 320, loss is: 0.20662245154380798, w is: 0.8658408522605896, b is -0.00935730803757906\n",
            "iteration 321, loss is: 0.20661364495754242, w is: 0.8659926056861877, b is -0.00930116418749094\n",
            "iteration 322, loss is: 0.2066049575805664, w is: 0.8661434054374695, b is -0.009245357476174831\n",
            "iteration 323, loss is: 0.20659635961055756, w is: 0.8662933111190796, b is -0.00918988510966301\n",
            "iteration 324, loss is: 0.20658785104751587, w is: 0.8664423227310181, b is -0.0091347461566329\n",
            "iteration 325, loss is: 0.20657949149608612, w is: 0.8665904402732849, b is -0.00907993782311678\n",
            "iteration 326, loss is: 0.20657119154930115, w is: 0.8667376637458801, b is -0.009025458246469498\n",
            "iteration 327, loss is: 0.20656302571296692, w is: 0.8668839931488037, b is -0.008971305564045906\n",
            "iteration 328, loss is: 0.20655490458011627, w is: 0.8670294880867004, b is -0.008917477913200855\n",
            "iteration 329, loss is: 0.20654690265655518, w is: 0.8671740889549255, b is -0.008863973431289196\n",
            "iteration 330, loss is: 0.20653900504112244, w is: 0.8673178553581238, b is -0.008810789324343204\n",
            "iteration 331, loss is: 0.20653118193149567, w is: 0.8674607276916504, b is -0.008757924661040306\n",
            "iteration 332, loss is: 0.20652350783348083, w is: 0.8676027655601501, b is -0.008705376647412777\n",
            "iteration 333, loss is: 0.2065158635377884, w is: 0.8677439093589783, b is -0.008653144352138042\n",
            "iteration 334, loss is: 0.2065083384513855, w is: 0.8678842186927795, b is -0.008601224981248379\n",
            "iteration 335, loss is: 0.20650090277194977, w is: 0.868023693561554, b is -0.008549617603421211\n",
            "iteration 336, loss is: 0.2064935564994812, w is: 0.8681623339653015, b is -0.008498319424688816\n",
            "iteration 337, loss is: 0.2064862996339798, w is: 0.8683001399040222, b is -0.008447329513728619\n",
            "iteration 338, loss is: 0.20647908747196198, w is: 0.8684371113777161, b is -0.008396645076572895\n",
            "iteration 339, loss is: 0.2064720243215561, w is: 0.8685732483863831, b is -0.00834626518189907\n",
            "iteration 340, loss is: 0.20646503567695618, w is: 0.868708610534668, b is -0.008296187967061996\n",
            "iteration 341, loss is: 0.20645810663700104, w is: 0.868843138217926, b is -0.008246410638093948\n",
            "iteration 342, loss is: 0.20645128190517426, w is: 0.868976891040802, b is -0.008196932263672352\n",
            "iteration 343, loss is: 0.20644450187683105, w is: 0.8691098093986511, b is -0.008147750981152058\n",
            "iteration 344, loss is: 0.20643781125545502, w is: 0.8692419528961182, b is -0.008098864927887917\n",
            "iteration 345, loss is: 0.20643122494220734, w is: 0.8693732619285583, b is -0.00805027224123478\n",
            "iteration 346, loss is: 0.20642471313476562, w is: 0.8695037961006165, b is -0.008001970127224922\n",
            "iteration 347, loss is: 0.2064182609319687, w is: 0.8696335554122925, b is -0.007953958585858345\n",
            "iteration 348, loss is: 0.2064119130373001, w is: 0.8697625398635864, b is -0.007906234823167324\n",
            "iteration 349, loss is: 0.2064056098461151, w is: 0.8698907494544983, b is -0.00785879697650671\n",
            "iteration 350, loss is: 0.20639942586421967, w is: 0.8700181841850281, b is -0.007811644114553928\n",
            "iteration 351, loss is: 0.2063932865858078, w is: 0.8701448440551758, b is -0.00776477437466383\n",
            "iteration 352, loss is: 0.2063872367143631, w is: 0.8702707290649414, b is -0.007718185894191265\n",
            "iteration 353, loss is: 0.20638124644756317, w is: 0.8703958988189697, b is -0.007671876810491085\n",
            "iteration 354, loss is: 0.20637530088424683, w is: 0.870520293712616, b is -0.007625845726579428\n",
            "iteration 355, loss is: 0.20636945962905884, w is: 0.8706439733505249, b is -0.007580090779811144\n",
            "iteration 356, loss is: 0.20636367797851562, w is: 0.8707668781280518, b is -0.007534610107541084\n",
            "iteration 357, loss is: 0.20635800063610077, w is: 0.8708890676498413, b is -0.007489402312785387\n",
            "iteration 358, loss is: 0.2063523530960083, w is: 0.8710105419158936, b is -0.00744446599856019\n",
            "iteration 359, loss is: 0.2063467651605606, w is: 0.8711312413215637, b is -0.0073997993022203445\n",
            "iteration 360, loss is: 0.20634126663208008, w is: 0.8712512254714966, b is -0.007355400361120701\n",
            "iteration 361, loss is: 0.20633582770824432, w is: 0.8713704943656921, b is -0.007311267778277397\n",
            "iteration 362, loss is: 0.20633046329021454, w is: 0.8714890480041504, b is -0.007267400156706572\n",
            "iteration 363, loss is: 0.20632514357566833, w is: 0.8716068863868713, b is -0.007223795633763075\n",
            "iteration 364, loss is: 0.2063198834657669, w is: 0.871724009513855, b is -0.007180452812463045\n",
            "iteration 365, loss is: 0.20631469786167145, w is: 0.8718404769897461, b is -0.00713737029582262\n",
            "iteration 366, loss is: 0.20630958676338196, w is: 0.8719562292098999, b is -0.0070945462211966515\n",
            "iteration 367, loss is: 0.20630452036857605, w is: 0.8720712661743164, b is -0.007051978725939989\n",
            "iteration 368, loss is: 0.20629951357841492, w is: 0.8721856474876404, b is -0.007009666878730059\n",
            "iteration 369, loss is: 0.20629458129405975, w is: 0.872299313545227, b is -0.006967608816921711\n",
            "iteration 370, loss is: 0.20628970861434937, w is: 0.8724123239517212, b is -0.006925803143531084\n",
            "iteration 371, loss is: 0.20628489553928375, w is: 0.872524619102478, b is -0.006884248461574316\n",
            "iteration 372, loss is: 0.20628009736537933, w is: 0.8726362586021423, b is -0.006842942908406258\n",
            "iteration 373, loss is: 0.20627540349960327, w is: 0.8727472424507141, b is -0.006801885087043047\n",
            "iteration 374, loss is: 0.2062707245349884, w is: 0.8728575706481934, b is -0.006761073600500822\n",
            "iteration 375, loss is: 0.2062661498785019, w is: 0.8729671835899353, b is -0.006720507051795721\n",
            "iteration 376, loss is: 0.20626160502433777, w is: 0.8730761408805847, b is -0.006680184043943882\n",
            "iteration 377, loss is: 0.20625711977481842, w is: 0.8731844425201416, b is -0.006640103179961443\n",
            "iteration 378, loss is: 0.20625269412994385, w is: 0.8732921481132507, b is -0.006600262597203255\n",
            "iteration 379, loss is: 0.20624832808971405, w is: 0.8733991980552673, b is -0.006560660898685455\n",
            "iteration 380, loss is: 0.20624396204948425, w is: 0.8735055923461914, b is -0.006521296687424183\n",
            "iteration 381, loss is: 0.2062397003173828, w is: 0.873611330986023, b is -0.006482169032096863\n",
            "iteration 382, loss is: 0.20623545348644257, w is: 0.8737164735794067, b is -0.006443276070058346\n",
            "iteration 383, loss is: 0.20623129606246948, w is: 0.873820960521698, b is -0.00640461640432477\n",
            "iteration 384, loss is: 0.2062271535396576, w is: 0.8739247918128967, b is -0.006366188637912273\n",
            "iteration 385, loss is: 0.20622310042381287, w is: 0.8740280270576477, b is -0.006327991373836994\n",
            "iteration 386, loss is: 0.20621907711029053, w is: 0.8741306662559509, b is -0.00629002321511507\n",
            "iteration 387, loss is: 0.20621509850025177, w is: 0.8742326498031616, b is -0.006252283230423927\n",
            "iteration 388, loss is: 0.2062111645936966, w is: 0.8743340373039246, b is -0.006214769557118416\n",
            "iteration 389, loss is: 0.206207275390625, w is: 0.8744348287582397, b is -0.006177480798214674\n",
            "iteration 390, loss is: 0.206203430891037, w is: 0.8745350241661072, b is -0.006140416022390127\n",
            "iteration 391, loss is: 0.20619966089725494, w is: 0.8746346235275269, b is -0.006103573366999626\n",
            "iteration 392, loss is: 0.2061958760023117, w is: 0.8747336268424988, b is -0.006066951900720596\n",
            "iteration 393, loss is: 0.20619218051433563, w is: 0.874832034111023, b is -0.006030550226569176\n",
            "iteration 394, loss is: 0.20618854463100433, w is: 0.8749298453330994, b is -0.0059943669475615025\n",
            "iteration 395, loss is: 0.20618492364883423, w is: 0.875027060508728, b is -0.005958400666713715\n",
            "iteration 396, loss is: 0.2061813473701477, w is: 0.8751236796379089, b is -0.0059226504527032375\n",
            "iteration 397, loss is: 0.20617783069610596, w is: 0.8752197027206421, b is -0.005887114442884922\n",
            "iteration 398, loss is: 0.2061743438243866, w is: 0.8753151893615723, b is -0.0058517917059361935\n",
            "iteration 399, loss is: 0.20617088675498962, w is: 0.8754100799560547, b is -0.00581668084487319\n",
            "iteration 400, loss is: 0.20616750419139862, w is: 0.8755043745040894, b is -0.005781780928373337\n",
            "iteration 401, loss is: 0.2061641365289688, w is: 0.875598132610321, b is -0.005747090093791485\n",
            "iteration 402, loss is: 0.20616081357002258, w is: 0.8756913542747498, b is -0.005712607409805059\n",
            "iteration 403, loss is: 0.20615752041339874, w is: 0.8757839798927307, b is -0.005678331945091486\n",
            "iteration 404, loss is: 0.20615428686141968, w is: 0.8758760690689087, b is -0.005644261837005615\n",
            "iteration 405, loss is: 0.206151083111763, w is: 0.8759676218032837, b is -0.005610396154224873\n",
            "iteration 406, loss is: 0.2061479240655899, w is: 0.8760585784912109, b is -0.005576733965426683\n",
            "iteration 407, loss is: 0.206144779920578, w is: 0.8761489987373352, b is -0.0055432734079658985\n",
            "iteration 408, loss is: 0.20614169538021088, w is: 0.8762388825416565, b is -0.0055100140161812305\n",
            "iteration 409, loss is: 0.20613864064216614, w is: 0.8763282299041748, b is -0.00547695392742753\n",
            "iteration 410, loss is: 0.2061356157064438, w is: 0.8764170408248901, b is -0.005444092210382223\n",
            "iteration 411, loss is: 0.2061326652765274, w is: 0.8765053153038025, b is -0.005411427468061447\n",
            "iteration 412, loss is: 0.20612971484661102, w is: 0.8765930533409119, b is -0.005378958769142628\n",
            "iteration 413, loss is: 0.20612679421901703, w is: 0.8766802549362183, b is -0.00534668518230319\n",
            "iteration 414, loss is: 0.2061239331960678, w is: 0.8767669796943665, b is -0.005314605310559273\n",
            "iteration 415, loss is: 0.20612108707427979, w is: 0.8768531680107117, b is -0.005282717756927013\n",
            "iteration 416, loss is: 0.20611827075481415, w is: 0.8769388198852539, b is -0.0052510215900838375\n",
            "iteration 417, loss is: 0.2061154991388321, w is: 0.8770239949226379, b is -0.005219515413045883\n",
            "iteration 418, loss is: 0.20611277222633362, w is: 0.877108633518219, b is -0.005188198294490576\n",
            "iteration 419, loss is: 0.20611006021499634, w is: 0.8771927356719971, b is -0.005157068837434053\n",
            "iteration 420, loss is: 0.20610737800598145, w is: 0.8772763609886169, b is -0.005126126576215029\n",
            "iteration 421, loss is: 0.20610475540161133, w is: 0.8773595094680786, b is -0.005095369648188353\n",
            "iteration 422, loss is: 0.20610211789608002, w is: 0.8774421215057373, b is -0.005064797587692738\n",
            "iteration 423, loss is: 0.20609955489635468, w is: 0.8775242567062378, b is -0.005034408997744322\n",
            "iteration 424, loss is: 0.20609700679779053, w is: 0.8776059150695801, b is -0.005004202481359243\n",
            "iteration 425, loss is: 0.20609448850154877, w is: 0.8776870369911194, b is -0.004974177107214928\n",
            "iteration 426, loss is: 0.2060919851064682, w is: 0.8777676820755005, b is -0.0049443319439888\n",
            "iteration 427, loss is: 0.20608952641487122, w is: 0.8778478503227234, b is -0.004914666060358286\n",
            "iteration 428, loss is: 0.2060871124267578, w is: 0.8779275417327881, b is -0.004885178059339523\n",
            "iteration 429, loss is: 0.2060846984386444, w is: 0.8780067563056946, b is -0.004855867009609938\n",
            "iteration 430, loss is: 0.20608234405517578, w is: 0.8780854940414429, b is -0.004826731979846954\n",
            "iteration 431, loss is: 0.20608001947402954, w is: 0.878163754940033, b is -0.004797771573066711\n",
            "iteration 432, loss is: 0.2060776799917221, w is: 0.8782415390014648, b is -0.004768984857946634\n",
            "iteration 433, loss is: 0.20607538521289825, w is: 0.8783188462257385, b is -0.004740370903164148\n",
            "iteration 434, loss is: 0.20607315003871918, w is: 0.8783957362174988, b is -0.004711928777396679\n",
            "iteration 435, loss is: 0.2060709297657013, w is: 0.8784721493721008, b is -0.004683657083660364\n",
            "iteration 436, loss is: 0.20606869459152222, w is: 0.8785480856895447, b is -0.004655555356293917\n",
            "iteration 437, loss is: 0.2060665339231491, w is: 0.8786236047744751, b is -0.004627622198313475\n",
            "iteration 438, loss is: 0.206064373254776, w is: 0.8786986470222473, b is -0.004599856212735176\n",
            "iteration 439, loss is: 0.20606224238872528, w is: 0.8787732124328613, b is -0.004572256933897734\n",
            "iteration 440, loss is: 0.20606012642383575, w is: 0.8788473606109619, b is -0.004544823430478573\n",
            "iteration 441, loss is: 0.2060580551624298, w is: 0.8789210319519043, b is -0.004517554305493832\n",
            "iteration 442, loss is: 0.20605602860450745, w is: 0.8789942860603333, b is -0.004490449093282223\n",
            "iteration 443, loss is: 0.2060539871454239, w is: 0.8790671229362488, b is -0.004463506396859884\n",
            "iteration 444, loss is: 0.20605197548866272, w is: 0.8791394829750061, b is -0.0044367252849042416\n",
            "iteration 445, loss is: 0.20605002343654633, w is: 0.87921142578125, b is -0.00441010482609272\n",
            "iteration 446, loss is: 0.20604807138442993, w is: 0.8792829513549805, b is -0.004383644089102745\n",
            "iteration 447, loss is: 0.20604611933231354, w is: 0.8793540596961975, b is -0.004357342142611742\n",
            "iteration 448, loss is: 0.20604419708251953, w is: 0.8794246912002563, b is -0.004331198055297136\n",
            "iteration 449, loss is: 0.2060423195362091, w is: 0.8794949054718018, b is -0.004305210895836353\n",
            "iteration 450, loss is: 0.20604045689105988, w is: 0.8795647025108337, b is -0.004279379732906818\n",
            "iteration 451, loss is: 0.20603860914707184, w is: 0.8796340823173523, b is -0.004253703635185957\n",
            "iteration 452, loss is: 0.20603682100772858, w is: 0.8797030448913574, b is -0.004228181205689907\n",
            "iteration 453, loss is: 0.20603498816490173, w is: 0.8797715902328491, b is -0.0042028119787573814\n",
            "iteration 454, loss is: 0.20603324472904205, w is: 0.8798397183418274, b is -0.0041775950230658054\n",
            "iteration 455, loss is: 0.20603147149085999, w is: 0.879907488822937, b is -0.0041525294072926044\n",
            "iteration 456, loss is: 0.2060297429561615, w is: 0.8799748420715332, b is -0.004127614200115204\n",
            "iteration 457, loss is: 0.20602798461914062, w is: 0.880041778087616, b is -0.004102848470211029\n",
            "iteration 458, loss is: 0.2060263305902481, w is: 0.8801082968711853, b is -0.004078231286257505\n",
            "iteration 459, loss is: 0.2060246467590332, w is: 0.880174458026886, b is -0.004053761716932058\n",
            "iteration 460, loss is: 0.20602300763130188, w is: 0.8802402019500732, b is -0.0040294392965734005\n",
            "iteration 461, loss is: 0.20602136850357056, w is: 0.8803055286407471, b is -0.00400526262819767\n",
            "iteration 462, loss is: 0.20601975917816162, w is: 0.8803704977035522, b is -0.0039812312461435795\n",
            "iteration 463, loss is: 0.20601816475391388, w is: 0.880435049533844, b is -0.003957343753427267\n",
            "iteration 464, loss is: 0.20601657032966614, w is: 0.8804992437362671, b is -0.0039335996843874454\n",
            "iteration 465, loss is: 0.20601503551006317, w is: 0.8805630207061768, b is -0.00390999810770154\n",
            "iteration 466, loss is: 0.206013485789299, w is: 0.8806264400482178, b is -0.003886538092046976\n",
            "iteration 467, loss is: 0.20601198077201843, w is: 0.8806894421577454, b is -0.0038632189389318228\n",
            "iteration 468, loss is: 0.20601047575473785, w is: 0.8807520866394043, b is -0.0038400397170335054\n",
            "iteration 469, loss is: 0.20600898563861847, w is: 0.8808143734931946, b is -0.0038169994950294495\n",
            "iteration 470, loss is: 0.20600749552249908, w is: 0.8808762431144714, b is -0.003794097574427724\n",
            "iteration 471, loss is: 0.20600609481334686, w is: 0.8809377551078796, b is -0.003771333023905754\n",
            "iteration 472, loss is: 0.20600466430187225, w is: 0.8809989094734192, b is -0.003748705144971609\n",
            "iteration 473, loss is: 0.20600321888923645, w is: 0.8810597062110901, b is -0.0037262130063027143\n",
            "iteration 474, loss is: 0.20600183308124542, w is: 0.8811201453208923, b is -0.003703855676576495\n",
            "iteration 475, loss is: 0.2060004472732544, w is: 0.8811802268028259, b is -0.0036816326901316643\n",
            "iteration 476, loss is: 0.20599909126758575, w is: 0.8812399506568909, b is -0.0036595428828150034\n",
            "iteration 477, loss is: 0.2059977501630783, w is: 0.8812993168830872, b is -0.0036375857889652252\n",
            "iteration 478, loss is: 0.20599642395973206, w is: 0.8813583254814148, b is -0.0036157602444291115\n",
            "iteration 479, loss is: 0.205995112657547, w is: 0.8814169764518738, b is -0.003594065783545375\n",
            "iteration 480, loss is: 0.20599378645420074, w is: 0.8814752697944641, b is -0.0035725014749914408\n",
            "iteration 481, loss is: 0.20599251985549927, w is: 0.8815332055091858, b is -0.0035510663874447346\n",
            "iteration 482, loss is: 0.2059912383556366, w is: 0.8815907835960388, b is -0.003529760055243969\n",
            "iteration 483, loss is: 0.2059899866580963, w is: 0.8816480040550232, b is -0.0035085815470665693\n",
            "iteration 484, loss is: 0.20598876476287842, w is: 0.8817049264907837, b is -0.0034875301644206047\n",
            "iteration 485, loss is: 0.20598755776882172, w is: 0.8817614912986755, b is -0.0034666049759835005\n",
            "iteration 486, loss is: 0.20598633587360382, w is: 0.8818176984786987, b is -0.0034458052832633257\n",
            "iteration 487, loss is: 0.20598512887954712, w is: 0.8818735480308533, b is -0.0034251303877681494\n",
            "iteration 488, loss is: 0.205983966588974, w is: 0.8819290995597839, b is -0.0034045795910060406\n",
            "iteration 489, loss is: 0.20598280429840088, w is: 0.881984293460846, b is -0.0033841521944850683\n",
            "iteration 490, loss is: 0.20598162710666656, w is: 0.8820391893386841, b is -0.003363847266882658\n",
            "iteration 491, loss is: 0.20598050951957703, w is: 0.8820937275886536, b is -0.0033436641097068787\n",
            "iteration 492, loss is: 0.2059793770313263, w is: 0.8821479678153992, b is -0.0033236020244657993\n",
            "iteration 493, loss is: 0.20597827434539795, w is: 0.8822018504142761, b is -0.0033036605454981327\n",
            "iteration 494, loss is: 0.2059771716594696, w is: 0.8822554349899292, b is -0.0032838385086506605\n",
            "iteration 495, loss is: 0.20597606897354126, w is: 0.8823086619377136, b is -0.0032641354482620955\n",
            "iteration 496, loss is: 0.2059750109910965, w is: 0.8823615908622742, b is -0.0032445506658405066\n",
            "iteration 497, loss is: 0.20597395300865173, w is: 0.8824142217636108, b is -0.003225083230063319\n",
            "iteration 498, loss is: 0.20597290992736816, w is: 0.8824664950370789, b is -0.003205732675269246\n",
            "iteration 499, loss is: 0.20597189664840698, w is: 0.882518470287323, b is -0.0031864983029663563\n",
            "iteration 500, loss is: 0.20597083866596222, w is: 0.8825701475143433, b is -0.0031673794146627188\n",
            "iteration 501, loss is: 0.20596984028816223, w is: 0.8826215267181396, b is -0.003148375079035759\n",
            "iteration 502, loss is: 0.20596884191036224, w is: 0.8826726078987122, b is -0.0031294848304241896\n",
            "iteration 503, loss is: 0.20596785843372345, w is: 0.882723331451416, b is -0.0031107079703360796\n",
            "iteration 504, loss is: 0.20596688985824585, w is: 0.882773756980896, b is -0.0030920435674488544\n",
            "iteration 505, loss is: 0.20596590638160706, w is: 0.8828238844871521, b is -0.0030734913889318705\n",
            "iteration 506, loss is: 0.20596498250961304, w is: 0.8828737139701843, b is -0.003055050503462553\n",
            "iteration 507, loss is: 0.20596404373645782, w is: 0.8829232454299927, b is -0.003036720212548971\n",
            "iteration 508, loss is: 0.2059631049633026, w is: 0.8829724788665771, b is -0.003018499817699194\n",
            "iteration 509, loss is: 0.2059621959924698, w is: 0.8830214142799377, b is -0.003000388853251934\n",
            "iteration 510, loss is: 0.20596131682395935, w is: 0.8830700516700745, b is -0.0029823866207152605\n",
            "iteration 511, loss is: 0.20596040785312653, w is: 0.8831183910369873, b is -0.0029644924215972424\n",
            "iteration 512, loss is: 0.2059595137834549, w is: 0.883166491985321, b is -0.0029467055574059486\n",
            "iteration 513, loss is: 0.20595863461494446, w is: 0.8832142949104309, b is -0.0029290253296494484\n",
            "iteration 514, loss is: 0.20595777034759521, w is: 0.8832617998123169, b is -0.0029114512726664543\n",
            "iteration 515, loss is: 0.20595695078372955, w is: 0.883309006690979, b is -0.002893982455134392\n",
            "iteration 516, loss is: 0.2059560865163803, w is: 0.8833559155464172, b is -0.002876618644222617\n",
            "iteration 517, loss is: 0.20595523715019226, w is: 0.8834025859832764, b is -0.002859358908608556\n",
            "iteration 518, loss is: 0.2059544324874878, w is: 0.8834489583969116, b is -0.0028422027826309204\n",
            "iteration 519, loss is: 0.20595362782478333, w is: 0.883495032787323, b is -0.00282514956779778\n",
            "iteration 520, loss is: 0.20595280826091766, w is: 0.8835408687591553, b is -0.0028081985656172037\n",
            "iteration 521, loss is: 0.2059520184993744, w is: 0.8835864067077637, b is -0.002791349310427904\n",
            "iteration 522, loss is: 0.2059512436389923, w is: 0.8836316466331482, b is -0.0027746011037379503\n",
            "iteration 523, loss is: 0.20595046877861023, w is: 0.8836766481399536, b is -0.002757953479886055\n",
            "iteration 524, loss is: 0.20594969391822815, w is: 0.8837213516235352, b is -0.002741405740380287\n",
            "iteration 525, loss is: 0.20594894886016846, w is: 0.8837658166885376, b is -0.002724957186728716\n",
            "iteration 526, loss is: 0.20594818890094757, w is: 0.8838099837303162, b is -0.002708607353270054\n",
            "iteration 527, loss is: 0.20594744384288788, w is: 0.8838539123535156, b is -0.0026923557743430138\n",
            "iteration 528, loss is: 0.20594674348831177, w is: 0.8838975429534912, b is -0.0026762017514556646\n",
            "iteration 529, loss is: 0.20594601333141327, w is: 0.8839409351348877, b is -0.0026601445861160755\n",
            "iteration 530, loss is: 0.20594528317451477, w is: 0.8839840888977051, b is -0.002644183812662959\n",
            "iteration 531, loss is: 0.20594459772109985, w is: 0.8840269446372986, b is -0.0026283187326043844\n",
            "iteration 532, loss is: 0.20594389736652374, w is: 0.884069561958313, b is -0.002612548880279064\n",
            "iteration 533, loss is: 0.20594321191310883, w is: 0.8841119408607483, b is -0.0025968735571950674\n",
            "iteration 534, loss is: 0.2059425264596939, w is: 0.8841540813446045, b is -0.002581292297691107\n",
            "iteration 535, loss is: 0.20594187080860138, w is: 0.8841959238052368, b is -0.002565804636105895\n",
            "iteration 536, loss is: 0.20594118535518646, w is: 0.88423752784729, b is -0.0025504096411168575\n",
            "iteration 537, loss is: 0.20594052970409393, w is: 0.8842788934707642, b is -0.0025351070798933506\n",
            "iteration 538, loss is: 0.2059398889541626, w is: 0.8843200206756592, b is -0.002519896486774087\n",
            "iteration 539, loss is: 0.20593926310539246, w is: 0.8843609094619751, b is -0.0025047771632671356\n",
            "iteration 540, loss is: 0.20593863725662231, w is: 0.8844015002250671, b is -0.0024897484108805656\n",
            "iteration 541, loss is: 0.20593802630901337, w is: 0.8844418525695801, b is -0.0024748099967837334\n",
            "iteration 542, loss is: 0.20593738555908203, w is: 0.8844819664955139, b is -0.002459961222484708\n",
            "iteration 543, loss is: 0.20593680441379547, w is: 0.8845218420028687, b is -0.002445201389491558\n",
            "iteration 544, loss is: 0.20593619346618652, w is: 0.8845614790916443, b is -0.002430530032142997\n",
            "iteration 545, loss is: 0.20593559741973877, w is: 0.8846008777618408, b is -0.0024159469176083803\n",
            "iteration 546, loss is: 0.20593498647212982, w is: 0.8846400380134583, b is -0.0024014513473957777\n",
            "iteration 547, loss is: 0.20593443512916565, w is: 0.8846789598464966, b is -0.002387042623013258\n",
            "iteration 548, loss is: 0.2059338390827179, w is: 0.8847176432609558, b is -0.002372720278799534\n",
            "iteration 549, loss is: 0.20593330264091492, w is: 0.8847561478614807, b is -0.002358483849093318\n",
            "iteration 550, loss is: 0.20593270659446716, w is: 0.8847944140434265, b is -0.0023443331010639668\n",
            "iteration 551, loss is: 0.20593218505382538, w is: 0.8848324418067932, b is -0.0023302671033889055\n",
            "iteration 552, loss is: 0.2059316188097, w is: 0.8848702311515808, b is -0.002316285390406847\n",
            "iteration 553, loss is: 0.20593106746673584, w is: 0.8849077820777893, b is -0.0023023877292871475\n",
            "iteration 554, loss is: 0.20593054592609406, w is: 0.8849450945854187, b is -0.002288573421537876\n",
            "iteration 555, loss is: 0.20593002438545227, w is: 0.8849822282791138, b is -0.0022748420014977455\n",
            "iteration 556, loss is: 0.20592950284481049, w is: 0.8850191235542297, b is -0.0022611930035054684\n",
            "iteration 557, loss is: 0.2059289813041687, w is: 0.8850557804107666, b is -0.0022476257290691137\n",
            "iteration 558, loss is: 0.2059284746646881, w is: 0.8850921988487244, b is -0.002234139945358038\n",
            "iteration 559, loss is: 0.2059279978275299, w is: 0.8851284384727478, b is -0.00222073495388031\n",
            "iteration 560, loss is: 0.20592747628688812, w is: 0.8851644396781921, b is -0.0022074105218052864\n",
            "iteration 561, loss is: 0.20592699944972992, w is: 0.8852002620697021, b is -0.0021941661834716797\n",
            "iteration 562, loss is: 0.20592652261257172, w is: 0.8852358460426331, b is -0.002181001240387559\n",
            "iteration 563, loss is: 0.2059260457754135, w is: 0.8852711915969849, b is -0.002167915226891637\n",
            "iteration 564, loss is: 0.2059255689382553, w is: 0.8853063583374023, b is -0.002154907677322626\n",
            "iteration 565, loss is: 0.2059250921010971, w is: 0.8853412866592407, b is -0.002141978358849883\n",
            "iteration 566, loss is: 0.2059246301651001, w is: 0.8853760361671448, b is -0.002129126572981477\n",
            "iteration 567, loss is: 0.20592418313026428, w is: 0.8854105472564697, b is -0.00211635185405612\n",
            "iteration 568, loss is: 0.20592375099658966, w is: 0.8854448795318604, b is -0.002103653736412525\n",
            "iteration 569, loss is: 0.20592327415943146, w is: 0.8854789733886719, b is -0.0020910317543894053\n",
            "iteration 570, loss is: 0.20592285692691803, w is: 0.8855128884315491, b is -0.0020784856751561165\n",
            "iteration 571, loss is: 0.2059224247932434, w is: 0.8855465650558472, b is -0.002066014800220728\n",
            "iteration 572, loss is: 0.2059219777584076, w is: 0.8855800628662109, b is -0.0020536186639219522\n",
            "iteration 573, loss is: 0.20592156052589417, w is: 0.8856133818626404, b is -0.002041296800598502\n",
            "iteration 574, loss is: 0.20592115819454193, w is: 0.8856464624404907, b is -0.002029048977419734\n",
            "iteration 575, loss is: 0.20592069625854492, w is: 0.8856793642044067, b is -0.0020168747287243605\n",
            "iteration 576, loss is: 0.20592030882835388, w is: 0.8857120871543884, b is -0.0020047733560204506\n",
            "iteration 577, loss is: 0.20591992139816284, w is: 0.885744571685791, b is -0.0019927446264773607\n",
            "iteration 578, loss is: 0.2059195190668106, w is: 0.8857768774032593, b is -0.001980788307264447\n",
            "iteration 579, loss is: 0.20591911673545837, w is: 0.8858090043067932, b is -0.0019689034670591354\n",
            "iteration 580, loss is: 0.20591872930526733, w is: 0.8858409523963928, b is -0.0019570901058614254\n",
            "iteration 581, loss is: 0.2059183418750763, w is: 0.8858726620674133, b is -0.0019453475251793861\n",
            "iteration 582, loss is: 0.20591795444488525, w is: 0.8859041929244995, b is -0.0019336753757670522\n",
            "iteration 583, loss is: 0.2059175819158554, w is: 0.8859355449676514, b is -0.001922073308378458\n",
            "iteration 584, loss is: 0.20591722428798676, w is: 0.8859667181968689, b is -0.0019105408573523164\n",
            "iteration 585, loss is: 0.2059168517589569, w is: 0.8859977126121521, b is -0.00189907755702734\n",
            "iteration 586, loss is: 0.20591649413108826, w is: 0.886028528213501, b is -0.0018876830581575632\n",
            "iteration 587, loss is: 0.2059161365032196, w is: 0.8860591650009155, b is -0.001876356895081699\n",
            "iteration 588, loss is: 0.20591576397418976, w is: 0.886089563369751, b is -0.0018650987185537815\n",
            "iteration 589, loss is: 0.2059154212474823, w is: 0.8861197829246521, b is -0.0018539080629125237\n",
            "iteration 590, loss is: 0.20591507852077484, w is: 0.8861498236656189, b is -0.0018427845789119601\n",
            "iteration 591, loss is: 0.20591475069522858, w is: 0.8861796855926514, b is -0.0018317279173061252\n",
            "iteration 592, loss is: 0.20591440796852112, w is: 0.8862093687057495, b is -0.0018207374960184097\n",
            "iteration 593, loss is: 0.20591406524181366, w is: 0.8862388730049133, b is -0.0018098130822181702\n",
            "iteration 594, loss is: 0.2059137374162674, w is: 0.8862681984901428, b is -0.0017989542102441192\n",
            "iteration 595, loss is: 0.20591342449188232, w is: 0.886297345161438, b is -0.0017881605308502913\n",
            "iteration 596, loss is: 0.20591308176517487, w is: 0.8863263726234436, b is -0.0017774315783753991\n",
            "iteration 597, loss is: 0.2059127688407898, w is: 0.8863552212715149, b is -0.0017667670035734773\n",
            "iteration 598, loss is: 0.20591244101524353, w is: 0.8863838911056519, b is -0.0017561664571985602\n",
            "iteration 599, loss is: 0.20591215789318085, w is: 0.8864123821258545, b is -0.0017456293571740389\n",
            "iteration 600, loss is: 0.20591185986995697, w is: 0.8864406943321228, b is -0.0017351555870845914\n",
            "iteration 601, loss is: 0.2059115320444107, w is: 0.8864688277244568, b is -0.0017247445648536086\n",
            "iteration 602, loss is: 0.20591123402118683, w is: 0.8864967823028564, b is -0.0017143961740657687\n",
            "iteration 603, loss is: 0.20591095089912415, w is: 0.8865245580673218, b is -0.0017041098326444626\n",
            "iteration 604, loss is: 0.20591065287590027, w is: 0.8865522146224976, b is -0.0016938851913437247\n",
            "iteration 605, loss is: 0.2059103548526764, w is: 0.886579692363739, b is -0.0016837219009175897\n",
            "iteration 606, loss is: 0.20591005682945251, w is: 0.8866069912910461, b is -0.001673619612120092\n",
            "iteration 607, loss is: 0.20590980350971222, w is: 0.886634111404419, b is -0.0016635778592899442\n",
            "iteration 608, loss is: 0.20590952038764954, w is: 0.8866611123085022, b is -0.0016535964095965028\n",
            "iteration 609, loss is: 0.20590923726558685, w is: 0.8866879343986511, b is -0.0016436747973784804\n",
            "iteration 610, loss is: 0.20590896904468536, w is: 0.8867145776748657, b is -0.0016338127898052335\n",
            "iteration 611, loss is: 0.20590870082378387, w is: 0.8867411017417908, b is -0.0016240099212154746\n",
            "iteration 612, loss is: 0.20590844750404358, w is: 0.8867674469947815, b is -0.0016142658423632383\n",
            "iteration 613, loss is: 0.2059081494808197, w is: 0.8867936134338379, b is -0.0016045802040025592\n",
            "iteration 614, loss is: 0.2059078961610794, w is: 0.8868196606636047, b is -0.0015949527733027935\n",
            "iteration 615, loss is: 0.2059076428413391, w is: 0.8868455290794373, b is -0.001585383084602654\n",
            "iteration 616, loss is: 0.2059074193239212, w is: 0.8868712186813354, b is -0.0015758707886561751\n",
            "iteration 617, loss is: 0.2059071660041809, w is: 0.8868967890739441, b is -0.0015664155362173915\n",
            "iteration 618, loss is: 0.20590689778327942, w is: 0.8869221806526184, b is -0.0015570170944556594\n",
            "iteration 619, loss is: 0.2059066742658615, w is: 0.8869474530220032, b is -0.0015476749977096915\n",
            "iteration 620, loss is: 0.20590642094612122, w is: 0.8869725465774536, b is -0.0015383888967335224\n",
            "iteration 621, loss is: 0.20590616762638092, w is: 0.8869975209236145, b is -0.0015291585586965084\n",
            "iteration 622, loss is: 0.2059059590101242, w is: 0.8870223164558411, b is -0.0015199835179373622\n",
            "iteration 623, loss is: 0.2059057205915451, w is: 0.8870469927787781, b is -0.001510863658040762\n",
            "iteration 624, loss is: 0.205905482172966, w is: 0.8870714902877808, b is -0.0015017985133454204\n",
            "iteration 625, loss is: 0.2059052586555481, w is: 0.8870958685874939, b is -0.001492787734605372\n",
            "iteration 626, loss is: 0.2059050351381302, w is: 0.8871200680732727, b is -0.001483831088989973\n",
            "iteration 627, loss is: 0.20590482652187347, w is: 0.887144148349762, b is -0.0014749281108379364\n",
            "iteration 628, loss is: 0.20590460300445557, w is: 0.8871680498123169, b is -0.0014660785673186183\n",
            "iteration 629, loss is: 0.20590437948703766, w is: 0.8871918320655823, b is -0.0014572821091860533\n",
            "iteration 630, loss is: 0.20590417087078094, w is: 0.8872154355049133, b is -0.0014485383871942759\n",
            "iteration 631, loss is: 0.20590394735336304, w is: 0.8872389197349548, b is -0.0014398471685126424\n",
            "iteration 632, loss is: 0.20590373873710632, w is: 0.8872622847557068, b is -0.0014312081038951874\n",
            "iteration 633, loss is: 0.205903559923172, w is: 0.8872854709625244, b is -0.0014226208440959454\n",
            "iteration 634, loss is: 0.2059033215045929, w is: 0.8873085379600525, b is -0.0014140850398689508\n",
            "iteration 635, loss is: 0.20590314269065857, w is: 0.887331485748291, b is -0.001405600574798882\n",
            "iteration 636, loss is: 0.20590296387672424, w is: 0.8873542547225952, b is -0.0013971669832244515\n",
            "iteration 637, loss is: 0.20590271055698395, w is: 0.8873769044876099, b is -0.0013887840323150158\n",
            "iteration 638, loss is: 0.205902561545372, w is: 0.887399435043335, b is -0.0013804513728246093\n",
            "iteration 639, loss is: 0.2059023529291153, w is: 0.8874218463897705, b is -0.0013721686555072665\n",
            "iteration 640, loss is: 0.20590217411518097, w is: 0.8874440789222717, b is -0.0013639356475323439\n",
            "iteration 641, loss is: 0.20590198040008545, w is: 0.8874661922454834, b is -0.0013557519996538758\n",
            "iteration 642, loss is: 0.20590180158615112, w is: 0.8874881863594055, b is -0.0013476174790412188\n",
            "iteration 643, loss is: 0.2059016227722168, w is: 0.8875100612640381, b is -0.0013395317364484072\n",
            "iteration 644, loss is: 0.20590144395828247, w is: 0.8875317573547363, b is -0.0013314945390447974\n",
            "iteration 645, loss is: 0.20590123534202576, w is: 0.887553334236145, b is -0.001323505537584424\n",
            "iteration 646, loss is: 0.205901101231575, w is: 0.8875747919082642, b is -0.0013155644992366433\n",
            "iteration 647, loss is: 0.20590092241764069, w is: 0.8875961303710938, b is -0.0013076710747554898\n",
            "iteration 648, loss is: 0.20590075850486755, w is: 0.8876173496246338, b is -0.00129982503131032\n",
            "iteration 649, loss is: 0.20590054988861084, w is: 0.8876384496688843, b is -0.0012920261360704899\n",
            "iteration 650, loss is: 0.2059004008769989, w is: 0.8876594305038452, b is -0.0012842740397900343\n",
            "iteration 651, loss is: 0.20590023696422577, w is: 0.8876802325248718, b is -0.0012765683932229877\n",
            "iteration 652, loss is: 0.20590005815029144, w is: 0.8877009153366089, b is -0.0012689089635387063\n",
            "iteration 653, loss is: 0.2058998942375183, w is: 0.8877214789390564, b is -0.0012612955179065466\n",
            "iteration 654, loss is: 0.20589976012706757, w is: 0.8877419233322144, b is -0.001253727707080543\n",
            "iteration 655, loss is: 0.20589959621429443, w is: 0.8877622485160828, b is -0.001246205298230052\n",
            "iteration 656, loss is: 0.2058994174003601, w is: 0.8877824544906616, b is -0.0012387280585244298\n",
            "iteration 657, loss is: 0.20589928328990936, w is: 0.8878025412559509, b is -0.0012312957551330328\n",
            "iteration 658, loss is: 0.20589911937713623, w is: 0.8878225088119507, b is -0.0012239080388098955\n",
            "iteration 659, loss is: 0.2058989405632019, w is: 0.8878423571586609, b is -0.0012165645603090525\n",
            "iteration 660, loss is: 0.20589882135391235, w is: 0.8878620862960815, b is -0.00120926508679986\n",
            "iteration 661, loss is: 0.2058986872434616, w is: 0.8878816962242126, b is -0.0012020095018669963\n",
            "iteration 662, loss is: 0.20589852333068848, w is: 0.8879011869430542, b is -0.0011947974562644958\n",
            "iteration 663, loss is: 0.20589837431907654, w is: 0.8879205584526062, b is -0.001187628717161715\n",
            "iteration 664, loss is: 0.2058982402086258, w is: 0.8879398107528687, b is -0.0011805029353126884\n",
            "iteration 665, loss is: 0.20589809119701385, w is: 0.8879589438438416, b is -0.0011734198778867722\n",
            "iteration 666, loss is: 0.2058979570865631, w is: 0.8879779577255249, b is -0.0011663794284686446\n",
            "iteration 667, loss is: 0.20589782297611237, w is: 0.8879968523979187, b is -0.0011593812378123403\n",
            "iteration 668, loss is: 0.20589770376682281, w is: 0.888015627861023, b is -0.0011524249566718936\n",
            "iteration 669, loss is: 0.20589755475521088, w is: 0.8880343437194824, b is -0.0011455104686319828\n",
            "iteration 670, loss is: 0.20589742064476013, w is: 0.8880529403686523, b is -0.0011386374244466424\n",
            "iteration 671, loss is: 0.20589733123779297, w is: 0.8880714178085327, b is -0.0011318055912852287\n",
            "iteration 672, loss is: 0.20589718222618103, w is: 0.8880897760391235, b is -0.0011250147363170981\n",
            "iteration 673, loss is: 0.2058970332145691, w is: 0.8881080150604248, b is -0.001118264626711607\n",
            "iteration 674, loss is: 0.20589691400527954, w is: 0.8881261348724365, b is -0.0011115550296381116\n",
            "iteration 675, loss is: 0.20589679479599, w is: 0.8881441354751587, b is -0.0011048857122659683\n",
            "iteration 676, loss is: 0.20589666068553925, w is: 0.8881620764732361, b is -0.0010982564417645335\n",
            "iteration 677, loss is: 0.2058965563774109, w is: 0.8881798982620239, b is -0.0010916668688878417\n",
            "iteration 678, loss is: 0.20589643716812134, w is: 0.8881976008415222, b is -0.001085116877220571\n",
            "iteration 679, loss is: 0.2058963179588318, w is: 0.888215184211731, b is -0.0010786062339320779\n",
            "iteration 680, loss is: 0.20589618384838104, w is: 0.8882326483726501, b is -0.0010721345897763968\n",
            "iteration 681, loss is: 0.20589609444141388, w is: 0.8882500529289246, b is -0.001065701711922884\n",
            "iteration 682, loss is: 0.20589596033096313, w is: 0.8882673382759094, b is -0.0010593074839562178\n",
            "iteration 683, loss is: 0.20589584112167358, w is: 0.8882845044136047, b is -0.0010529516730457544\n",
            "iteration 684, loss is: 0.20589575171470642, w is: 0.8883015513420105, b is -0.0010466339299455285\n",
            "iteration 685, loss is: 0.20589561760425568, w is: 0.8883185386657715, b is -0.0010403540218248963\n",
            "iteration 686, loss is: 0.2058955281972885, w is: 0.8883354067802429, b is -0.001034111948683858\n",
            "iteration 687, loss is: 0.20589540898799896, w is: 0.8883521556854248, b is -0.001027907244861126\n",
            "iteration 688, loss is: 0.2058953046798706, w is: 0.8883688449859619, b is -0.0010217397939413786\n",
            "iteration 689, loss is: 0.20589518547058105, w is: 0.8883854150772095, b is -0.0010156093630939722\n",
            "iteration 690, loss is: 0.2058950960636139, w is: 0.8884018659591675, b is -0.0010095157194882631\n",
            "iteration 691, loss is: 0.20589500665664673, w is: 0.8884182572364807, b is -0.0010034586302936077\n",
            "iteration 692, loss is: 0.20589491724967957, w is: 0.8884345293045044, b is -0.0009974378626793623\n",
            "iteration 693, loss is: 0.20589479804039001, w is: 0.8884506821632385, b is -0.000991453300230205\n",
            "iteration 694, loss is: 0.20589467883110046, w is: 0.8884667754173279, b is -0.0009855045937001705\n",
            "iteration 695, loss is: 0.20589463412761688, w is: 0.8884827494621277, b is -0.000979591510258615\n",
            "iteration 696, loss is: 0.20589450001716614, w is: 0.8884986639022827, b is -0.0009737139334902167\n",
            "iteration 697, loss is: 0.20589441061019897, w is: 0.8885144591331482, b is -0.0009678716887719929\n",
            "iteration 698, loss is: 0.2058943212032318, w is: 0.8885301351547241, b is -0.000962064485065639\n",
            "iteration 699, loss is: 0.20589423179626465, w is: 0.8885457515716553, b is -0.0009562920895405114\n",
            "iteration 700, loss is: 0.20589414238929749, w is: 0.8885612487792969, b is -0.0009505543275736272\n",
            "iteration 701, loss is: 0.20589402318000793, w is: 0.8885766863822937, b is -0.000944850966334343\n",
            "iteration 702, loss is: 0.20589393377304077, w is: 0.888592004776001, b is -0.0009391818894073367\n",
            "iteration 703, loss is: 0.2058938443660736, w is: 0.8886072635650635, b is -0.0009335468639619648\n",
            "iteration 704, loss is: 0.20589376986026764, w is: 0.8886224031448364, b is -0.0009279455989599228\n",
            "iteration 705, loss is: 0.20589368045330048, w is: 0.8886374235153198, b is -0.0009223779779858887\n",
            "iteration 706, loss is: 0.2058935910463333, w is: 0.8886523842811584, b is -0.0009168437100015581\n",
            "iteration 707, loss is: 0.20589354634284973, w is: 0.8886672258377075, b is -0.0009113426203839481\n",
            "iteration 708, loss is: 0.205893412232399, w is: 0.8886820077896118, b is -0.000905874534510076\n",
            "iteration 709, loss is: 0.2058933675289154, w is: 0.8886966705322266, b is -0.0009004392777569592\n",
            "iteration 710, loss is: 0.20589324831962585, w is: 0.8887112736701965, b is -0.0008950366755016148\n",
            "iteration 711, loss is: 0.20589318871498108, w is: 0.8887258172035217, b is -0.0008896664949133992\n",
            "iteration 712, loss is: 0.2058931142091751, w is: 0.8887402415275574, b is -0.0008843285031616688\n",
            "iteration 713, loss is: 0.20589302480220795, w is: 0.8887546062469482, b is -0.0008790225256234407\n",
            "iteration 714, loss is: 0.20589296519756317, w is: 0.8887688517570496, b is -0.0008737483294680715\n",
            "iteration 715, loss is: 0.20589286088943481, w is: 0.8887830376625061, b is -0.0008685057982802391\n",
            "iteration 716, loss is: 0.20589280128479004, w is: 0.8887971043586731, b is -0.0008632947574369609\n",
            "iteration 717, loss is: 0.20589272677898407, w is: 0.8888111114501953, b is -0.0008581149158999324\n",
            "iteration 718, loss is: 0.2058926373720169, w is: 0.888824999332428, b is -0.0008529662736691535\n",
            "iteration 719, loss is: 0.20589257776737213, w is: 0.8888388276100159, b is -0.0008478484814986587\n",
            "iteration 720, loss is: 0.20589250326156616, w is: 0.888852596282959, b is -0.0008427613647654653\n",
            "iteration 721, loss is: 0.2058924287557602, w is: 0.8888662457466125, b is -0.0008377048070542514\n",
            "iteration 722, loss is: 0.20589236915111542, w is: 0.8888798356056213, b is -0.0008326786337420344\n",
            "iteration 723, loss is: 0.20589229464530945, w is: 0.8888933658599854, b is -0.0008276825537905097\n",
            "iteration 724, loss is: 0.20589223504066467, w is: 0.8889067769050598, b is -0.0008227164507843554\n",
            "iteration 725, loss is: 0.2058921456336975, w is: 0.8889201283454895, b is -0.0008177801501005888\n",
            "iteration 726, loss is: 0.20589210093021393, w is: 0.8889334201812744, b is -0.0008128734771162271\n",
            "iteration 727, loss is: 0.20589202642440796, w is: 0.8889465928077698, b is -0.0008079961990006268\n",
            "iteration 728, loss is: 0.205891951918602, w is: 0.8889597058296204, b is -0.0008031481993384659\n",
            "iteration 729, loss is: 0.20589189231395721, w is: 0.8889727592468262, b is -0.0007983293035067618\n",
            "iteration 730, loss is: 0.20589181780815125, w is: 0.8889856934547424, b is -0.0007935393368825316\n",
            "iteration 731, loss is: 0.20589175820350647, w is: 0.8889985680580139, b is -0.0007887781248427927\n",
            "iteration 732, loss is: 0.2058917135000229, w is: 0.8890113830566406, b is -0.0007840454345569015\n",
            "iteration 733, loss is: 0.20589163899421692, w is: 0.8890240788459778, b is -0.0007793411496095359\n",
            "iteration 734, loss is: 0.20589159429073334, w is: 0.8890367150306702, b is -0.0007746650953777134\n",
            "iteration 735, loss is: 0.20589150488376617, w is: 0.8890492916107178, b is -0.0007700171554461122\n",
            "iteration 736, loss is: 0.2058914750814438, w is: 0.8890617489814758, b is -0.0007653970387764275\n",
            "iteration 737, loss is: 0.20589138567447662, w is: 0.8890741467475891, b is -0.0007608046871609986\n",
            "iteration 738, loss is: 0.20589134097099304, w is: 0.8890864849090576, b is -0.0007562399259768426\n",
            "iteration 739, loss is: 0.20589128136634827, w is: 0.8890987634658813, b is -0.0007517024641856551\n",
            "iteration 740, loss is: 0.20589123666286469, w is: 0.8891109824180603, b is -0.0007471922435797751\n",
            "iteration 741, loss is: 0.2058911770582199, w is: 0.8891230821609497, b is -0.0007427090313285589\n",
            "iteration 742, loss is: 0.20589111745357513, w is: 0.8891351222991943, b is -0.0007382527692243457\n",
            "iteration 743, loss is: 0.20589104294776917, w is: 0.8891471028327942, b is -0.0007338232244364917\n",
            "iteration 744, loss is: 0.20589099824428558, w is: 0.8891590237617493, b is -0.0007294202223420143\n",
            "iteration 745, loss is: 0.205890953540802, w is: 0.8891708254814148, b is -0.0007250437047332525\n",
            "iteration 746, loss is: 0.20589090883731842, w is: 0.8891825675964355, b is -0.0007206934969872236\n",
            "iteration 747, loss is: 0.20589086413383484, w is: 0.8891942501068115, b is -0.000716369308065623\n",
            "iteration 748, loss is: 0.20589080452919006, w is: 0.8892058730125427, b is -0.0007120710797607899\n",
            "iteration 749, loss is: 0.2058907449245453, w is: 0.8892174363136292, b is -0.0007077986374497414\n",
            "iteration 750, loss is: 0.2058906853199005, w is: 0.8892289400100708, b is -0.0007035518647171557\n",
            "iteration 751, loss is: 0.20589064061641693, w is: 0.8892403244972229, b is -0.0006993305287323892\n",
            "iteration 752, loss is: 0.20589059591293335, w is: 0.8892516493797302, b is -0.000695134571287781\n",
            "iteration 753, loss is: 0.20589055120944977, w is: 0.8892629146575928, b is -0.0006909638177603483\n",
            "iteration 754, loss is: 0.2058904767036438, w is: 0.8892741203308105, b is -0.0006868180935271084\n",
            "iteration 755, loss is: 0.2058904618024826, w is: 0.8892852663993835, b is -0.0006826971657574177\n",
            "iteration 756, loss is: 0.20589038729667664, w is: 0.8892963528633118, b is -0.0006786009180359542\n",
            "iteration 757, loss is: 0.20589037239551544, w is: 0.8893073797225952, b is -0.0006745292921550572\n",
            "iteration 758, loss is: 0.20589029788970947, w is: 0.8893183469772339, b is -0.0006704821716994047\n",
            "iteration 759, loss is: 0.2058902531862259, w is: 0.889329195022583, b is -0.0006664593238383532\n",
            "iteration 760, loss is: 0.2058902084827423, w is: 0.8893399834632874, b is -0.0006624605739489198\n",
            "iteration 761, loss is: 0.20589017868041992, w is: 0.8893507122993469, b is -0.0006584858056157827\n",
            "iteration 762, loss is: 0.20589013397693634, w is: 0.8893613815307617, b is -0.0006545348442159593\n",
            "iteration 763, loss is: 0.20589008927345276, w is: 0.8893719911575317, b is -0.0006506076315417886\n",
            "iteration 764, loss is: 0.20589004456996918, w is: 0.889382541179657, b is -0.0006467039929702878\n",
            "iteration 765, loss is: 0.2058899998664856, w is: 0.8893930315971375, b is -0.0006428237538784742\n",
            "iteration 766, loss is: 0.20588995516300201, w is: 0.8894034624099731, b is -0.000638966856058687\n",
            "iteration 767, loss is: 0.20588991045951843, w is: 0.8894138336181641, b is -0.0006351330666802824\n",
            "iteration 768, loss is: 0.20588988065719604, w is: 0.8894241452217102, b is -0.0006313222111202776\n",
            "iteration 769, loss is: 0.20588986575603485, w is: 0.8894343972206116, b is -0.0006275342311710119\n",
            "iteration 770, loss is: 0.20588979125022888, w is: 0.8894445896148682, b is -0.0006237690686248243\n",
            "iteration 771, loss is: 0.2058897763490677, w is: 0.88945472240448, b is -0.0006200264324434102\n",
            "iteration 772, loss is: 0.2058897316455841, w is: 0.889464795589447, b is -0.0006163062644191086\n",
            "iteration 773, loss is: 0.20588968694210052, w is: 0.8894748091697693, b is -0.000612608389928937\n",
            "iteration 774, loss is: 0.20588964223861694, w is: 0.8894847631454468, b is -0.0006089327507652342\n",
            "iteration 775, loss is: 0.20588961243629456, w is: 0.8894946575164795, b is -0.0006052792305126786\n",
            "iteration 776, loss is: 0.20588956773281097, w is: 0.8895044922828674, b is -0.0006016475381329656\n",
            "iteration 777, loss is: 0.2058895230293274, w is: 0.8895142674446106, b is -0.0005980376736260951\n",
            "iteration 778, loss is: 0.2058895081281662, w is: 0.889523983001709, b is -0.0005944494623690844\n",
            "iteration 779, loss is: 0.20588944852352142, w is: 0.8895336389541626, b is -0.0005908827879466116\n",
            "iteration 780, loss is: 0.20588943362236023, w is: 0.8895432353019714, b is -0.0005873374757356942\n",
            "iteration 781, loss is: 0.20588938891887665, w is: 0.8895527720451355, b is -0.000583813467528671\n",
            "iteration 782, loss is: 0.20588935911655426, w is: 0.8895622491836548, b is -0.0005803105887025595\n",
            "iteration 783, loss is: 0.20588931441307068, w is: 0.8895716667175293, b is -0.0005768287228420377\n",
            "iteration 784, loss is: 0.20588929951190948, w is: 0.889581024646759, b is -0.0005733676953241229\n",
            "iteration 785, loss is: 0.2058892697095871, w is: 0.889590322971344, b is -0.0005699273897334933\n",
            "iteration 786, loss is: 0.20588921010494232, w is: 0.8895995616912842, b is -0.0005665078060701489\n",
            "iteration 787, loss is: 0.20588921010494232, w is: 0.8896087408065796, b is -0.0005631088279187679\n",
            "iteration 788, loss is: 0.20588918030261993, w is: 0.8896178603172302, b is -0.0005597301642410457\n",
            "iteration 789, loss is: 0.20588913559913635, w is: 0.8896269202232361, b is -0.0005563718150369823\n",
            "iteration 790, loss is: 0.20588912069797516, w is: 0.8896359205245972, b is -0.0005530336056835949\n",
            "iteration 791, loss is: 0.20588909089565277, w is: 0.8896448612213135, b is -0.0005497154197655618\n",
            "iteration 792, loss is: 0.2058890461921692, w is: 0.8896538019180298, b is -0.0005464171408675611\n",
            "iteration 793, loss is: 0.205889031291008, w is: 0.8896626830101013, b is -0.000543138652574271\n",
            "iteration 794, loss is: 0.20588897168636322, w is: 0.8896715044975281, b is -0.0005398798384703696\n",
            "iteration 795, loss is: 0.20588895678520203, w is: 0.8896802663803101, b is -0.0005366405821405351\n",
            "iteration 796, loss is: 0.20588891208171844, w is: 0.8896889686584473, b is -0.0005334207089617848\n",
            "iteration 797, loss is: 0.20588888227939606, w is: 0.8896976113319397, b is -0.0005302201607264578\n",
            "iteration 798, loss is: 0.20588886737823486, w is: 0.8897061944007874, b is -0.0005270387628115714\n",
            "iteration 799, loss is: 0.20588883757591248, w is: 0.8897147178649902, b is -0.0005238765734247863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        # d. pytorch modules; one-layer model\n",
        "        d = 1 # only one regressor.\n",
        "        model = one_layer(d,1) # input and output dimensions\n",
        "        optim = torch.optim.SGD(model.parameters(),lr=3e-3)\n",
        "\n",
        "        for t in range(niter):\n",
        "            y_pred = model(xT) # it can be any model!\n",
        "            loss = torch.nn.functional.mse_loss(y_pred, y)\n",
        "            loss.backward() # run backpropagation\n",
        "\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            print(f\"iteration {t}, loss is: {loss}, w is: {float(model.w)}, b is {float(model.b)}\")\n",
        "\n",
        "            #pred = model(xT)\n",
        "\n",
        "        pred = model(xT)\n",
        "        pred_runoff = (pred * runoff_mean.std()+ runoff_mean.mean()).detach().numpy() # detach() so it can be plotted\n",
        "\n",
        "        plt.plot(precip_mean, pred_runoff,  '-r')\n",
        "\n",
        "        ax.legend(['observations','model'])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#Add more predictors into the equation! Can you modify my code, update options a (by hand gradient calculation) and d (nn.module), to accept more predictors?\n",
        "#Add lasso regularization! 𝐲=𝐱^𝐓 𝐰^𝐓+𝛼∑abs(w)\n",
        "#Split the dataset into train and test, evaluate metrics on the test dataset!\n",
        "#See how does the parameter 𝛼 influence the test results?\n",
        "#Hints:\n",
        "#fields = ['pet_mean', 'p_seasonality', 'frac_snow', 'aridity', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest']\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import r2_score \n",
        "# segway to the LSTM notebook: https://bit.ly/3Fvnwyp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tjRKPd_nHdmi",
        "outputId": "6770d7ba-a59f-4ba7-fa12-85d1c9292735"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0, loss is: 1.6804213523864746, w is: -0.2700542211532593, b is 0.32945963740348816\n",
            "iteration 1, loss is: 1.6627799272537231, w is: -0.26308712363243103, b is 0.3274828791618347\n",
            "iteration 2, loss is: 1.6453497409820557, w is: -0.25616180896759033, b is 0.3255179822444916\n",
            "iteration 3, loss is: 1.628127932548523, w is: -0.24927805364131927, b is 0.32356488704681396\n",
            "iteration 4, loss is: 1.6111122369766235, w is: -0.24243560433387756, b is 0.3216235041618347\n",
            "iteration 5, loss is: 1.5943001508712769, w is: -0.2356342077255249, b is 0.31969377398490906\n",
            "iteration 6, loss is: 1.577689290046692, w is: -0.2288736253976822, b is 0.3177756071090698\n",
            "iteration 7, loss is: 1.561276912689209, w is: -0.22215360403060913, b is 0.31586894392967224\n",
            "iteration 8, loss is: 1.5450612306594849, w is: -0.21547390520572662, b is 0.31397372484207153\n",
            "iteration 9, loss is: 1.5290392637252808, w is: -0.20883427560329437, b is 0.3120898902416229\n",
            "iteration 10, loss is: 1.5132089853286743, w is: -0.20223449170589447, b is 0.31021735072135925\n",
            "iteration 11, loss is: 1.4975682497024536, w is: -0.19567430019378662, b is 0.30835604667663574\n",
            "iteration 12, loss is: 1.4821144342422485, w is: -0.18915347754955292, b is 0.3065059185028076\n",
            "iteration 13, loss is: 1.4668456315994263, w is: -0.18267177045345306, b is 0.3046668767929077\n",
            "iteration 14, loss is: 1.4517594575881958, w is: -0.17622895538806915, b is 0.30283886194229126\n",
            "iteration 15, loss is: 1.4368540048599243, w is: -0.16982479393482208, b is 0.3010218143463135\n",
            "iteration 16, loss is: 1.4221266508102417, w is: -0.16345906257629395, b is 0.2992156744003296\n",
            "iteration 17, loss is: 1.4075756072998047, w is: -0.15713152289390564, b is 0.2974203824996948\n",
            "iteration 18, loss is: 1.3931986093521118, w is: -0.15084195137023926, b is 0.295635849237442\n",
            "iteration 19, loss is: 1.3789936304092407, w is: -0.1445901244878769, b is 0.2938620448112488\n",
            "iteration 20, loss is: 1.3649585247039795, w is: -0.13837580382823944, b is 0.29209887981414795\n",
            "iteration 21, loss is: 1.3510913848876953, w is: -0.132198765873909, b is 0.29034629464149475\n",
            "iteration 22, loss is: 1.3373901844024658, w is: -0.12605878710746765, b is 0.2886042296886444\n",
            "iteration 23, loss is: 1.323852777481079, w is: -0.1199556514620781, b is 0.28687259554862976\n",
            "iteration 24, loss is: 1.310477614402771, w is: -0.11388913542032242, b is 0.2851513624191284\n",
            "iteration 25, loss is: 1.2972620725631714, w is: -0.10785901546478271, b is 0.2834404408931732\n",
            "iteration 26, loss is: 1.2842049598693848, w is: -0.10186508297920227, b is 0.2817398011684418\n",
            "iteration 27, loss is: 1.2713040113449097, w is: -0.09590710699558258, b is 0.2800493538379669\n",
            "iteration 28, loss is: 1.2585573196411133, w is: -0.08998487889766693, b is 0.27836906909942627\n",
            "iteration 29, loss is: 1.2459630966186523, w is: -0.08409818261861801, b is 0.27669885754585266\n",
            "iteration 30, loss is: 1.2335195541381836, w is: -0.07824680954217911, b is 0.2750386595726013\n",
            "iteration 31, loss is: 1.2212250232696533, w is: -0.07243054360151291, b is 0.27338841557502747\n",
            "iteration 32, loss is: 1.2090774774551392, w is: -0.0666491761803627, b is 0.2717480957508087\n",
            "iteration 33, loss is: 1.197075366973877, w is: -0.06090249866247177, b is 0.2701176106929779\n",
            "iteration 34, loss is: 1.1852167844772339, w is: -0.055190298706293106, b is 0.26849690079689026\n",
            "iteration 35, loss is: 1.1735000610351562, w is: -0.04951237514615059, b is 0.266885906457901\n",
            "iteration 36, loss is: 1.1619235277175903, w is: -0.04386851564049721, b is 0.26528459787368774\n",
            "iteration 37, loss is: 1.150485634803772, w is: -0.038258522748947144, b is 0.2636928856372833\n",
            "iteration 38, loss is: 1.1391843557357788, w is: -0.03268218785524368, b is 0.26211073994636536\n",
            "iteration 39, loss is: 1.1280182600021362, w is: -0.027139311656355858, b is 0.2605380713939667\n",
            "iteration 40, loss is: 1.1169859170913696, w is: -0.02162969298660755, b is 0.2589748501777649\n",
            "iteration 41, loss is: 1.1060854196548462, w is: -0.016153134405612946, b is 0.25742098689079285\n",
            "iteration 42, loss is: 1.0953155755996704, w is: -0.010709431953728199, b is 0.25587645173072815\n",
            "iteration 43, loss is: 1.0846744775772095, w is: -0.005298393778502941, b is 0.254341185092926\n",
            "iteration 44, loss is: 1.0741605758666992, w is: 8.017884829314426e-05, b is 0.2528151273727417\n",
            "iteration 45, loss is: 1.0637725591659546, w is: 0.00542647996917367, b is 0.2512982487678528\n",
            "iteration 46, loss is: 1.0535088777542114, w is: 0.010740705765783787, b is 0.24979045987129211\n",
            "iteration 47, loss is: 1.0433679819107056, w is: 0.016023043543100357, b is 0.2482917159795761\n",
            "iteration 48, loss is: 1.0333483219146729, w is: 0.021273689344525337, b is 0.2468019723892212\n",
            "iteration 49, loss is: 1.0234484672546387, w is: 0.026492830365896225, b is 0.2453211545944214\n",
            "iteration 50, loss is: 1.0136672258377075, w is: 0.03168065473437309, b is 0.2438492327928543\n",
            "iteration 51, loss is: 1.0040029287338257, w is: 0.03683735430240631, b is 0.242386132478714\n",
            "iteration 52, loss is: 0.9944542646408081, w is: 0.041963111609220505, b is 0.24093180894851685\n",
            "iteration 53, loss is: 0.9850198030471802, w is: 0.047058116644620895, b is 0.2394862174987793\n",
            "iteration 54, loss is: 0.9756983518600464, w is: 0.05212254822254181, b is 0.23804929852485657\n",
            "iteration 55, loss is: 0.966488242149353, w is: 0.05715659633278847, b is 0.23662100732326508\n",
            "iteration 56, loss is: 0.9573883414268494, w is: 0.0621604397892952, b is 0.23520128428936005\n",
            "iteration 57, loss is: 0.9483974575996399, w is: 0.06713426113128662, b is 0.2337900698184967\n",
            "iteration 58, loss is: 0.9395139813423157, w is: 0.07207823544740677, b is 0.23238733410835266\n",
            "iteration 59, loss is: 0.9307370185852051, w is: 0.07699254900217056, b is 0.23099300265312195\n",
            "iteration 60, loss is: 0.9220648407936096, w is: 0.08187737315893173, b is 0.22960704565048218\n",
            "iteration 61, loss is: 0.9134964346885681, w is: 0.0867328941822052, b is 0.22822940349578857\n",
            "iteration 62, loss is: 0.9050306677818298, w is: 0.0915592759847641, b is 0.22686003148555756\n",
            "iteration 63, loss is: 0.8966660499572754, w is: 0.09635670483112335, b is 0.22549887001514435\n",
            "iteration 64, loss is: 0.8884015083312988, w is: 0.10112534463405609, b is 0.22414587438106537\n",
            "iteration 65, loss is: 0.8802359104156494, w is: 0.10586537420749664, b is 0.22280099987983704\n",
            "iteration 66, loss is: 0.8721680045127869, w is: 0.11057696491479874, b is 0.22146418690681458\n",
            "iteration 67, loss is: 0.8641966581344604, w is: 0.1152602881193161, b is 0.2201354056596756\n",
            "iteration 68, loss is: 0.8563206791877747, w is: 0.11991550773382187, b is 0.21881459653377533\n",
            "iteration 69, loss is: 0.848538875579834, w is: 0.12454279512166977, b is 0.2175017148256302\n",
            "iteration 70, loss is: 0.8408501744270325, w is: 0.12914232909679413, b is 0.2161967009305954\n",
            "iteration 71, loss is: 0.8332533836364746, w is: 0.13371425867080688, b is 0.21489952504634857\n",
            "iteration 72, loss is: 0.8257476091384888, w is: 0.13825875520706177, b is 0.21361012756824493\n",
            "iteration 73, loss is: 0.8183316588401794, w is: 0.1427759826183319, b is 0.2123284637928009\n",
            "iteration 74, loss is: 0.8110043406486511, w is: 0.14726610481739044, b is 0.2110544890165329\n",
            "iteration 75, loss is: 0.8037645816802979, w is: 0.1517292857170105, b is 0.20978815853595734\n",
            "iteration 76, loss is: 0.7966115474700928, w is: 0.1561656892299652, b is 0.20852942764759064\n",
            "iteration 77, loss is: 0.7895442247390747, w is: 0.1605754792690277, b is 0.20727825164794922\n",
            "iteration 78, loss is: 0.7825613021850586, w is: 0.16495880484580994, b is 0.2060345858335495\n",
            "iteration 79, loss is: 0.7756620645523071, w is: 0.16931582987308502, b is 0.2047983855009079\n",
            "iteration 80, loss is: 0.7688452005386353, w is: 0.1736467182636261, b is 0.20356959104537964\n",
            "iteration 81, loss is: 0.7621099948883057, w is: 0.1779516190290451, b is 0.20234817266464233\n",
            "iteration 82, loss is: 0.7554553151130676, w is: 0.18223069608211517, b is 0.2011340856552124\n",
            "iteration 83, loss is: 0.7488803267478943, w is: 0.18648409843444824, b is 0.19992728531360626\n",
            "iteration 84, loss is: 0.7423839569091797, w is: 0.19071197509765625, b is 0.19872772693634033\n",
            "iteration 85, loss is: 0.7359652519226074, w is: 0.19491448998451233, b is 0.19753536581993103\n",
            "iteration 86, loss is: 0.7296233773231506, w is: 0.19909179210662842, b is 0.19635015726089478\n",
            "iteration 87, loss is: 0.7233574390411377, w is: 0.20324403047561646, b is 0.19517205655574799\n",
            "iteration 88, loss is: 0.7171663641929626, w is: 0.20737135410308838, b is 0.19400101900100708\n",
            "iteration 89, loss is: 0.711049497127533, w is: 0.21147391200065613, b is 0.19283701479434967\n",
            "iteration 90, loss is: 0.7050056457519531, w is: 0.21555185317993164, b is 0.19167999923229218\n",
            "iteration 91, loss is: 0.6990342140197754, w is: 0.21960532665252686, b is 0.19052991271018982\n",
            "iteration 92, loss is: 0.6931341290473938, w is: 0.2236344814300537, b is 0.1893867403268814\n",
            "iteration 93, loss is: 0.6873047351837158, w is: 0.22763945162296295, b is 0.18825042247772217\n",
            "iteration 94, loss is: 0.6815449595451355, w is: 0.2316204011440277, b is 0.18712091445922852\n",
            "iteration 95, loss is: 0.6758542656898499, w is: 0.23557746410369873, b is 0.18599818646907806\n",
            "iteration 96, loss is: 0.6702315807342529, w is: 0.23951078951358795, b is 0.18488219380378723\n",
            "iteration 97, loss is: 0.6646761298179626, w is: 0.24342051148414612, b is 0.18377290666103363\n",
            "iteration 98, loss is: 0.6591871976852417, w is: 0.24730676412582397, b is 0.18267026543617249\n",
            "iteration 99, loss is: 0.6537638902664185, w is: 0.25116971135139465, b is 0.1815742403268814\n",
            "iteration 100, loss is: 0.6484054923057556, w is: 0.2550094723701477, b is 0.180484801530838\n",
            "iteration 101, loss is: 0.6431111693382263, w is: 0.25882619619369507, b is 0.17940188944339752\n",
            "iteration 102, loss is: 0.637880265712738, w is: 0.2626200318336487, b is 0.17832547426223755\n",
            "iteration 103, loss is: 0.6327118873596191, w is: 0.2663910984992981, b is 0.1772555261850357\n",
            "iteration 104, loss is: 0.6276053190231323, w is: 0.27013954520225525, b is 0.17619198560714722\n",
            "iteration 105, loss is: 0.6225598454475403, w is: 0.2738654911518097, b is 0.1751348376274109\n",
            "iteration 106, loss is: 0.6175747513771057, w is: 0.27756908535957336, b is 0.17408402264118195\n",
            "iteration 107, loss is: 0.6126493215560913, w is: 0.2812504470348358, b is 0.1730395257472992\n",
            "iteration 108, loss is: 0.607782781124115, w is: 0.284909725189209, b is 0.17200128734111786\n",
            "iteration 109, loss is: 0.6029744744300842, w is: 0.2885470390319824, b is 0.17096927762031555\n",
            "iteration 110, loss is: 0.5982237458229065, w is: 0.29216253757476807, b is 0.16994346678256989\n",
            "iteration 111, loss is: 0.5935298800468445, w is: 0.29575634002685547, b is 0.16892381012439728\n",
            "iteration 112, loss is: 0.5888920426368713, w is: 0.29932859539985657, b is 0.16791026294231415\n",
            "iteration 113, loss is: 0.5843098163604736, w is: 0.3028793931007385, b is 0.1669027954339981\n",
            "iteration 114, loss is: 0.5797823667526245, w is: 0.30640891194343567, b is 0.16590137779712677\n",
            "iteration 115, loss is: 0.5753090381622314, w is: 0.30991724133491516, b is 0.16490596532821655\n",
            "iteration 116, loss is: 0.5708892941474915, w is: 0.31340453028678894, b is 0.16391652822494507\n",
            "iteration 117, loss is: 0.5665223598480225, w is: 0.31687089800834656, b is 0.16293302178382874\n",
            "iteration 118, loss is: 0.5622076988220215, w is: 0.32031646370887756, b is 0.16195541620254517\n",
            "iteration 119, loss is: 0.557944655418396, w is: 0.3237413465976715, b is 0.16098368167877197\n",
            "iteration 120, loss is: 0.553732693195343, w is: 0.32714566588401794, b is 0.16001777350902557\n",
            "iteration 121, loss is: 0.5495710372924805, w is: 0.3305295705795288, b is 0.15905766189098358\n",
            "iteration 122, loss is: 0.5454592108726501, w is: 0.33389317989349365, b is 0.1581033170223236\n",
            "iteration 123, loss is: 0.5413965582847595, w is: 0.337236613035202, b is 0.15715469419956207\n",
            "iteration 124, loss is: 0.5373825430870056, w is: 0.3405599892139435, b is 0.1562117636203766\n",
            "iteration 125, loss is: 0.5334164500236511, w is: 0.3438633978366852, b is 0.15527449548244476\n",
            "iteration 126, loss is: 0.5294979214668274, w is: 0.34714698791503906, b is 0.15434284508228302\n",
            "iteration 127, loss is: 0.5256262421607971, w is: 0.3504108786582947, b is 0.15341678261756897\n",
            "iteration 128, loss is: 0.5218009352684021, w is: 0.3536551892757416, b is 0.15249627828598022\n",
            "iteration 129, loss is: 0.518021285533905, w is: 0.3568800389766693, b is 0.1515813022851944\n",
            "iteration 130, loss is: 0.5142869353294373, w is: 0.36008554697036743, b is 0.1506718099117279\n",
            "iteration 131, loss is: 0.5105971693992615, w is: 0.3632718026638031, b is 0.14976778626441956\n",
            "iteration 132, loss is: 0.5069516897201538, w is: 0.36643895506858826, b is 0.14886918663978577\n",
            "iteration 133, loss is: 0.5033497214317322, w is: 0.36958709359169006, b is 0.14797596633434296\n",
            "iteration 134, loss is: 0.4997909367084503, w is: 0.37271636724472046, b is 0.14708811044692993\n",
            "iteration 135, loss is: 0.49627459049224854, w is: 0.3758268654346466, b is 0.1462055891752243\n",
            "iteration 136, loss is: 0.49280038475990295, w is: 0.37891867756843567, b is 0.1453283578157425\n",
            "iteration 137, loss is: 0.4893677532672882, w is: 0.3819919526576996, b is 0.1444563865661621\n",
            "iteration 138, loss is: 0.4859761893749237, w is: 0.3850467801094055, b is 0.14358964562416077\n",
            "iteration 139, loss is: 0.48262518644332886, w is: 0.388083279132843, b is 0.14272810518741608\n",
            "iteration 140, loss is: 0.47931429743766785, w is: 0.39110156893730164, b is 0.14187173545360565\n",
            "iteration 141, loss is: 0.4760430157184601, w is: 0.39410173892974854, b is 0.1410205066204071\n",
            "iteration 142, loss is: 0.4728108048439026, w is: 0.39708390831947327, b is 0.14017438888549805\n",
            "iteration 143, loss is: 0.4696173667907715, w is: 0.4000481963157654, b is 0.1393333375453949\n",
            "iteration 144, loss is: 0.46646204590797424, w is: 0.40299469232559204, b is 0.13849733769893646\n",
            "iteration 145, loss is: 0.4633445739746094, w is: 0.4059235155582428, b is 0.13766635954380035\n",
            "iteration 146, loss is: 0.46026432514190674, w is: 0.4088347554206848, b is 0.136840358376503\n",
            "iteration 147, loss is: 0.4572209119796753, w is: 0.41172853112220764, b is 0.13601931929588318\n",
            "iteration 148, loss is: 0.4542139768600464, w is: 0.41460493206977844, b is 0.13520319759845734\n",
            "iteration 149, loss is: 0.45124301314353943, w is: 0.41746407747268677, b is 0.13439197838306427\n",
            "iteration 150, loss is: 0.44830748438835144, w is: 0.42030608654022217, b is 0.1335856318473816\n",
            "iteration 151, loss is: 0.4454071819782257, w is: 0.4231310188770294, b is 0.13278411328792572\n",
            "iteration 152, loss is: 0.44254159927368164, w is: 0.42593902349472046, b is 0.13198740780353546\n",
            "iteration 153, loss is: 0.43971025943756104, w is: 0.42873015999794006, b is 0.13119548559188843\n",
            "iteration 154, loss is: 0.43691280484199524, w is: 0.4315045475959778, b is 0.13040831685066223\n",
            "iteration 155, loss is: 0.43414878845214844, w is: 0.43426230549812317, b is 0.12962587177753448\n",
            "iteration 156, loss is: 0.431417852640152, w is: 0.4370035231113434, b is 0.1288481205701828\n",
            "iteration 157, loss is: 0.4287196099758148, w is: 0.4397282898426056, b is 0.1280750334262848\n",
            "iteration 158, loss is: 0.42605364322662354, w is: 0.44243669509887695, b is 0.12730658054351807\n",
            "iteration 159, loss is: 0.4234195649623871, w is: 0.445128858089447, b is 0.12654274702072144\n",
            "iteration 160, loss is: 0.4208170175552368, w is: 0.44780486822128296, b is 0.12578348815441132\n",
            "iteration 161, loss is: 0.4182455837726593, w is: 0.45046481490135193, b is 0.1250287890434265\n",
            "iteration 162, loss is: 0.4157049059867859, w is: 0.4531088173389435, b is 0.12427861988544464\n",
            "iteration 163, loss is: 0.4131946265697479, w is: 0.4557369351387024, b is 0.12353295087814331\n",
            "iteration 164, loss is: 0.41071444749832153, w is: 0.4583492875099182, b is 0.12279175221920013\n",
            "iteration 165, loss is: 0.40826383233070374, w is: 0.4609459638595581, b is 0.12205500155687332\n",
            "iteration 166, loss is: 0.4058426320552826, w is: 0.4635270833969116, b is 0.12132266908884048\n",
            "iteration 167, loss is: 0.4034503400325775, w is: 0.46609270572662354, b is 0.12059473246335983\n",
            "iteration 168, loss is: 0.4010867178440094, w is: 0.468642920255661, b is 0.11987116187810898\n",
            "iteration 169, loss is: 0.3987513482570648, w is: 0.4711778461933136, b is 0.11915193498134613\n",
            "iteration 170, loss is: 0.3964439034461975, w is: 0.47369757294654846, b is 0.1184370219707489\n",
            "iteration 171, loss is: 0.3941640257835388, w is: 0.4762021601200104, b is 0.1177264004945755\n",
            "iteration 172, loss is: 0.3919115662574768, w is: 0.4786917269229889, b is 0.11702004075050354\n",
            "iteration 173, loss is: 0.38968589901924133, w is: 0.48116636276245117, b is 0.11631792038679123\n",
            "iteration 174, loss is: 0.38748690485954285, w is: 0.4836261570453644, b is 0.11562000960111618\n",
            "iteration 175, loss is: 0.38531431555747986, w is: 0.4860711693763733, b is 0.1149262860417366\n",
            "iteration 176, loss is: 0.3831676244735718, w is: 0.48850151896476746, b is 0.1142367273569107\n",
            "iteration 177, loss is: 0.3810465633869171, w is: 0.49091729521751404, b is 0.1135513037443161\n",
            "iteration 178, loss is: 0.37895098328590393, w is: 0.4933185875415802, b is 0.112869992852211\n",
            "iteration 179, loss is: 0.3768804371356964, w is: 0.4957054555416107, b is 0.11219277232885361\n",
            "iteration 180, loss is: 0.37483468651771545, w is: 0.49807801842689514, b is 0.11151961237192154\n",
            "iteration 181, loss is: 0.3728133738040924, w is: 0.5004363059997559, b is 0.1108504980802536\n",
            "iteration 182, loss is: 0.37081629037857056, w is: 0.5027804970741272, b is 0.11018539220094681\n",
            "iteration 183, loss is: 0.36884304881095886, w is: 0.5051106214523315, b is 0.10952427983283997\n",
            "iteration 184, loss is: 0.3668934106826782, w is: 0.5074267387390137, b is 0.10886713117361069\n",
            "iteration 185, loss is: 0.36496710777282715, w is: 0.5097289681434631, b is 0.10821393132209778\n",
            "iteration 186, loss is: 0.36306387186050415, w is: 0.5120173692703247, b is 0.10756465047597885\n",
            "iteration 187, loss is: 0.36118340492248535, w is: 0.5142920613288879, b is 0.10691926628351212\n",
            "iteration 188, loss is: 0.35932543873786926, w is: 0.5165531039237976, b is 0.10627774894237518\n",
            "iteration 189, loss is: 0.357489675283432, w is: 0.5188005566596985, b is 0.10564008355140686\n",
            "iteration 190, loss is: 0.35567590594291687, w is: 0.5210345387458801, b is 0.10500624030828476\n",
            "iteration 191, loss is: 0.3538838326931, w is: 0.5232551097869873, b is 0.10437620431184769\n",
            "iteration 192, loss is: 0.35211318731307983, w is: 0.5254623889923096, b is 0.10374994575977325\n",
            "iteration 193, loss is: 0.35036370158195496, w is: 0.5276563763618469, b is 0.10312744975090027\n",
            "iteration 194, loss is: 0.348635196685791, w is: 0.5298371911048889, b is 0.10250868648290634\n",
            "iteration 195, loss is: 0.34692734479904175, w is: 0.5320049524307251, b is 0.10189363360404968\n",
            "iteration 196, loss is: 0.3452399671077728, w is: 0.5341597199440002, b is 0.1012822687625885\n",
            "iteration 197, loss is: 0.3435727059841156, w is: 0.5363015532493591, b is 0.1006745770573616\n",
            "iteration 198, loss is: 0.34192541241645813, w is: 0.5384305119514465, b is 0.1000705286860466\n",
            "iteration 199, loss is: 0.3402979075908661, w is: 0.540546715259552, b is 0.0994701087474823\n",
            "iteration 200, loss is: 0.33868977427482605, w is: 0.5426502227783203, b is 0.09887328743934631\n",
            "iteration 201, loss is: 0.33710092306137085, w is: 0.5447410941123962, b is 0.09828004986047745\n",
            "iteration 202, loss is: 0.335531085729599, w is: 0.5468194484710693, b is 0.09769036620855331\n",
            "iteration 203, loss is: 0.333979994058609, w is: 0.5488852858543396, b is 0.09710422158241272\n",
            "iteration 204, loss is: 0.33244749903678894, w is: 0.5509387850761414, b is 0.09652159363031387\n",
            "iteration 205, loss is: 0.3309333026409149, w is: 0.5529799461364746, b is 0.09594246745109558\n",
            "iteration 206, loss is: 0.329437255859375, w is: 0.5550088286399841, b is 0.09536681324243546\n",
            "iteration 207, loss is: 0.3279591202735901, w is: 0.5570255517959595, b is 0.0947946161031723\n",
            "iteration 208, loss is: 0.3264985978603363, w is: 0.5590301752090454, b is 0.09422584623098373\n",
            "iteration 209, loss is: 0.32505562901496887, w is: 0.5610227584838867, b is 0.09366048872470856\n",
            "iteration 210, loss is: 0.3236299157142639, w is: 0.563003420829773, b is 0.09309852868318558\n",
            "iteration 211, loss is: 0.32222121953964233, w is: 0.5649721622467041, b is 0.09253993630409241\n",
            "iteration 212, loss is: 0.3208293914794922, w is: 0.5669291019439697, b is 0.09198469668626785\n",
            "iteration 213, loss is: 0.31945422291755676, w is: 0.5688742995262146, b is 0.09143278747797012\n",
            "iteration 214, loss is: 0.31809550523757935, w is: 0.5708078145980835, b is 0.09088419377803802\n",
            "iteration 215, loss is: 0.3167530596256256, w is: 0.572729766368866, b is 0.09033888578414917\n",
            "iteration 216, loss is: 0.31542664766311646, w is: 0.574640154838562, b is 0.08979685604572296\n",
            "iteration 217, loss is: 0.31411612033843994, w is: 0.5765390992164612, b is 0.08925807476043701\n",
            "iteration 218, loss is: 0.31282123923301697, w is: 0.5784266591072083, b is 0.08872252702713013\n",
            "iteration 219, loss is: 0.311541885137558, w is: 0.580302894115448, b is 0.08819019049406052\n",
            "iteration 220, loss is: 0.3102777898311615, w is: 0.5821678638458252, b is 0.08766105026006699\n",
            "iteration 221, loss is: 0.30902886390686035, w is: 0.5840216279029846, b is 0.08713508397340775\n",
            "iteration 222, loss is: 0.30779486894607544, w is: 0.5858643054962158, b is 0.0866122767329216\n",
            "iteration 223, loss is: 0.3065756559371948, w is: 0.5876958966255188, b is 0.08609260618686676\n",
            "iteration 224, loss is: 0.3053709864616394, w is: 0.5895165205001831, b is 0.08557604998350143\n",
            "iteration 225, loss is: 0.30418074131011963, w is: 0.5913261771202087, b is 0.08506259322166443\n",
            "iteration 226, loss is: 0.30300477147102356, w is: 0.5931249856948853, b is 0.08455222100019455\n",
            "iteration 227, loss is: 0.3018428683280945, w is: 0.5949130058288574, b is 0.08404491096735\n",
            "iteration 228, loss is: 0.3006948232650757, w is: 0.59669029712677, b is 0.08354064077138901\n",
            "iteration 229, loss is: 0.2995605766773224, w is: 0.5984569191932678, b is 0.08303939551115036\n",
            "iteration 230, loss is: 0.2984398901462555, w is: 0.6002129316329956, b is 0.08254116028547287\n",
            "iteration 231, loss is: 0.2973325848579407, w is: 0.6019584536552429, b is 0.08204591274261475\n",
            "iteration 232, loss is: 0.296238511800766, w is: 0.6036934852600098, b is 0.0815536379814148\n",
            "iteration 233, loss is: 0.2951575219631195, w is: 0.6054180860519409, b is 0.08106431365013123\n",
            "iteration 234, loss is: 0.29408949613571167, w is: 0.6071323752403259, b is 0.08057792484760284\n",
            "iteration 235, loss is: 0.29303425550460815, w is: 0.6088363528251648, b is 0.08009445667266846\n",
            "iteration 236, loss is: 0.29199162125587463, w is: 0.6105301380157471, b is 0.07961388677358627\n",
            "iteration 237, loss is: 0.29096144437789917, w is: 0.6122137308120728, b is 0.0791362002491951\n",
            "iteration 238, loss is: 0.2899436354637146, w is: 0.6138872504234314, b is 0.07866138219833374\n",
            "iteration 239, loss is: 0.2889379560947418, w is: 0.615550696849823, b is 0.0781894102692604\n",
            "iteration 240, loss is: 0.28794431686401367, w is: 0.6172041893005371, b is 0.0777202770113945\n",
            "iteration 241, loss is: 0.2869625389575958, w is: 0.6188477277755737, b is 0.07725395262241364\n",
            "iteration 242, loss is: 0.2859925627708435, w is: 0.6204814314842224, b is 0.07679042965173721\n",
            "iteration 243, loss is: 0.2850342094898224, w is: 0.6221053004264832, b is 0.07632968574762344\n",
            "iteration 244, loss is: 0.28408730030059814, w is: 0.6237194538116455, b is 0.07587170600891113\n",
            "iteration 245, loss is: 0.28315165638923645, w is: 0.6253238916397095, b is 0.07541647553443909\n",
            "iteration 246, loss is: 0.2822272777557373, w is: 0.6269187331199646, b is 0.07496397942304611\n",
            "iteration 247, loss is: 0.281313955783844, w is: 0.6285039782524109, b is 0.07451419532299042\n",
            "iteration 248, loss is: 0.2804115414619446, w is: 0.6300797462463379, b is 0.07406710833311081\n",
            "iteration 249, loss is: 0.2795199453830719, w is: 0.6316460371017456, b is 0.0736227035522461\n",
            "iteration 250, loss is: 0.2786389887332916, w is: 0.6332029700279236, b is 0.07318096607923508\n",
            "iteration 251, loss is: 0.2777685523033142, w is: 0.6347505450248718, b is 0.07274188101291656\n",
            "iteration 252, loss is: 0.2769085764884949, w is: 0.6362888216972351, b is 0.07230543345212936\n",
            "iteration 253, loss is: 0.2760588824748993, w is: 0.6378178596496582, b is 0.07187160104513168\n",
            "iteration 254, loss is: 0.2752193510532379, w is: 0.6393377184867859, b is 0.07144036889076233\n",
            "iteration 255, loss is: 0.2743898332118988, w is: 0.6408484578132629, b is 0.0710117295384407\n",
            "iteration 256, loss is: 0.2735702693462372, w is: 0.6423501372337341, b is 0.07058566063642502\n",
            "iteration 257, loss is: 0.2727605104446411, w is: 0.6438428163528442, b is 0.07016214728355408\n",
            "iteration 258, loss is: 0.27196040749549866, w is: 0.645326554775238, b is 0.06974117457866669\n",
            "iteration 259, loss is: 0.27116990089416504, w is: 0.6468013525009155, b is 0.06932272762060165\n",
            "iteration 260, loss is: 0.2703888714313507, w is: 0.6482673287391663, b is 0.06890679150819778\n",
            "iteration 261, loss is: 0.2696171700954437, w is: 0.6497244834899902, b is 0.06849335134029388\n",
            "iteration 262, loss is: 0.2688547372817993, w is: 0.651172935962677, b is 0.06808239221572876\n",
            "iteration 263, loss is: 0.26810136437416077, w is: 0.6526126861572266, b is 0.06767389923334122\n",
            "iteration 264, loss is: 0.2673570215702057, w is: 0.6540437936782837, b is 0.06726785749197006\n",
            "iteration 265, loss is: 0.26662155985832214, w is: 0.6554663181304932, b is 0.0668642520904541\n",
            "iteration 266, loss is: 0.26589491963386536, w is: 0.6568803191184998, b is 0.06646306812763214\n",
            "iteration 267, loss is: 0.2651769816875458, w is: 0.6582857966423035, b is 0.06606429070234299\n",
            "iteration 268, loss is: 0.26446768641471863, w is: 0.6596828699111938, b is 0.06566790491342545\n",
            "iteration 269, loss is: 0.2637667953968048, w is: 0.6610715389251709, b is 0.06527389585971832\n",
            "iteration 270, loss is: 0.2630743086338043, w is: 0.6624518632888794, b is 0.06488225609064102\n",
            "iteration 271, loss is: 0.26239013671875, w is: 0.6638239622116089, b is 0.06449296325445175\n",
            "iteration 272, loss is: 0.2617141008377075, w is: 0.6651877760887146, b is 0.06410600244998932\n",
            "iteration 273, loss is: 0.2610461711883545, w is: 0.6665434241294861, b is 0.06372136622667313\n",
            "iteration 274, loss is: 0.26038628816604614, w is: 0.6678909659385681, b is 0.06333903968334198\n",
            "iteration 275, loss is: 0.25973421335220337, w is: 0.6692304015159607, b is 0.06295900791883469\n",
            "iteration 276, loss is: 0.25909000635147095, w is: 0.6705617904663086, b is 0.06258125603199005\n",
            "iteration 277, loss is: 0.25845345854759216, w is: 0.6718851923942566, b is 0.06220576912164688\n",
            "iteration 278, loss is: 0.25782454013824463, w is: 0.6732006669044495, b is 0.06183253601193428\n",
            "iteration 279, loss is: 0.2572031617164612, w is: 0.674508273601532, b is 0.061461541801691055\n",
            "iteration 280, loss is: 0.25658926367759705, w is: 0.6758080124855042, b is 0.06109277158975601\n",
            "iteration 281, loss is: 0.2559826374053955, w is: 0.6770999431610107, b is 0.060726214200258255\n",
            "iteration 282, loss is: 0.2553832530975342, w is: 0.6783841252326965, b is 0.06036185845732689\n",
            "iteration 283, loss is: 0.2547910809516907, w is: 0.6796606183052063, b is 0.05999968573451042\n",
            "iteration 284, loss is: 0.25420600175857544, w is: 0.68092942237854, b is 0.059639688581228256\n",
            "iteration 285, loss is: 0.2536278963088989, w is: 0.6821906566619873, b is 0.0592818520963192\n",
            "iteration 286, loss is: 0.25305670499801636, w is: 0.6834443211555481, b is 0.058926161378622055\n",
            "iteration 287, loss is: 0.25249236822128296, w is: 0.6846904158592224, b is 0.05857260525226593\n",
            "iteration 288, loss is: 0.2519347369670868, w is: 0.6859290599822998, b is 0.05822116881608963\n",
            "iteration 289, loss is: 0.25138384103775024, w is: 0.6871602535247803, b is 0.05787184089422226\n",
            "iteration 290, loss is: 0.2508395314216614, w is: 0.6883840560913086, b is 0.05752461031079292\n",
            "iteration 291, loss is: 0.250301718711853, w is: 0.6896005272865295, b is 0.05717946216464043\n",
            "iteration 292, loss is: 0.24977032840251923, w is: 0.6908097267150879, b is 0.056836385279893875\n",
            "iteration 293, loss is: 0.249245285987854, w is: 0.6920116543769836, b is 0.05649536848068237\n",
            "iteration 294, loss is: 0.24872656166553497, w is: 0.6932063698768616, b is 0.05615639686584473\n",
            "iteration 295, loss is: 0.2482140213251114, w is: 0.6943939328193665, b is 0.05581945925951004\n",
            "iteration 296, loss is: 0.24770759046077728, w is: 0.6955743432044983, b is 0.05548454076051712\n",
            "iteration 297, loss is: 0.24720725417137146, w is: 0.6967476606369019, b is 0.05515163391828537\n",
            "iteration 298, loss is: 0.24671287834644318, w is: 0.6979139447212219, b is 0.054820723831653595\n",
            "iteration 299, loss is: 0.24622446298599243, w is: 0.6990732550621033, b is 0.0544917993247509\n",
            "iteration 300, loss is: 0.2457418441772461, w is: 0.7002255916595459, b is 0.05416484922170639\n",
            "iteration 301, loss is: 0.24526502192020416, w is: 0.7013710141181946, b is 0.05383985862135887\n",
            "iteration 302, loss is: 0.24479389190673828, w is: 0.7025095820426941, b is 0.05351682007312775\n",
            "iteration 303, loss is: 0.24432840943336487, w is: 0.7036412954330444, b is 0.05319571867585182\n",
            "iteration 304, loss is: 0.24386848509311676, w is: 0.7047662138938904, b is 0.0528765432536602\n",
            "iteration 305, loss is: 0.24341407418251038, w is: 0.7058843970298767, b is 0.05255928263068199\n",
            "iteration 306, loss is: 0.24296510219573975, w is: 0.7069958448410034, b is 0.052243925631046295\n",
            "iteration 307, loss is: 0.2425215095281601, w is: 0.7081006765365601, b is 0.05193046107888222\n",
            "iteration 308, loss is: 0.24208319187164307, w is: 0.7091988325119019, b is 0.05161887779831886\n",
            "iteration 309, loss is: 0.24165014922618866, w is: 0.7102904319763184, b is 0.051309164613485336\n",
            "iteration 310, loss is: 0.2412223070859909, w is: 0.7113754749298096, b is 0.05100131034851074\n",
            "iteration 311, loss is: 0.24079950153827667, w is: 0.7124540209770203, b is 0.050695303827524185\n",
            "iteration 312, loss is: 0.24038180708885193, w is: 0.7135260701179504, b is 0.05039113014936447\n",
            "iteration 313, loss is: 0.23996910452842712, w is: 0.7145916819572449, b is 0.050088781863451004\n",
            "iteration 314, loss is: 0.23956133425235748, w is: 0.7156509160995483, b is 0.04978824779391289\n",
            "iteration 315, loss is: 0.23915846645832062, w is: 0.7167037725448608, b is 0.04948951676487923\n",
            "iteration 316, loss is: 0.23876039683818817, w is: 0.7177503108978271, b is 0.049192581325769424\n",
            "iteration 317, loss is: 0.23836711049079895, w is: 0.718790590763092, b is 0.04889742657542229\n",
            "iteration 318, loss is: 0.2379785031080246, w is: 0.7198246121406555, b is 0.04860404133796692\n",
            "iteration 319, loss is: 0.23759455978870392, w is: 0.7208524346351624, b is 0.04831241816282272\n",
            "iteration 320, loss is: 0.23721522092819214, w is: 0.7218741178512573, b is 0.048022542148828506\n",
            "iteration 321, loss is: 0.2368403971195221, w is: 0.7228896617889404, b is 0.04773440584540367\n",
            "iteration 322, loss is: 0.2364700734615326, w is: 0.7238991260528564, b is 0.04744799807667732\n",
            "iteration 323, loss is: 0.23610416054725647, w is: 0.7249025106430054, b is 0.04716331139206886\n",
            "iteration 324, loss is: 0.23574265837669373, w is: 0.725899875164032, b is 0.0468803308904171\n",
            "iteration 325, loss is: 0.2353854477405548, w is: 0.726891279220581, b is 0.046599049121141434\n",
            "iteration 326, loss is: 0.23503249883651733, w is: 0.7278767228126526, b is 0.04631945490837097\n",
            "iteration 327, loss is: 0.2346837967634201, w is: 0.7288562655448914, b is 0.04604153707623482\n",
            "iteration 328, loss is: 0.23433929681777954, w is: 0.7298299074172974, b is 0.045765288174152374\n",
            "iteration 329, loss is: 0.2339988499879837, w is: 0.7307977080345154, b is 0.04549069702625275\n",
            "iteration 330, loss is: 0.23366251587867737, w is: 0.7317597270011902, b is 0.04521775245666504\n",
            "iteration 331, loss is: 0.23333021998405457, w is: 0.7327159643173218, b is 0.044946447014808655\n",
            "iteration 332, loss is: 0.23300187289714813, w is: 0.7336664795875549, b is 0.0446767695248127\n",
            "iteration 333, loss is: 0.23267745971679688, w is: 0.7346112728118896, b is 0.044408708810806274\n",
            "iteration 334, loss is: 0.23235690593719482, w is: 0.7355504035949707, b is 0.044142257422208786\n",
            "iteration 335, loss is: 0.23204024136066437, w is: 0.7364838719367981, b is 0.04387740418314934\n",
            "iteration 336, loss is: 0.23172731697559357, w is: 0.7374117374420166, b is 0.043614137917757034\n",
            "iteration 337, loss is: 0.2314181625843048, w is: 0.738334059715271, b is 0.04335245117545128\n",
            "iteration 338, loss is: 0.23111268877983093, w is: 0.7392508387565613, b is 0.043092336505651474\n",
            "iteration 339, loss is: 0.23081089556217194, w is: 0.7401621341705322, b is 0.042833782732486725\n",
            "iteration 340, loss is: 0.23051270842552185, w is: 0.7410679459571838, b is 0.042576778680086136\n",
            "iteration 341, loss is: 0.2302180826663971, w is: 0.7419683337211609, b is 0.04232131689786911\n",
            "iteration 342, loss is: 0.2299269586801529, w is: 0.7428632974624634, b is 0.04206738993525505\n",
            "iteration 343, loss is: 0.22963933646678925, w is: 0.7437528967857361, b is 0.04181498661637306\n",
            "iteration 344, loss is: 0.229355126619339, w is: 0.7446371912956238, b is 0.04156409576535225\n",
            "iteration 345, loss is: 0.2290744036436081, w is: 0.7455161213874817, b is 0.041314709931612015\n",
            "iteration 346, loss is: 0.22879697382450104, w is: 0.7463898062705994, b is 0.04106682166457176\n",
            "iteration 347, loss is: 0.22852285206317902, w is: 0.7472582459449768, b is 0.040820419788360596\n",
            "iteration 348, loss is: 0.22825206816196442, w is: 0.7481215000152588, b is 0.04057549685239792\n",
            "iteration 349, loss is: 0.2279844731092453, w is: 0.7489795684814453, b is 0.040332045406103134\n",
            "iteration 350, loss is: 0.22772008180618286, w is: 0.7498324513435364, b is 0.04009005427360535\n",
            "iteration 351, loss is: 0.22745883464813232, w is: 0.7506802678108215, b is 0.03984951227903366\n",
            "iteration 352, loss is: 0.22720076143741608, w is: 0.751522958278656, b is 0.03961041569709778\n",
            "iteration 353, loss is: 0.22694578766822815, w is: 0.7523605823516846, b is 0.039372753351926804\n",
            "iteration 354, loss is: 0.22669382393360138, w is: 0.753193199634552, b is 0.03913651779294014\n",
            "iteration 355, loss is: 0.22644488513469696, w is: 0.7540208101272583, b is 0.03890169784426689\n",
            "iteration 356, loss is: 0.2261989265680313, w is: 0.7548434734344482, b is 0.03866828605532646\n",
            "iteration 357, loss is: 0.22595590353012085, w is: 0.7556611895561218, b is 0.038436274975538254\n",
            "iteration 358, loss is: 0.22571580111980438, w is: 0.7564740180969238, b is 0.03820565715432167\n",
            "iteration 359, loss is: 0.22547855973243713, w is: 0.7572819590568542, b is 0.03797642141580582\n",
            "iteration 360, loss is: 0.2252441793680191, w is: 0.7580850720405579, b is 0.037748564034700394\n",
            "iteration 361, loss is: 0.22501255571842194, w is: 0.7588833570480347, b is 0.037522073835134506\n",
            "iteration 362, loss is: 0.2247837483882904, w is: 0.7596768140792847, b is 0.03729693964123726\n",
            "iteration 363, loss is: 0.22455766797065735, w is: 0.7604655623435974, b is 0.03707315772771835\n",
            "iteration 364, loss is: 0.224334254860878, w is: 0.7612495422363281, b is 0.03685072064399719\n",
            "iteration 365, loss is: 0.22411353886127472, w is: 0.7620288133621216, b is 0.03662961721420288\n",
            "iteration 366, loss is: 0.22389549016952515, w is: 0.7628034353256226, b is 0.03640983998775482\n",
            "iteration 367, loss is: 0.2236800193786621, w is: 0.763573408126831, b is 0.03619138151407242\n",
            "iteration 368, loss is: 0.2234671711921692, w is: 0.7643387317657471, b is 0.03597423434257507\n",
            "iteration 369, loss is: 0.22325679659843445, w is: 0.7650994658470154, b is 0.03575838729739189\n",
            "iteration 370, loss is: 0.22304897010326385, w is: 0.7658556699752808, b is 0.035543836653232574\n",
            "iteration 371, loss is: 0.22284364700317383, w is: 0.7666073441505432, b is 0.035330574959516525\n",
            "iteration 372, loss is: 0.2226407676935196, w is: 0.7673544883728027, b is 0.03511859104037285\n",
            "iteration 373, loss is: 0.22244033217430115, w is: 0.7680971622467041, b is 0.034907881170511246\n",
            "iteration 374, loss is: 0.22224228084087372, w is: 0.7688353657722473, b is 0.03469843417406082\n",
            "iteration 375, loss is: 0.2220465987920761, w is: 0.7695691585540771, b is 0.03449024260044098\n",
            "iteration 376, loss is: 0.22185324132442474, w is: 0.7702985405921936, b is 0.03428330272436142\n",
            "iteration 377, loss is: 0.2216622233390808, w is: 0.7710235118865967, b is 0.03407760336995125\n",
            "iteration 378, loss is: 0.22147348523139954, w is: 0.7717441320419312, b is 0.03387313708662987\n",
            "iteration 379, loss is: 0.22128702700138092, w is: 0.7724604606628418, b is 0.03366989642381668\n",
            "iteration 380, loss is: 0.2211027592420578, w is: 0.7731724977493286, b is 0.03346787765622139\n",
            "iteration 381, loss is: 0.22092071175575256, w is: 0.7738802433013916, b is 0.0332670696079731\n",
            "iteration 382, loss is: 0.22074085474014282, w is: 0.7745837569236755, b is 0.03306746855378151\n",
            "iteration 383, loss is: 0.2205631136894226, w is: 0.7752830386161804, b is 0.032869063317775726\n",
            "iteration 384, loss is: 0.2203875333070755, w is: 0.775978147983551, b is 0.03267185017466545\n",
            "iteration 385, loss is: 0.22021402418613434, w is: 0.7766690850257874, b is 0.03247581794857979\n",
            "iteration 386, loss is: 0.2200426310300827, w is: 0.7773558497428894, b is 0.03228096291422844\n",
            "iteration 387, loss is: 0.21987326443195343, w is: 0.778038501739502, b is 0.03208727762103081\n",
            "iteration 388, loss is: 0.21970589458942413, w is: 0.778717041015625, b is 0.031894754618406296\n",
            "iteration 389, loss is: 0.2195405662059784, w is: 0.7793915271759033, b is 0.03170338645577431\n",
            "iteration 390, loss is: 0.21937721967697144, w is: 0.7800619602203369, b is 0.031513165682554245\n",
            "iteration 391, loss is: 0.21921581029891968, w is: 0.7807283997535706, b is 0.03132408484816551\n",
            "iteration 392, loss is: 0.21905633807182312, w is: 0.7813907861709595, b is 0.03113614022731781\n",
            "iteration 393, loss is: 0.21889878809452057, w is: 0.7820492386817932, b is 0.030949322506785393\n",
            "iteration 394, loss is: 0.21874308586120605, w is: 0.782703697681427, b is 0.030763626098632812\n",
            "iteration 395, loss is: 0.21858929097652435, w is: 0.7833542823791504, b is 0.030579043552279472\n",
            "iteration 396, loss is: 0.2184372991323471, w is: 0.7840009331703186, b is 0.030395569279789925\n",
            "iteration 397, loss is: 0.2182871401309967, w is: 0.7846437096595764, b is 0.030213195830583572\n",
            "iteration 398, loss is: 0.21813878417015076, w is: 0.7852826118469238, b is 0.03003191575407982\n",
            "iteration 399, loss is: 0.21799221634864807, w is: 0.7859176993370056, b is 0.029851723462343216\n",
            "iteration 400, loss is: 0.21784739196300507, w is: 0.7865489721298218, b is 0.029672613367438316\n",
            "iteration 401, loss is: 0.21770426630973816, w is: 0.7871764898300171, b is 0.029494578018784523\n",
            "iteration 402, loss is: 0.21756291389465332, w is: 0.7878001928329468, b is 0.02931760996580124\n",
            "iteration 403, loss is: 0.2174231857061386, w is: 0.7884202003479004, b is 0.029141703620553017\n",
            "iteration 404, loss is: 0.2172851860523224, w is: 0.7890364527702332, b is 0.028966853395104408\n",
            "iteration 405, loss is: 0.2171487808227539, w is: 0.7896490097045898, b is 0.028793051838874817\n",
            "iteration 406, loss is: 0.21701404452323914, w is: 0.7902578711509705, b is 0.028620293363928795\n",
            "iteration 407, loss is: 0.2168809175491333, w is: 0.7908630967140198, b is 0.028448572382330894\n",
            "iteration 408, loss is: 0.2167493999004364, w is: 0.7914646863937378, b is 0.02827788144350052\n",
            "iteration 409, loss is: 0.21661943197250366, w is: 0.7920626997947693, b is 0.02810821495950222\n",
            "iteration 410, loss is: 0.21649101376533508, w is: 0.7926570773124695, b is 0.0279395654797554\n",
            "iteration 411, loss is: 0.21636417508125305, w is: 0.7932479381561279, b is 0.027771927416324615\n",
            "iteration 412, loss is: 0.21623878180980682, w is: 0.7938352227210999, b is 0.027605295181274414\n",
            "iteration 413, loss is: 0.21611493825912476, w is: 0.79441899061203, b is 0.02743966318666935\n",
            "iteration 414, loss is: 0.21599256992340088, w is: 0.7949992418289185, b is 0.027275025844573975\n",
            "iteration 415, loss is: 0.215871661901474, w is: 0.7955760359764099, b is 0.027111375704407692\n",
            "iteration 416, loss is: 0.21575219929218292, w is: 0.7961493730545044, b is 0.026948707178235054\n",
            "iteration 417, loss is: 0.21563416719436646, w is: 0.7967192530632019, b is 0.026787014678120613\n",
            "iteration 418, loss is: 0.2155175507068634, w is: 0.7972857356071472, b is 0.02662629261612892\n",
            "iteration 419, loss is: 0.21540230512619019, w is: 0.7978488206863403, b is 0.02646653540432453\n",
            "iteration 420, loss is: 0.2152884602546692, w is: 0.7984085083007812, b is 0.026307735592126846\n",
            "iteration 421, loss is: 0.21517598628997803, w is: 0.7989648580551147, b is 0.026149889454245567\n",
            "iteration 422, loss is: 0.21506483852863312, w is: 0.7995178699493408, b is 0.025992989540100098\n",
            "iteration 423, loss is: 0.21495504677295685, w is: 0.8000675439834595, b is 0.02583703212440014\n",
            "iteration 424, loss is: 0.21484655141830444, w is: 0.8006139397621155, b is 0.025682009756565094\n",
            "iteration 425, loss is: 0.2147393524646759, w is: 0.8011570572853088, b is 0.025527916848659515\n",
            "iteration 426, loss is: 0.21463342010974884, w is: 0.8016968965530396, b is 0.025374749675393105\n",
            "iteration 427, loss is: 0.21452875435352325, w is: 0.8022335171699524, b is 0.025222500786185265\n",
            "iteration 428, loss is: 0.21442537009716034, w is: 0.8027669191360474, b is 0.025071166455745697\n",
            "iteration 429, loss is: 0.2143232226371765, w is: 0.8032971024513245, b is 0.024920739233493805\n",
            "iteration 430, loss is: 0.21422229707241058, w is: 0.8038241267204285, b is 0.02477121539413929\n",
            "iteration 431, loss is: 0.21412253379821777, w is: 0.8043479919433594, b is 0.024622587487101555\n",
            "iteration 432, loss is: 0.21402400732040405, w is: 0.8048686981201172, b is 0.0244748517870903\n",
            "iteration 433, loss is: 0.21392665803432465, w is: 0.8053862452507019, b is 0.024328002706170082\n",
            "iteration 434, loss is: 0.21383045613765717, w is: 0.8059006929397583, b is 0.02418203465640545\n",
            "iteration 435, loss is: 0.2137354016304016, w is: 0.8064121007919312, b is 0.024036942049860954\n",
            "iteration 436, loss is: 0.21364150941371918, w is: 0.8069204092025757, b is 0.0238927211612463\n",
            "iteration 437, loss is: 0.21354873478412628, w is: 0.8074256777763367, b is 0.02374936453998089\n",
            "iteration 438, loss is: 0.21345706284046173, w is: 0.8079279065132141, b is 0.02360686846077442\n",
            "iteration 439, loss is: 0.21336649358272552, w is: 0.808427095413208, b is 0.023465227335691452\n",
            "iteration 440, loss is: 0.21327698230743408, w is: 0.8089233040809631, b is 0.02332443557679653\n",
            "iteration 441, loss is: 0.21318857371807098, w is: 0.8094165325164795, b is 0.023184489458799362\n",
            "iteration 442, loss is: 0.21310120820999146, w is: 0.8099068403244019, b is 0.023045383393764496\n",
            "iteration 443, loss is: 0.2130149006843567, w is: 0.8103941679000854, b is 0.022907111793756485\n",
            "iteration 444, loss is: 0.2129296064376831, w is: 0.810878574848175, b is 0.022769669070839882\n",
            "iteration 445, loss is: 0.2128453552722931, w is: 0.8113600611686707, b is 0.022633051499724388\n",
            "iteration 446, loss is: 0.21276210248470306, w is: 0.811838686466217, b is 0.022497253492474556\n",
            "iteration 447, loss is: 0.21267984807491302, w is: 0.8123144507408142, b is 0.022362269461154938\n",
            "iteration 448, loss is: 0.21259857714176178, w is: 0.8127873539924622, b is 0.022228095680475235\n",
            "iteration 449, loss is: 0.21251824498176575, w is: 0.8132573962211609, b is 0.0220947265625\n",
            "iteration 450, loss is: 0.2124389261007309, w is: 0.8137246370315552, b is 0.021962158381938934\n",
            "iteration 451, loss is: 0.21236053109169006, w is: 0.814189076423645, b is 0.02183038555085659\n",
            "iteration 452, loss is: 0.21228304505348206, w is: 0.8146507143974304, b is 0.02169940248131752\n",
            "iteration 453, loss is: 0.21220654249191284, w is: 0.8151096105575562, b is 0.021569205448031425\n",
            "iteration 454, loss is: 0.21213091909885406, w is: 0.8155657649040222, b is 0.021439790725708008\n",
            "iteration 455, loss is: 0.2120562195777893, w is: 0.8160191774368286, b is 0.02131115272641182\n",
            "iteration 456, loss is: 0.21198241412639618, w is: 0.8164698481559753, b is 0.021183285862207413\n",
            "iteration 457, loss is: 0.2119094729423523, w is: 0.8169178366661072, b is 0.02105618640780449\n",
            "iteration 458, loss is: 0.21183741092681885, w is: 0.8173630833625793, b is 0.0209298487752676\n",
            "iteration 459, loss is: 0.21176622807979584, w is: 0.8178057074546814, b is 0.02080426923930645\n",
            "iteration 460, loss is: 0.2116958647966385, w is: 0.8182456493377686, b is 0.020679444074630737\n",
            "iteration 461, loss is: 0.21162638068199158, w is: 0.8186829686164856, b is 0.020555367693305016\n",
            "iteration 462, loss is: 0.21155770123004913, w is: 0.8191176652908325, b is 0.020432036370038986\n",
            "iteration 463, loss is: 0.21148984134197235, w is: 0.8195497393608093, b is 0.0203094445168972\n",
            "iteration 464, loss is: 0.21142280101776123, w is: 0.8199792504310608, b is 0.020187588408589363\n",
            "iteration 465, loss is: 0.21135656535625458, w is: 0.8204061388969421, b is 0.020066462457180023\n",
            "iteration 466, loss is: 0.2112911194562912, w is: 0.8208304643630981, b is 0.019946062937378883\n",
            "iteration 467, loss is: 0.2112264335155487, w is: 0.8212522864341736, b is 0.019826386123895645\n",
            "iteration 468, loss is: 0.21116258203983307, w is: 0.8216715455055237, b is 0.01970742829144001\n",
            "iteration 469, loss is: 0.21109946072101593, w is: 0.8220883011817932, b is 0.01958918385207653\n",
            "iteration 470, loss is: 0.21103708446025848, w is: 0.8225025534629822, b is 0.019471649080514908\n",
            "iteration 471, loss is: 0.21097548305988312, w is: 0.8229143023490906, b is 0.019354818388819695\n",
            "iteration 472, loss is: 0.21091458201408386, w is: 0.8233236074447632, b is 0.01923868991434574\n",
            "iteration 473, loss is: 0.2108544260263443, w is: 0.82373046875, b is 0.0191232580691576\n",
            "iteration 474, loss is: 0.21079498529434204, w is: 0.824134886264801, b is 0.019008519127964973\n",
            "iteration 475, loss is: 0.21073627471923828, w is: 0.8245368599891663, b is 0.018894467502832413\n",
            "iteration 476, loss is: 0.21067823469638824, w is: 0.8249364495277405, b is 0.01878110133111477\n",
            "iteration 477, loss is: 0.21062088012695312, w is: 0.8253335952758789, b is 0.018668415024876595\n",
            "iteration 478, loss is: 0.2105642557144165, w is: 0.8257283568382263, b is 0.01855640485882759\n",
            "iteration 479, loss is: 0.2105083167552948, w is: 0.8261207938194275, b is 0.01844506710767746\n",
            "iteration 480, loss is: 0.21045303344726562, w is: 0.8265108466148376, b is 0.018334396183490753\n",
            "iteration 481, loss is: 0.2103983759880066, w is: 0.8268985748291016, b is 0.018224390223622322\n",
            "iteration 482, loss is: 0.21034440398216248, w is: 0.8272839784622192, b is 0.01811504364013672\n",
            "iteration 483, loss is: 0.2102910429239273, w is: 0.8276670575141907, b is 0.018006352707743645\n",
            "iteration 484, loss is: 0.21023835241794586, w is: 0.8280478119850159, b is 0.01789831556379795\n",
            "iteration 485, loss is: 0.21018631756305695, w is: 0.8284263014793396, b is 0.01779092475771904\n",
            "iteration 486, loss is: 0.2101348489522934, w is: 0.8288025259971619, b is 0.017684178426861763\n",
            "iteration 487, loss is: 0.21008403599262238, w is: 0.8291764855384827, b is 0.01757807284593582\n",
            "iteration 488, loss is: 0.2100338190793991, w is: 0.829548180103302, b is 0.017472604289650917\n",
            "iteration 489, loss is: 0.2099841982126236, w is: 0.8299176692962646, b is 0.01736776903271675\n",
            "iteration 490, loss is: 0.20993517339229584, w is: 0.8302849531173706, b is 0.017263561487197876\n",
            "iteration 491, loss is: 0.20988672971725464, w is: 0.8306500315666199, b is 0.017159979790449142\n",
            "iteration 492, loss is: 0.2098388671875, w is: 0.8310129046440125, b is 0.017057020217180252\n",
            "iteration 493, loss is: 0.2097916156053543, w is: 0.8313736319541931, b is 0.016954677179455757\n",
            "iteration 494, loss is: 0.20974485576152802, w is: 0.8317321538925171, b is 0.01685294881463051\n",
            "iteration 495, loss is: 0.20969872176647186, w is: 0.8320885300636292, b is 0.016751831397414207\n",
            "iteration 496, loss is: 0.2096530944108963, w is: 0.8324427604675293, b is 0.016651321202516556\n",
            "iteration 497, loss is: 0.2096080482006073, w is: 0.8327949047088623, b is 0.016551412642002106\n",
            "iteration 498, loss is: 0.20956352353096008, w is: 0.8331449031829834, b is 0.016452103853225708\n",
            "iteration 499, loss is: 0.20951952040195465, w is: 0.8334928154945374, b is 0.016353391110897064\n",
            "iteration 500, loss is: 0.2094760686159134, w is: 0.8338386416435242, b is 0.016255270689725876\n",
            "iteration 501, loss is: 0.20943310856819153, w is: 0.8341823816299438, b is 0.016157738864421844\n",
            "iteration 502, loss is: 0.20939069986343384, w is: 0.8345240950584412, b is 0.01606079190969467\n",
            "iteration 503, loss is: 0.20934873819351196, w is: 0.8348637223243713, b is 0.015964427962899208\n",
            "iteration 504, loss is: 0.20930732786655426, w is: 0.8352013230323792, b is 0.015868641436100006\n",
            "iteration 505, loss is: 0.20926639437675476, w is: 0.8355368971824646, b is 0.015773430466651917\n",
            "iteration 506, loss is: 0.20922596752643585, w is: 0.8358704447746277, b is 0.01567878946661949\n",
            "iteration 507, loss is: 0.20918601751327515, w is: 0.8362020254135132, b is 0.015584716573357582\n",
            "iteration 508, loss is: 0.20914652943611145, w is: 0.8365315794944763, b is 0.01549120806157589\n",
            "iteration 509, loss is: 0.20910753309726715, w is: 0.8368591666221619, b is 0.01539826113730669\n",
            "iteration 510, loss is: 0.20906901359558105, w is: 0.8371847867965698, b is 0.015305871143937111\n",
            "iteration 511, loss is: 0.20903094112873077, w is: 0.8375084400177002, b is 0.015214036218822002\n",
            "iteration 512, loss is: 0.2089933156967163, w is: 0.8378301858901978, b is 0.015122751705348492\n",
            "iteration 513, loss is: 0.20895618200302124, w is: 0.8381499648094177, b is 0.015032014809548855\n",
            "iteration 514, loss is: 0.2089194357395172, w is: 0.8384678363800049, b is 0.014941822737455368\n",
            "iteration 515, loss is: 0.2088831514120102, w is: 0.8387838006019592, b is 0.014852171763777733\n",
            "iteration 516, loss is: 0.2088472694158554, w is: 0.8390978574752808, b is 0.014763059094548225\n",
            "iteration 517, loss is: 0.20881186425685883, w is: 0.8394100666046143, b is 0.014674481004476547\n",
            "iteration 518, loss is: 0.20877689123153687, w is: 0.8397203683853149, b is 0.0145864337682724\n",
            "iteration 519, loss is: 0.20874227583408356, w is: 0.8400288224220276, b is 0.014498915523290634\n",
            "iteration 520, loss is: 0.20870812237262726, w is: 0.8403354287147522, b is 0.014411921612918377\n",
            "iteration 521, loss is: 0.2086743712425232, w is: 0.8406401872634888, b is 0.014325450174510479\n",
            "iteration 522, loss is: 0.20864102244377136, w is: 0.8409431576728821, b is 0.014239497482776642\n",
            "iteration 523, loss is: 0.20860804617404938, w is: 0.8412442803382874, b is 0.014154060743749142\n",
            "iteration 524, loss is: 0.20857550203800201, w is: 0.8415436148643494, b is 0.01406913623213768\n",
            "iteration 525, loss is: 0.2085433304309845, w is: 0.8418411612510681, b is 0.013984721153974533\n",
            "iteration 526, loss is: 0.2085115611553192, w is: 0.8421369194984436, b is 0.013900812715291977\n",
            "iteration 527, loss is: 0.20848014950752258, w is: 0.8424308896064758, b is 0.013817408122122288\n",
            "iteration 528, loss is: 0.2084491103887558, w is: 0.8427230715751648, b is 0.013734503649175167\n",
            "iteration 529, loss is: 0.20841845870018005, w is: 0.8430135250091553, b is 0.013652096502482891\n",
            "iteration 530, loss is: 0.20838814973831177, w is: 0.8433022499084473, b is 0.013570183888077736\n",
            "iteration 531, loss is: 0.20835824310779572, w is: 0.8435892462730408, b is 0.013488763011991978\n",
            "iteration 532, loss is: 0.20832863450050354, w is: 0.8438745141029358, b is 0.013407830148935318\n",
            "iteration 533, loss is: 0.2082994282245636, w is: 0.8441580533981323, b is 0.013327383436262608\n",
            "iteration 534, loss is: 0.20827054977416992, w is: 0.8444398641586304, b is 0.013247419148683548\n",
            "iteration 535, loss is: 0.2082420438528061, w is: 0.8447200059890747, b is 0.013167934492230415\n",
            "iteration 536, loss is: 0.20821386575698853, w is: 0.8449984788894653, b is 0.013088926672935486\n",
            "iteration 537, loss is: 0.20818600058555603, w is: 0.8452752828598022, b is 0.013010392896831036\n",
            "iteration 538, loss is: 0.2081584930419922, w is: 0.8455504179000854, b is 0.01293233036994934\n",
            "iteration 539, loss is: 0.208131343126297, w is: 0.8458238840103149, b is 0.012854736298322678\n",
            "iteration 540, loss is: 0.2081044614315033, w is: 0.8460957407951355, b is 0.012777607887983322\n",
            "iteration 541, loss is: 0.20807792246341705, w is: 0.8463659286499023, b is 0.01270094234496355\n",
            "iteration 542, loss is: 0.20805169641971588, w is: 0.8466345071792603, b is 0.012624736875295639\n",
            "iteration 543, loss is: 0.20802578330039978, w is: 0.8469014763832092, b is 0.012548988685011864\n",
            "iteration 544, loss is: 0.20800021290779114, w is: 0.8471668362617493, b is 0.0124736949801445\n",
            "iteration 545, loss is: 0.20797495543956757, w is: 0.8474306464195251, b is 0.012398852966725826\n",
            "iteration 546, loss is: 0.2079499363899231, w is: 0.8476928472518921, b is 0.012324459850788116\n",
            "iteration 547, loss is: 0.2079252451658249, w is: 0.8479534983634949, b is 0.012250512838363647\n",
            "iteration 548, loss is: 0.20790088176727295, w is: 0.8482125401496887, b is 0.01217701006680727\n",
            "iteration 549, loss is: 0.2078767567873001, w is: 0.8484700322151184, b is 0.012103947810828686\n",
            "iteration 550, loss is: 0.20785292983055115, w is: 0.8487259745597839, b is 0.012031324207782745\n",
            "iteration 551, loss is: 0.20782943069934845, w is: 0.8489804267883301, b is 0.011959136463701725\n",
            "iteration 552, loss is: 0.20780619978904724, w is: 0.8492333292961121, b is 0.0118873817846179\n",
            "iteration 553, loss is: 0.20778320729732513, w is: 0.8494847416877747, b is 0.011816057376563549\n",
            "iteration 554, loss is: 0.2077605128288269, w is: 0.8497346043586731, b is 0.01174516137689352\n",
            "iteration 555, loss is: 0.20773808658123016, w is: 0.8499829769134521, b is 0.011674690060317516\n",
            "iteration 556, loss is: 0.2077159434556961, w is: 0.8502298593521118, b is 0.011604641564190388\n",
            "iteration 557, loss is: 0.20769406855106354, w is: 0.8504752516746521, b is 0.011535014025866985\n",
            "iteration 558, loss is: 0.20767243206501007, w is: 0.850719153881073, b is 0.01146580372005701\n",
            "iteration 559, loss is: 0.2076510787010193, w is: 0.8509616255760193, b is 0.011397008784115314\n",
            "iteration 560, loss is: 0.20762993395328522, w is: 0.851202666759491, b is 0.011328626424074173\n",
            "iteration 561, loss is: 0.20760910212993622, w is: 0.8514422178268433, b is 0.011260654777288437\n",
            "iteration 562, loss is: 0.20758850872516632, w is: 0.851680338382721, b is 0.011193091049790382\n",
            "iteration 563, loss is: 0.20756812393665314, w is: 0.851917028427124, b is 0.011125932447612286\n",
            "iteration 564, loss is: 0.20754800736904144, w is: 0.8521522879600525, b is 0.011059177108108997\n",
            "iteration 565, loss is: 0.20752812922000885, w is: 0.8523861765861511, b is 0.010992822237312794\n",
            "iteration 566, loss is: 0.20750850439071655, w is: 0.8526186347007751, b is 0.010926865041255951\n",
            "iteration 567, loss is: 0.20748910307884216, w is: 0.8528497219085693, b is 0.01086130365729332\n",
            "iteration 568, loss is: 0.2074698954820633, w is: 0.8530793786048889, b is 0.01079613622277975\n",
            "iteration 569, loss is: 0.2074509710073471, w is: 0.8533076643943787, b is 0.010731359012424946\n",
            "iteration 570, loss is: 0.20743225514888763, w is: 0.8535345792770386, b is 0.01066697109490633\n",
            "iteration 571, loss is: 0.20741376280784607, w is: 0.8537601828575134, b is 0.01060296967625618\n",
            "iteration 572, loss is: 0.20739547908306122, w is: 0.8539844155311584, b is 0.010539351962506771\n",
            "iteration 573, loss is: 0.20737744867801666, w is: 0.8542072772979736, b is 0.010476116091012955\n",
            "iteration 574, loss is: 0.20735959708690643, w is: 0.8544288277626038, b is 0.010413259267807007\n",
            "iteration 575, loss is: 0.2073419690132141, w is: 0.8546490669250488, b is 0.010350779630243778\n",
            "iteration 576, loss is: 0.2073245793581009, w is: 0.8548679351806641, b is 0.01028867531567812\n",
            "iteration 577, loss is: 0.207307368516922, w is: 0.8550854921340942, b is 0.010226943530142307\n",
            "iteration 578, loss is: 0.20729035139083862, w is: 0.8553017377853394, b is 0.010165581479668617\n",
            "iteration 579, loss is: 0.20727354288101196, w is: 0.8555167317390442, b is 0.010104588232934475\n",
            "iteration 580, loss is: 0.2072569727897644, w is: 0.855730414390564, b is 0.010043960995972157\n",
            "iteration 581, loss is: 0.20724058151245117, w is: 0.8559427857398987, b is 0.009983696974813938\n",
            "iteration 582, loss is: 0.20722438395023346, w is: 0.8561539053916931, b is 0.009923795238137245\n",
            "iteration 583, loss is: 0.20720838010311127, w is: 0.8563637733459473, b is 0.00986425206065178\n",
            "iteration 584, loss is: 0.2071925401687622, w is: 0.8565723896026611, b is 0.009805066511034966\n",
            "iteration 585, loss is: 0.20717692375183105, w is: 0.8567797541618347, b is 0.00974623579531908\n",
            "iteration 586, loss is: 0.20716150104999542, w is: 0.856985867023468, b is 0.009687758050858974\n",
            "iteration 587, loss is: 0.20714624226093292, w is: 0.857190728187561, b is 0.009629631415009499\n",
            "iteration 588, loss is: 0.20713117718696594, w is: 0.8573943376541138, b is 0.009571854025125504\n",
            "iteration 589, loss is: 0.2071162760257721, w is: 0.857596755027771, b is 0.009514423087239265\n",
            "iteration 590, loss is: 0.20710156857967377, w is: 0.8577979803085327, b is 0.009457336738705635\n",
            "iteration 591, loss is: 0.20708702504634857, w is: 0.8579979538917542, b is 0.009400593116879463\n",
            "iteration 592, loss is: 0.2070726901292801, w is: 0.8581967353820801, b is 0.009344189427793026\n",
            "iteration 593, loss is: 0.20705847442150116, w is: 0.8583943247795105, b is 0.009288124740123749\n",
            "iteration 594, loss is: 0.20704446732997894, w is: 0.8585907220840454, b is 0.009232396259903908\n",
            "iteration 595, loss is: 0.20703060925006866, w is: 0.8587859869003296, b is 0.009177002124488354\n",
            "iteration 596, loss is: 0.2070169299840927, w is: 0.8589800596237183, b is 0.009121940471231937\n",
            "iteration 597, loss is: 0.2070033997297287, w is: 0.8591729402542114, b is 0.009067208506166935\n",
            "iteration 598, loss is: 0.206990048289299, w is: 0.8593646883964539, b is 0.009012805297970772\n",
            "iteration 599, loss is: 0.20697683095932007, w is: 0.8595553040504456, b is 0.008958728052675724\n",
            "iteration 600, loss is: 0.20696377754211426, w is: 0.8597447276115417, b is 0.008904975838959217\n",
            "iteration 601, loss is: 0.20695088803768158, w is: 0.8599330186843872, b is 0.008851545862853527\n",
            "iteration 602, loss is: 0.20693817734718323, w is: 0.8601201772689819, b is 0.008798436261713505\n",
            "iteration 603, loss is: 0.20692557096481323, w is: 0.8603062629699707, b is 0.008745645172894001\n",
            "iteration 604, loss is: 0.20691314339637756, w is: 0.8604912161827087, b is 0.008693171665072441\n",
            "iteration 605, loss is: 0.20690084993839264, w is: 0.860675036907196, b is 0.008641012944281101\n",
            "iteration 606, loss is: 0.20688873529434204, w is: 0.8608577847480774, b is 0.008589167147874832\n",
            "iteration 607, loss is: 0.2068767249584198, w is: 0.861039400100708, b is 0.008537632413208485\n",
            "iteration 608, loss is: 0.20686490833759308, w is: 0.8612199425697327, b is 0.00848640687763691\n",
            "iteration 609, loss is: 0.20685319602489471, w is: 0.8613994121551514, b is 0.008435488678514957\n",
            "iteration 610, loss is: 0.2068416178226471, w is: 0.8615778088569641, b is 0.00838487595319748\n",
            "iteration 611, loss is: 0.2068302035331726, w is: 0.8617551326751709, b is 0.008334566839039326\n",
            "iteration 612, loss is: 0.20681889355182648, w is: 0.8619313836097717, b is 0.008284559473395348\n",
            "iteration 613, loss is: 0.20680776238441467, w is: 0.8621065616607666, b is 0.008234851993620396\n",
            "iteration 614, loss is: 0.20679673552513123, w is: 0.8622807264328003, b is 0.00818544253706932\n",
            "iteration 615, loss is: 0.20678584277629852, w is: 0.862453818321228, b is 0.008136330172419548\n",
            "iteration 616, loss is: 0.20677508413791656, w is: 0.8626258969306946, b is 0.008087512105703354\n",
            "iteration 617, loss is: 0.20676445960998535, w is: 0.8627969026565552, b is 0.008038987405598164\n",
            "iteration 618, loss is: 0.2067539542913437, w is: 0.8629668951034546, b is 0.007990753278136253\n",
            "iteration 619, loss is: 0.20674358308315277, w is: 0.8631358742713928, b is 0.007942808791995049\n",
            "iteration 620, loss is: 0.2067333310842514, w is: 0.8633038401603699, b is 0.0078951520845294\n",
            "iteration 621, loss is: 0.2067231833934784, w is: 0.8634707927703857, b is 0.007847781293094158\n",
            "iteration 622, loss is: 0.20671318471431732, w is: 0.8636367321014404, b is 0.007800694555044174\n",
            "iteration 623, loss is: 0.2067032903432846, w is: 0.8638017177581787, b is 0.007753890473395586\n",
            "iteration 624, loss is: 0.20669353008270264, w is: 0.8639656901359558, b is 0.007707367185503244\n",
            "iteration 625, loss is: 0.20668387413024902, w is: 0.8641286492347717, b is 0.007661122828722\n",
            "iteration 626, loss is: 0.20667432248592377, w is: 0.8642906546592712, b is 0.007615156006067991\n",
            "iteration 627, loss is: 0.20666490495204926, w is: 0.8644517064094543, b is 0.007569464854896069\n",
            "iteration 628, loss is: 0.2066555768251419, w is: 0.864611804485321, b is 0.00752404797822237\n",
            "iteration 629, loss is: 0.2066463977098465, w is: 0.8647708892822266, b is 0.007478903513401747\n",
            "iteration 630, loss is: 0.20663729310035706, w is: 0.8649290204048157, b is 0.0074340300634503365\n",
            "iteration 631, loss is: 0.20662830770015717, w is: 0.8650862574577332, b is 0.00738942576572299\n",
            "iteration 632, loss is: 0.20661944150924683, w is: 0.8652425408363342, b is 0.0073450892232358456\n",
            "iteration 633, loss is: 0.20661064982414246, w is: 0.8653978705406189, b is 0.007301018573343754\n",
            "iteration 634, loss is: 0.20660200715065002, w is: 0.8655522465705872, b is 0.007257212419062853\n",
            "iteration 635, loss is: 0.20659345388412476, w is: 0.8657057285308838, b is 0.0072136688977479935\n",
            "iteration 636, loss is: 0.20658497512340546, w is: 0.865858256816864, b is 0.007170387078076601\n",
            "iteration 637, loss is: 0.2065766304731369, w is: 0.8660098910331726, b is 0.007127364631742239\n",
            "iteration 638, loss is: 0.2065683752298355, w is: 0.8661606311798096, b is 0.007084600627422333\n",
            "iteration 639, loss is: 0.20656020939350128, w is: 0.8663104772567749, b is 0.007042093202471733\n",
            "iteration 640, loss is: 0.2065521627664566, w is: 0.8664593696594238, b is 0.006999840494245291\n",
            "iteration 641, loss is: 0.2065441906452179, w is: 0.8666073679924011, b is 0.006957841571420431\n",
            "iteration 642, loss is: 0.20653632283210754, w is: 0.8667545318603516, b is 0.006916094571352005\n",
            "iteration 643, loss is: 0.20652854442596436, w is: 0.8669008016586304, b is 0.0068745980970561504\n",
            "iteration 644, loss is: 0.20652088522911072, w is: 0.8670461773872375, b is 0.0068333507515490055\n",
            "iteration 645, loss is: 0.20651328563690186, w is: 0.8671906590461731, b is 0.006792350672185421\n",
            "iteration 646, loss is: 0.20650579035282135, w is: 0.8673343062400818, b is 0.006751596461981535\n",
            "iteration 647, loss is: 0.20649836957454681, w is: 0.8674770593643188, b is 0.0067110867239534855\n",
            "iteration 648, loss is: 0.20649105310440063, w is: 0.867618978023529, b is 0.006670820061117411\n",
            "iteration 649, loss is: 0.2064838409423828, w is: 0.8677600622177124, b is 0.0066307950764894485\n",
            "iteration 650, loss is: 0.20647665858268738, w is: 0.8679003119468689, b is 0.006591010373085737\n",
            "iteration 651, loss is: 0.2064696103334427, w is: 0.8680396676063538, b is 0.006551464553922415\n",
            "iteration 652, loss is: 0.20646265149116516, w is: 0.8681781888008118, b is 0.006512155756354332\n",
            "iteration 653, loss is: 0.20645573735237122, w is: 0.8683158755302429, b is 0.006473082583397627\n",
            "iteration 654, loss is: 0.20644895732402802, w is: 0.868452787399292, b is 0.006434244103729725\n",
            "iteration 655, loss is: 0.2064422070980072, w is: 0.8685888648033142, b is 0.006395638454705477\n",
            "iteration 656, loss is: 0.20643556118011475, w is: 0.8687241077423096, b is 0.006357264705002308\n",
            "iteration 657, loss is: 0.20642898976802826, w is: 0.8688585162162781, b is 0.006319120991975069\n",
            "iteration 658, loss is: 0.20642249286174774, w is: 0.8689921498298645, b is 0.006281206384301186\n",
            "iteration 659, loss is: 0.20641610026359558, w is: 0.8691250085830688, b is 0.006243519019335508\n",
            "iteration 660, loss is: 0.2064097821712494, w is: 0.8692570328712463, b is 0.006206057965755463\n",
            "iteration 661, loss is: 0.20640350878238678, w is: 0.8693882822990417, b is 0.006168821826577187\n",
            "iteration 662, loss is: 0.20639730989933014, w is: 0.8695187568664551, b is 0.006131808739155531\n",
            "iteration 663, loss is: 0.20639120042324066, w is: 0.8696484565734863, b is 0.006095017772167921\n",
            "iteration 664, loss is: 0.20638515055179596, w is: 0.8697773218154907, b is 0.006058447528630495\n",
            "iteration 665, loss is: 0.2063792198896408, w is: 0.869905412197113, b is 0.006022096611559391\n",
            "iteration 666, loss is: 0.20637330412864685, w is: 0.870032787322998, b is 0.005985964089632034\n",
            "iteration 667, loss is: 0.20636747777462006, w is: 0.870159387588501, b is 0.005950048100203276\n",
            "iteration 668, loss is: 0.20636172592639923, w is: 0.8702852129936218, b is 0.0059143477119505405\n",
            "iteration 669, loss is: 0.20635604858398438, w is: 0.8704102635383606, b is 0.005878861527889967\n",
            "iteration 670, loss is: 0.2063504308462143, w is: 0.8705345988273621, b is 0.005843588151037693\n",
            "iteration 671, loss is: 0.2063448578119278, w is: 0.8706581592559814, b is 0.005808526650071144\n",
            "iteration 672, loss is: 0.20633940398693085, w is: 0.8707810044288635, b is 0.005773675628006458\n",
            "iteration 673, loss is: 0.2063339650630951, w is: 0.8709030747413635, b is 0.005739033687859774\n",
            "iteration 674, loss is: 0.2063286006450653, w is: 0.8710244297981262, b is 0.005704599432647228\n",
            "iteration 675, loss is: 0.20632332563400269, w is: 0.8711450695991516, b is 0.0056703719310462475\n",
            "iteration 676, loss is: 0.20631809532642365, w is: 0.8712649941444397, b is 0.0056363497860729694\n",
            "iteration 677, loss is: 0.20631292462348938, w is: 0.8713842034339905, b is 0.005602531600743532\n",
            "iteration 678, loss is: 0.20630782842636108, w is: 0.871502697467804, b is 0.005568916443735361\n",
            "iteration 679, loss is: 0.20630279183387756, w is: 0.8716204762458801, b is 0.005535502918064594\n",
            "iteration 680, loss is: 0.2062978297472, w is: 0.871737539768219, b is 0.005502290092408657\n",
            "iteration 681, loss is: 0.20629289746284485, w is: 0.8718538880348206, b is 0.005469276569783688\n",
            "iteration 682, loss is: 0.20628802478313446, w is: 0.8719695210456848, b is 0.005436460953205824\n",
            "iteration 683, loss is: 0.20628322660923004, w is: 0.8720844984054565, b is 0.005403842311352491\n",
            "iteration 684, loss is: 0.2062784731388092, w is: 0.872198760509491, b is 0.005371419247239828\n",
            "iteration 685, loss is: 0.20627380907535553, w is: 0.8723123669624329, b is 0.0053391908295452595\n",
            "iteration 686, loss is: 0.20626917481422424, w is: 0.8724252581596375, b is 0.0053071556612849236\n",
            "iteration 687, loss is: 0.20626458525657654, w is: 0.8725374937057495, b is 0.005275312811136246\n",
            "iteration 688, loss is: 0.2062600553035736, w is: 0.872649073600769, b is 0.005243660882115364\n",
            "iteration 689, loss is: 0.20625559985637665, w is: 0.8727599382400513, b is 0.005212198942899704\n",
            "iteration 690, loss is: 0.20625115931034088, w is: 0.872870147228241, b is 0.0051809255965054035\n",
            "iteration 691, loss is: 0.20624679327011108, w is: 0.8729797005653381, b is 0.005149839911609888\n",
            "iteration 692, loss is: 0.20624250173568726, w is: 0.8730885982513428, b is 0.005118940956890583\n",
            "iteration 693, loss is: 0.206238254904747, w is: 0.8731968402862549, b is 0.0050882273353636265\n",
            "iteration 694, loss is: 0.20623402297496796, w is: 0.8733044266700745, b is 0.005057698115706444\n",
            "iteration 695, loss is: 0.20622988045215607, w is: 0.8734113574028015, b is 0.005027351900935173\n",
            "iteration 696, loss is: 0.20622578263282776, w is: 0.8735176920890808, b is 0.00499718775972724\n",
            "iteration 697, loss is: 0.20622171461582184, w is: 0.8736233711242676, b is 0.004967204760760069\n",
            "iteration 698, loss is: 0.2062177062034607, w is: 0.8737283945083618, b is 0.004937401507049799\n",
            "iteration 699, loss is: 0.20621372759342194, w is: 0.8738328218460083, b is 0.004907777067273855\n",
            "iteration 700, loss is: 0.20620982348918915, w is: 0.8739365935325623, b is 0.004878330510109663\n",
            "iteration 701, loss is: 0.20620597898960114, w is: 0.8740397691726685, b is 0.0048490604385733604\n",
            "iteration 702, loss is: 0.20620213449001312, w is: 0.8741422891616821, b is 0.004819965921342373\n",
            "iteration 703, loss is: 0.20619836449623108, w is: 0.874244213104248, b is 0.004791046027094126\n",
            "iteration 704, loss is: 0.2061946541070938, w is: 0.8743455410003662, b is 0.004762299824506044\n",
            "iteration 705, loss is: 0.20619094371795654, w is: 0.8744462728500366, b is 0.004733725916594267\n",
            "iteration 706, loss is: 0.20618729293346405, w is: 0.8745463490486145, b is 0.004705323372036219\n",
            "iteration 707, loss is: 0.20618368685245514, w is: 0.8746458292007446, b is 0.004677091259509325\n",
            "iteration 708, loss is: 0.206180140376091, w is: 0.874744713306427, b is 0.004649028647691011\n",
            "iteration 709, loss is: 0.20617663860321045, w is: 0.8748430013656616, b is 0.004621134605258703\n",
            "iteration 710, loss is: 0.20617316663265228, w is: 0.8749407529830933, b is 0.0045934077352285385\n",
            "iteration 711, loss is: 0.2061697244644165, w is: 0.8750379085540771, b is 0.004565847106277943\n",
            "iteration 712, loss is: 0.2061663419008255, w is: 0.8751344680786133, b is 0.004538452252745628\n",
            "iteration 713, loss is: 0.20616300404071808, w is: 0.8752304315567017, b is 0.004511221311986446\n",
            "iteration 714, loss is: 0.20615969598293304, w is: 0.8753258585929871, b is 0.004484153818339109\n",
            "iteration 715, loss is: 0.2061564326286316, w is: 0.8754206895828247, b is 0.004457248840481043\n",
            "iteration 716, loss is: 0.20615319907665253, w is: 0.8755149245262146, b is 0.004430505447089672\n",
            "iteration 717, loss is: 0.20615001022815704, w is: 0.8756086230278015, b is 0.004403922241181135\n",
            "iteration 718, loss is: 0.20614685118198395, w is: 0.8757017254829407, b is 0.004377498757094145\n",
            "iteration 719, loss is: 0.20614373683929443, w is: 0.8757942914962769, b is 0.004351233597844839\n",
            "iteration 720, loss is: 0.2061406672000885, w is: 0.8758863210678101, b is 0.004325126297771931\n",
            "iteration 721, loss is: 0.20613762736320496, w is: 0.8759778141975403, b is 0.004299175459891558\n",
            "iteration 722, loss is: 0.2061346471309662, w is: 0.8760687112808228, b is 0.004273380618542433\n",
            "iteration 723, loss is: 0.20613163709640503, w is: 0.8761590719223022, b is 0.004247740376740694\n",
            "iteration 724, loss is: 0.20612870156764984, w is: 0.8762488961219788, b is 0.004222253803163767\n",
            "iteration 725, loss is: 0.20612584054470062, w is: 0.8763381838798523, b is 0.004196920432150364\n",
            "iteration 726, loss is: 0.20612294971942902, w is: 0.8764269351959229, b is 0.004171738866716623\n",
            "iteration 727, loss is: 0.20612013339996338, w is: 0.8765151500701904, b is 0.004146708641201258\n",
            "iteration 728, loss is: 0.20611733198165894, w is: 0.876602828502655, b is 0.004121828358620405\n",
            "iteration 729, loss is: 0.20611459016799927, w is: 0.8766899704933167, b is 0.0040970975533127785\n",
            "iteration 730, loss is: 0.2061118632555008, w is: 0.8767766356468201, b is 0.004072514828294516\n",
            "iteration 731, loss is: 0.20610912144184113, w is: 0.8768627643585205, b is 0.004048079717904329\n",
            "iteration 732, loss is: 0.20610648393630981, w is: 0.876948356628418, b is 0.004023791290819645\n",
            "iteration 733, loss is: 0.2061038464307785, w is: 0.8770334720611572, b is 0.003999648615717888\n",
            "iteration 734, loss is: 0.20610123872756958, w is: 0.8771180510520935, b is 0.0039756507612764835\n",
            "iteration 735, loss is: 0.20609869062900543, w is: 0.8772021532058716, b is 0.003951796796172857\n",
            "iteration 736, loss is: 0.20609617233276367, w is: 0.8772857189178467, b is 0.003928086254745722\n",
            "iteration 737, loss is: 0.20609363913536072, w is: 0.8773688077926636, b is 0.003904517740011215\n",
            "iteration 738, loss is: 0.20609116554260254, w is: 0.8774513602256775, b is 0.0038810905534774065\n",
            "iteration 739, loss is: 0.20608873665332794, w is: 0.8775334358215332, b is 0.0038578039966523647\n",
            "iteration 740, loss is: 0.20608629286289215, w is: 0.8776150345802307, b is 0.0038346571382135153\n",
            "iteration 741, loss is: 0.20608390867710114, w is: 0.87769615650177, b is 0.003811649279668927\n",
            "iteration 742, loss is: 0.2060815691947937, w is: 0.8777767419815063, b is 0.003788779489696026\n",
            "iteration 743, loss is: 0.20607919991016388, w is: 0.8778568506240845, b is 0.0037660468369722366\n",
            "iteration 744, loss is: 0.20607690513134003, w is: 0.8779364824295044, b is 0.003743450390174985\n",
            "iteration 745, loss is: 0.20607462525367737, w is: 0.8780156373977661, b is 0.0037209896836429834\n",
            "iteration 746, loss is: 0.2060723751783371, w is: 0.8780943155288696, b is 0.0036986637860536575\n",
            "iteration 747, loss is: 0.2060701549053192, w is: 0.8781725168228149, b is 0.0036764717660844326\n",
            "iteration 748, loss is: 0.20606796443462372, w is: 0.878250241279602, b is 0.0036544129252433777\n",
            "iteration 749, loss is: 0.20606578886508942, w is: 0.8783275485038757, b is 0.003632486332207918\n",
            "iteration 750, loss is: 0.2060636281967163, w is: 0.8784043788909912, b is 0.003610691288486123\n",
            "iteration 751, loss is: 0.20606152713298798, w is: 0.8784807324409485, b is 0.0035890270955860615\n",
            "iteration 752, loss is: 0.20605945587158203, w is: 0.8785566091537476, b is 0.0035674928221851587\n",
            "iteration 753, loss is: 0.2060573697090149, w is: 0.8786320686340332, b is 0.003546087769791484\n",
            "iteration 754, loss is: 0.20605529844760895, w is: 0.8787070512771606, b is 0.003524811239913106\n",
            "iteration 755, loss is: 0.20605327188968658, w is: 0.8787816166877747, b is 0.0035036623012274504\n",
            "iteration 756, loss is: 0.206051304936409, w is: 0.8788557052612305, b is 0.003482640255242586\n",
            "iteration 757, loss is: 0.2060493379831314, w is: 0.8789293766021729, b is 0.0034617444034665823\n",
            "iteration 758, loss is: 0.20604737102985382, w is: 0.879002571105957, b is 0.003440974047407508\n",
            "iteration 759, loss is: 0.20604544878005981, w is: 0.8790753483772278, b is 0.0034203282557427883\n",
            "iteration 760, loss is: 0.206043541431427, w is: 0.8791477084159851, b is 0.003399806097149849\n",
            "iteration 761, loss is: 0.20604166388511658, w is: 0.8792195916175842, b is 0.003379407338798046\n",
            "iteration 762, loss is: 0.20603981614112854, w is: 0.8792910575866699, b is 0.0033591308165341616\n",
            "iteration 763, loss is: 0.2060379981994629, w is: 0.8793621063232422, b is 0.003338976064696908\n",
            "iteration 764, loss is: 0.20603618025779724, w is: 0.879432737827301, b is 0.003318942151963711\n",
            "iteration 765, loss is: 0.20603440701961517, w is: 0.8795029520988464, b is 0.003299028379842639\n",
            "iteration 766, loss is: 0.20603260397911072, w is: 0.8795726895332336, b is 0.0032792342826724052\n",
            "iteration 767, loss is: 0.20603087544441223, w is: 0.8796420097351074, b is 0.003259558929130435\n",
            "iteration 768, loss is: 0.20602913200855255, w is: 0.8797109127044678, b is 0.0032400016207247972\n",
            "iteration 769, loss is: 0.20602741837501526, w is: 0.8797794580459595, b is 0.003220561658963561\n",
            "iteration 770, loss is: 0.20602576434612274, w is: 0.8798475861549377, b is 0.0032012383453547955\n",
            "iteration 771, loss is: 0.20602408051490784, w is: 0.8799152970314026, b is 0.0031820309814065695\n",
            "iteration 772, loss is: 0.2060224413871765, w is: 0.879982590675354, b is 0.003162938868626952\n",
            "iteration 773, loss is: 0.2060208022594452, w is: 0.880049467086792, b is 0.0031439613085240126\n",
            "iteration 774, loss is: 0.20601920783519745, w is: 0.8801159262657166, b is 0.0031250976026058197\n",
            "iteration 775, loss is: 0.2060176134109497, w is: 0.8801820278167725, b is 0.0031063470523804426\n",
            "iteration 776, loss is: 0.20601604878902435, w is: 0.8802477121353149, b is 0.0030877089593559504\n",
            "iteration 777, loss is: 0.206014484167099, w is: 0.880312979221344, b is 0.003069182625040412\n",
            "iteration 778, loss is: 0.20601297914981842, w is: 0.8803778886795044, b is 0.00305076758377254\n",
            "iteration 779, loss is: 0.20601145923137665, w is: 0.8804423809051514, b is 0.00303246290422976\n",
            "iteration 780, loss is: 0.20600998401641846, w is: 0.8805065155029297, b is 0.003014268120750785\n",
            "iteration 781, loss is: 0.20600847899913788, w is: 0.8805702328681946, b is 0.0029961825348436832\n",
            "iteration 782, loss is: 0.20600701868534088, w is: 0.8806335926055908, b is 0.0029782054480165243\n",
            "iteration 783, loss is: 0.20600558817386627, w is: 0.8806965947151184, b is 0.002960336161777377\n",
            "iteration 784, loss is: 0.20600415766239166, w is: 0.8807591795921326, b is 0.0029425742104649544\n",
            "iteration 785, loss is: 0.20600274205207825, w is: 0.8808214068412781, b is 0.002924918895587325\n",
            "iteration 786, loss is: 0.2060013860464096, w is: 0.8808832764625549, b is 0.0029073692858219147\n",
            "iteration 787, loss is: 0.20600000023841858, w is: 0.8809447884559631, b is 0.002889924915507436\n",
            "iteration 788, loss is: 0.20599862933158875, w is: 0.8810058832168579, b is 0.002872585318982601\n",
            "iteration 789, loss is: 0.2059973180294037, w is: 0.881066620349884, b is 0.00285534979775548\n",
            "iteration 790, loss is: 0.20599597692489624, w is: 0.8811269998550415, b is 0.0028382176533341408\n",
            "iteration 791, loss is: 0.20599465072155, w is: 0.8811870217323303, b is 0.0028211884200572968\n",
            "iteration 792, loss is: 0.20599335432052612, w is: 0.8812466859817505, b is 0.002804261166602373\n",
            "iteration 793, loss is: 0.20599208772182465, w is: 0.881305992603302, b is 0.0027874356601387262\n",
            "iteration 794, loss is: 0.20599083602428436, w is: 0.8813649415969849, b is 0.0027707109693437815\n",
            "iteration 795, loss is: 0.20598958432674408, w is: 0.8814235329627991, b is 0.0027540866285562515\n",
            "iteration 796, loss is: 0.205988347530365, w is: 0.8814817667007446, b is 0.002737562172114849\n",
            "iteration 797, loss is: 0.2059871256351471, w is: 0.8815396428108215, b is 0.002721136901527643\n",
            "iteration 798, loss is: 0.2059859037399292, w is: 0.8815971612930298, b is 0.002704810118302703\n",
            "iteration 799, loss is: 0.2059847116470337, w is: 0.8816543817520142, b is 0.002688581356778741\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YoZh0aFCQoi",
        "outputId": "f2e73f3b-b2fc-442a-d8fb-22c23e4b3c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[0.8852]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}